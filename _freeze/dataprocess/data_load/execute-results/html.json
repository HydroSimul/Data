{
  "hash": "bc13efca38336042b5c048a45079b80d",
  "result": {
    "markdown": "---\ntitle: \"Data Loading\"\nexecute:\n  warning: false\n  error: false\nsidebar:\n  contents: auto\nnumber-sections: true\n---\n\n\n\nThis Aritcl will show the process to load data from other files. I t will divide into four paties: plain text (read able ASCII), Excel, NetCDF and spatial data. \n\nOverview:\n\n\n![](../images/dataload_overview.svg)\n\n# Plain text File\n\nFor more details about date (file) format, you can refer to the article titled [Basic Data & File Format](basic_format.qmd).\n\n## Example File\n\nLet's start with an example CSV file named `Bachum_2763190000100.csv`. This file contains pegel discharge data and is sourced from open data available at [ELWAS-WEB NRW](https://www.elwasweb.nrw.de/elwas-web/index.xhtml). You can also access it directly from the internet via [Github](https://raw.githubusercontent.com/HydroSimul/Web/main/data_share/Bachum_2763190000100.csv), just like you would access a local file.\n\nTake a look:\n\n\n![](../images/bachum.png)\n\n## Library and functions\n\n::: {.panel-tabset}\n\n## R\n\nFirst, we need to load the necessary library `tidyverse`. This library collection includes `readr` for reading files and `dplyr` for data manipulation, among others.\n\nAnd, we set the URL address as the file path (including the file name).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load the library\nlibrary(tidyverse)\nfn_Bachum <- \"https://raw.githubusercontent.com/HydroSimul/Web/main/data_share/Bachum_2763190000100.csv\"\nfn_Datatype <- \"https://raw.githubusercontent.com/HydroSimul/Web/main/data_share/load_Datatype.txt\"\n```\n:::\n\n\nThe documentation for the `readr` library is available online and can be accessed at [https://readr.tidyverse.org](https://readr.tidyverse.org). \n\nOf particular interest are the following functions:\n\n- [readr::read_csv()](https://readr.tidyverse.org/reference/read_delim.html)\n- [readr::read_table()](https://readr.tidyverse.org/reference/read_table.html)\n\nWe can observe that the CSV file is divided by semicolons. Therefore, it's more appropriate to use `read_csv2()` rather than `read_csv()`.\n\nThe difference between `read_*()` functions in the `readr` package is determined by the delimiter character used in the files:\n\n\n![CHEAT SHEET from [Rstudio](https://github.com/rstudio/cheatsheets/blob/main/data-import.pdf)](../images/readr_read_.png)\n\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# load the library\nimport pandas as pd\nfn_Bachum = \"https://raw.githubusercontent.com/HydroSimul/Web/main/data_share/Bachum_2763190000100.csv\"\nfn_Datatype = \"https://raw.githubusercontent.com/HydroSimul/Web/main/data_share/load_Datatype.txt\"\n```\n:::\n\n\nThe documentation for the `pandas` library is available online and can be accessed at [https://pandas.pydata.org/docs/index.html](https://pandas.pydata.org/docs/index.html). \n\nOf particular interest are the following functions:\n\n- [pandas.read_csv()](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)\n- [pandas.read_table()](https://pandas.pydata.org/docs/reference/api/pandas.read_table.html)\n\n\n:::\n\n## Metadata Handel\n\nMetadata can vary widely between datasets, so it's handled separately from the data body.\n\nThere are three ways to deal with metadata:\n\n1. **Directly Ignore:** This approach involves ignoring metadata when it's redundant or readily available from other data sources, such as file names or external references.\n\n2. **Extract from Text:** When metadata is crucial but not in table form, you can extract information from text strings. For more information, refer to the section on string manipulation [@sec-string].\n\n3. **Read as a Second Table:** If metadata is well-organized in a tabular format, it can be read as a separate table to facilitate its use.\n\nIn the `Bachum_2763190000100.csv` file, you will find that there are 10 lines of metadata, which are well-organized in a tabular format. However, it's important to note that the consistency in values column varies.\n\n### Directly Ignore use grguments `skip`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# skip = 10\nread_csv2(fn_Bachum, skip = 10, n_max = 10, col_names = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 × 2\n   X1            X2\n   <chr>      <dbl>\n 1 01.01.1990  20.6\n 2 02.01.1990  19.0\n 3 03.01.1990  17.9\n 4 04.01.1990  16.8\n 5 05.01.1990  16.0\n 6 06.01.1990  14.8\n 7 07.01.1990  14.3\n 8 08.01.1990  14.0\n 9 09.01.1990  14.4\n10 10.01.1990  14.5\n```\n:::\n:::\n\n\n### Read metadata as table\n\nWhen directly reading all metadata into one table, you may encounter mixed data types. In the metadata, there are three data types:\n\n- Numeric: Examples include `Pegelnullpunkt` and `Einzugsgebiet`.\n- String: This category covers fields like `Name`, `Pegelnummer`, and others.\n- Date: Date values are present in columns like `Datum von` and `Datum bis`.\n\nIn a data frame (tibble), columns must have the same data type. Consequently, R will automatically convert them to a single data type, which is typically string.\n\nTo address this situation, you should specify the data type you want to read. For example, to read the date values in lines 4 and 5, you can use the following settings:\n1. `skip = 3` to skip the first three lines of metadata.\n2. `n_max = 2` to read the next two lines (lines 4 and 5) as date values.\n\n\n::: {.panel-tabset}\n\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# skip = 3\nread_csv2(fn_Bachum, skip = 3, n_max = 2, col_names = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 2\n  X1        X2        \n  <chr>     <chr>     \n1 Datum von 01.01.1990\n2 Datum bis 31.12.2022\n```\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndf_bach = pd.read_csv(fn_Bachum, skiprows=3, nrows=2, header=None, delimiter=';', encoding='latin-1')\nprint(df_bach)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           0           1\n0  Datum von  01.01.1990\n1  Datum bis  31.12.2022\n```\n:::\n:::\n\n\n:::\n\n\nUnfortunately, R may not always recognize date values correctly, so you may need to perform additional steps for conversion:\n\n1. **After Reading:** This involves transforming the data from its initial format to the desired date format within your R environment.\n\n2. **Set the Data Type by Reading:** Another approach is to set the data type while reading the data. \n\nMore details in the next section:\n\n## Load tabular data\n\n::: {.panel-tabset}\n\n## R\n\n\n\nTo read the first 10 lines of metadata, you can use the `n_max` setting with a value of `n_max = 10` in the `read_csv2()` function.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nread_csv2(fn_Bachum, n_max = 10, col_names = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 × 2\n   X1                          X2             \n   <chr>                       <chr>          \n 1 \"Name\"                      \"Bachum\"       \n 2 \"Pegelnummer\"               \"2763190000100\"\n 3 \"Gew\\xe4sser\"               \"Ruhr\"         \n 4 \"Datum von\"                 \"01.01.1990\"   \n 5 \"Datum bis\"                 \"31.12.2022\"   \n 6 \"Parameter\"                 \"Abfluss\"      \n 7 \"Q Einheit\"                 \"m\\xb3/s\"      \n 8 \"Tagesmittelwerte\"           <NA>          \n 9 \"Pegelnullpunkt [m\\xfcNHN]\" \"146,83\"       \n10 \"Einzugsgebiet [km\\xb2]\"    \"1.532,02\"     \n```\n:::\n:::\n\n\nAfter dealing with the metadata, we can proceed to load the data body using the `readr::read_*()` function cluster. \nPlain text files typically store data in a tabular or matrix format, both of which have at most two dimensions. \nWhen using the `readr::read_()` function, it automatically returns a `tibble`. If your data in the text file is in matrix format, you can use conversion functions like `as.matrix()` to transform it into other data structures.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1. load\ntb_Read <- read_csv2(fn_Bachum, skip = 10, n_max = 10, col_names = FALSE)\ntb_Read\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 × 2\n   X1            X2\n   <chr>      <dbl>\n 1 01.01.1990  20.6\n 2 02.01.1990  19.0\n 3 03.01.1990  17.9\n 4 04.01.1990  16.8\n 5 05.01.1990  16.0\n 6 06.01.1990  14.8\n 7 07.01.1990  14.3\n 8 08.01.1990  14.0\n 9 09.01.1990  14.4\n10 10.01.1990  14.5\n```\n:::\n\n```{.r .cell-code}\n# 2. convert\ndf_Read <- as.data.frame(tb_Read)\nmat_Read <- as.matrix(tb_Read)\n\ndf_Read\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           X1     X2\n1  01.01.1990 20.640\n2  02.01.1990 18.994\n3  03.01.1990 17.949\n4  04.01.1990 16.779\n5  05.01.1990 16.019\n6  06.01.1990 14.817\n7  07.01.1990 14.296\n8  08.01.1990 13.952\n9  09.01.1990 14.403\n10 10.01.1990 14.500\n```\n:::\n\n```{.r .cell-code}\nmat_Read\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      X1           X2      \n [1,] \"01.01.1990\" \"20.640\"\n [2,] \"02.01.1990\" \"18.994\"\n [3,] \"03.01.1990\" \"17.949\"\n [4,] \"04.01.1990\" \"16.779\"\n [5,] \"05.01.1990\" \"16.019\"\n [6,] \"06.01.1990\" \"14.817\"\n [7,] \"07.01.1990\" \"14.296\"\n [8,] \"08.01.1990\" \"13.952\"\n [9,] \"09.01.1990\" \"14.403\"\n[10,] \"10.01.1990\" \"14.500\"\n```\n:::\n:::\n\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntb_Read = pd.read_csv(fn_Bachum, skiprows=10, nrows=10, header=None, delimiter=';', decimal=',', encoding='latin-1')\nprint(tb_Read)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            0       1\n0  01.01.1990  20.640\n1  02.01.1990  18.994\n2  03.01.1990  17.949\n3  04.01.1990  16.779\n4  05.01.1990  16.019\n5  06.01.1990  14.817\n6  07.01.1990  14.296\n7  08.01.1990  13.952\n8  09.01.1990  14.403\n9  10.01.1990  14.500\n```\n:::\n:::\n\n\n:::\n\n## Data type {#sec-datatype}\n\nIn this section, we will work with a custom-made text file that contains various data types and formats. The file consists of three rows, with one of them serving as the header containing column names, and six columns in total.\n\nLet's take a look:\n\n![](../images/dataload_datatype_txt.png)\n\nActually the function will always guse the dattype for each column, when the data really normally format the function will return the right datatype for the data:\n\n::: {.panel-tabset}\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nread_table(fn_Datatype)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 6\n    int float_en float_de date_en    date_de    str  \n  <dbl>    <dbl> <chr>    <date>     <chr>      <chr>\n1     1      0.1 0,1      2023-09-15 15.09.2023 en   \n2     9      9.6 9,6      2023-09-16 16.09.2023 de   \n```\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndf = pd.read_table(fn_Datatype)\nprint(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   int  float_en float_de     date_en     date_de str\n0    1       0.1      0,1  2023-09-15  15.09.2023  en\n1    9       9.6      9,6  2023-09-16  16.09.2023  de\n```\n:::\n\n```{.python .cell-code}\nprint(df.dtypes)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nint           int64\nfloat_en    float64\nfloat_de     object\ndate_en      object\ndate_de      object\nstr          object\ndtype: object\n```\n:::\n:::\n\n\n:::\n\nBy default, functions like `readr::read_table()` in R and `pandas.read_table()` in Python will attempt to guess data types automatically when reading data. Here's how this guessing typically works:\n\n- If a column contains only numbers and decimal dots (periods), it will be recognized as numeric (double in R and int or float in Python).\n\n- If a date is formatted in \"Y-M-D\" (e.g., \"2023-08-27\") or \"h:m:s\" (e.g., \"15:30:00\") formats, it may be recognized as a date or time type. **Nur in R**\n\n- If the data type cannot be confidently determined, it is often treated as a string (str in R and object in Python).\n\nThis automatic guessing is convenient, but it's essential to verify the inferred data types, especially when working with diverse datasets. \n\n### Set the Data Type by Reading\n\nExplicitly setting data types using the `col_types` (in R) or `dtype` (in Python) argument can help ensure correct data handling.\n\n\n::: {.panel-tabset}\n\n## R\n\n\n\n\n\nTo address the issue of date recognition, you can set the `col_types` argument,\nyou can use a compact string representation where each character represents one column:\n\n- `c`: Character\n- `i`: Integer\n- `n`: Number\n- `d`: Double\n- `l`: Logical\n- `f`: Factor\n- `D`: Date\n- `T`: Date Time\n- `t`: Time\n- `?`: Guess\n- `_` or `-`: Skip\n\n\n\nto `\"cD\"` when reading the data. This informs the function that the first column contains `c`haracters (`c`) and the second column contains `D`ates (`D`).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nread_table(fn_Datatype, col_types = \"iddDDc\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 6\n    int float_en float_de date_en    date_de str  \n  <int>    <dbl>    <dbl> <date>     <date>  <chr>\n1     1      0.1       NA 2023-09-15 NA      en   \n2     9      9.6       NA 2023-09-16 NA      de   \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nread_table(fn_Datatype, col_types = \"idd?Dc\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 6\n    int float_en float_de date_en    date_de str  \n  <int>    <dbl>    <dbl> <date>     <date>  <chr>\n1     1      0.1       NA 2023-09-15 NA      en   \n2     9      9.6       NA 2023-09-16 NA      de   \n```\n:::\n:::\n\n\n\n## Python\n\nTo set data types when reading data using functions `pandas.read_*`, you have three main choices by using the `dtype` parameter:\n\n- `str`: Specify the data type as a string.\n- `int`: Specify the data type as an integer.\n- `float`: Specify the data type as a floating-point number.\n\nHowever, you can also use the `dtype` parameter with a callable function to perform more advanced type conversions. Some commonly used functions include:\n\n- `pd.to_datetime`: Converts a column to datetime format.\n- `pd.to_numeric`: Converts a column to numeric (integer or float) format.\n- `pd.to_timedelta`: Converts a column to timedelta format.\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Define column names and types as a dictionary\ncol_types = {\"X1\": str, \"X2\": pd.to_datetime}\n# Read the CSV file, skip 3 rows, read 2 rows, and specify column names and types\ndf = pd.read_csv(fn_Bachum, skiprows=3, nrows=2, header=None, delimiter=';', names=[\"X1\", \"X2\"], dtype=col_types, encoding='latin-1')\n\n# Display the loaded data\nprint(df)\n```\n:::\n\n\n::: {.callout-waring}\nDON'T RUN\nError, because data doesn't match the default format of 'Y-m-d'.\n:::\n\n\n:::\n\n\n::: {.callout-waring}\nUnfortunately, the default date format in R and Python may not work for German-style dates like \"d.m.Y\" as R and Python primarily recognizes the \"Y-m-d\" format. \n:::\n\n\n\n\n\n### After Reading\n\nTo address this issue, you can perform date conversions after reading the data:\n\n\n\n::: {.panel-tabset}\n\n## R\n\nUsing function `as.Date()` and specify the date format using the `format` argument, such as `format = \"%d.%m.%Y\"`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_Date <- read_csv2(fn_Bachum, skip = 3, n_max = 2, col_names = FALSE)\ndf_Date$X2 <- df_Date$X2 |> as.Date(format = \"%d.%m.%Y\")\ndf_Date\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 2\n  X1        X2        \n  <chr>     <date>    \n1 Datum von 1990-01-01\n2 Datum bis 2022-12-31\n```\n:::\n:::\n\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndf_Date = pd.read_csv(fn_Bachum, skiprows=3, nrows=2, header=None, delimiter=';', encoding='latin-1')\n\n# Display the loaded data\nprint(df_Date)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           0           1\n0  Datum von  01.01.1990\n1  Datum bis  31.12.2022\n```\n:::\n\n```{.python .cell-code}\n# 2. Convert the second column (X2) to a date format\ndf_Date[1] = pd.to_datetime(df_Date[1], format='%d.%m.%Y')\n\n# Display the DataFrame with the second column converted to date format\nprint(df_Date)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           0          1\n0  Datum von 1990-01-01\n1  Datum bis 2022-12-31\n```\n:::\n:::\n\n\n:::\n\n\n\n\n\n\n\n\n\n# Excel File\n\nWhen we discuss the combination of the software Excel with data files in formats such as .xls or .xlsx, there are numerous possibilities for data science. However, when we specifically consider the file format, there are distinct differences between plain text and Excel files:\n\n**Plain Text File vs. Excel for Data Storage**\n\n| Aspect                      | Plain Text File                   | Excel File                            |\n|-----------------------------|-----------------------------------|--------------------------------------|\n| **Data Structure**          | Typically stores data in a tabular format or matrix. | Stores data in structured worksheets with multiple tables (sheets). |\n| **Compatibility**           | Universally compatible with various software and programming languages. | Compatibility may vary, and not all software can read Excel files. |\n| **Human-Readable**          | Easily readable by humans in a simple text format. | Readable by humans but may include formatting that isn't immediately apparent. |\n| **Data Transfer**           | Easily shared and transferred between different platforms and systems. | May require conversion or specific software for seamless data transfer. |\n| **Data Import/Export**      | May require custom import/export scripts for specific applications. | Supports standardized import/export formats for various applications. |\n| **Version Control**         | Suitable for version control systems (e.g., Git) for tracking changes. | Not well-suited for version control due to binary format and complex changes. |\n| **Data Analysis**           | Requires additional software to analyze data (e.g., R or Python). | Offers built-in data analysis tools (e.g., formulas, charts). |\n| **Openness and Access**     | Open and transparent; data can be accessed and edited with any text editor. | Proprietary format may require specific software (Microsoft Excel) to access and edit. |\n\n\nUnlike plain text files, Excel files have the capability to contain multiple tables, known as sheets. In Excel, each cell within a sheet is uniquely identified by its specific coordinates. Rows are indexed with numerical values, and columns are identified using alphabetical indices. By combining the sheet name with these coordinates, it is possible to precisely locate any cell within an Excel file and retrieve the value it contains.\n\n\n## Example File\n\nLet's begin with an example Excel file named `Pegeln_NRW.xlsx`. This file contains information about measurement stations in NRW (Nordrhein-Westfalen, Germany) and is sourced from open data available at [ELWAS-WEB NRW](https://www.elwasweb.nrw.de/elwas-web/index.xhtml). You can also access it directly from  [Github](https://github.com/HydroSimul/Web/blob/main/data_share/Pegeln_NRW.xlsx).\n\nTake a look:\n\n![](../images/dataload_excel_pegeln.png)\n\n\n## R library and functions\n\nTo load the necessary library, `readxl`, and access its help documentation, you can visit [this link](https://readxl.tidyverse.org/). The `readxl::read_excel()` function is versatile, as it can read both .xls and .xlsx files and automatically detects the format based on the file extension. Additionally, you have the options of using `read_xls()` for .xls files and `read_xlsx()` for .xlsx files. More details in the [Page](https://readxl.tidyverse.org/reference/read_excel.html).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load the library\nlibrary(readxl)\n# The Excel file cannot be read directly from GitHub. You will need to download it to your local machine first\nfn_Pegeln <- \"C:\\\\Lei\\\\HS_Web\\\\data_share/Pegeln_NRW.xlsx\"\n```\n:::\n\n\n\n## Load tabular data\n\nSimilar to plain text files, metadata is often provided before the data body in Excel files. \nIn Excel, each cell can be assigned a specific data type, while in R tables (data.frame or tibble), every column must have the same data type. This necessitates separate handling of metadata and data body to ensure that the correct data types are maintained.\n\nUnlike plain text files where we can only select lines to load, Excel allows us to define coordinates to access a specific celles-box wherever they are located.\n\n### First try without any setting\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# try without setting\ntb_Pegeln <- read_excel(fn_Pegeln)\ntb_Pegeln\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 277 × 16\n   Suchergebnisse Pegel.…¹ ...2  ...3  ...4  ...5  ...6  ...7  ...8  ...9  ...10\n   <chr>                   <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr>\n 1 \"Suchkriterien:\\n -- \\… <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA> \n 2  <NA>                   <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA> \n 3  <NA>                   <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA> \n 4 \"Name\"                  Pege… Gewä… Betr… Pege… Einz… Q von Q bis NQ    MNQ  \n 5 \"Ahlen\"                 3211… Werse LANU… 73,47 46,62 1975  2013  0     0,07 \n 6 \"Ahmsen\"                4639… Werre LANU… 64,28 593   1963  2022  1,21  2,22 \n 7 \"Ahrhütte-Neuhof\"       2718… Ahr   LANU… 340,… 124   1986  2011  0,22  0,36 \n 8 \"Albersloh\"             3259… Werse LANU… 48,68 321,… 1973  2020  0,12  0,24 \n 9 \"Altena\"                2766… Lenne LANU… 154,… 1.190 1950  2021  1,36  6,48 \n10 \"Altena_Rahmedestraße\"  2766… Rahm… LANU… 157,… 29,6  <NA>  <NA>  <NA>  <NA> \n# ℹ 267 more rows\n# ℹ abbreviated name: ¹​`Suchergebnisse Pegel.xlsx 14.09.2023 10:01`\n# ℹ 6 more variables: ...11 <chr>, ...12 <chr>, ...13 <chr>, ...14 <chr>,\n#   ...15 <chr>, ...16 <chr>\n```\n:::\n:::\n\n\n\nWhen we provide only the file name to the function, we will always retrieve all the content from the first sheet. However, due to the limitations in R tables, every column will be recognized as the same data type, typically character.\n\n### Give a `range`\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# using the range argument\ntb_Pegeln_Range <- read_excel(fn_Pegeln, range = \"Suchergebnisse Pegel!A5:P10\")\ntb_Pegeln_Range\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 16\n  Name            Pegelnummer   Gewässername Betreiber  `Pegelnullpunkt [müNHN]`\n  <chr>           <chr>         <chr>        <chr>      <chr>                   \n1 Ahlen           3211000000300 Werse        LANUV, NRW 73,47                   \n2 Ahmsen          4639000000100 Werre        LANUV, NRW 64,28                   \n3 Ahrhütte-Neuhof 2718193000100 Ahr          LANUV, NRW 340,58                  \n4 Albersloh       3259000000100 Werse        LANUV, NRW 48,68                   \n5 Altena          2766930000100 Lenne        LANUV, NRW 154,22                  \n# ℹ 11 more variables: `Einzugsgebiet [km²]` <chr>, `Q von` <chr>,\n#   `Q bis` <chr>, NQ <chr>, MNQ <chr>, MQ <chr>, MHQ <chr>, HQ <chr>,\n#   `Q Einheit` <chr>, `Ostwert in UTM` <chr>, `Nordwert in UTM` <chr>\n```\n:::\n:::\n\n\n::: {.callout-warning}\nThe data type of \"Pegelnullpunkt [müNHN]\" appears to be incorrect due to improper settings in Excel.\n:::\n\n\n## Data type\n\nCompared to plain text files, Excel data already contains data type information for each cell. Therefore, the data type will be directly determined by the data type specified in Excel.\n\nHowever, there are instances where the data type in Excel is not correctly set, so manual data type conversion may be necessary. For more details, refer to [@sec-datatype].\n\n\n# NetCDF\n\nthe process loading of netCDF is seperated in the page\n\n# Text & String {#sec-string}\n\nmore details ",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}