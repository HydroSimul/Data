[
  {
    "objectID": "Postprocess/PP_0.html",
    "href": "Postprocess/PP_0.html",
    "title": "Post-Processing",
    "section": "",
    "text": "About this site test1\n\n1 + 1\n\n[1] 2\n\n\n\n\nAbout this site test2\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Postprocess/PP_0.html#h2",
    "href": "Postprocess/PP_0.html#h2",
    "title": "Post-Processing",
    "section": "",
    "text": "About this site test2\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcom Page",
    "section": "",
    "text": "Welcome to HydroSimul: Your Gateway to Understanding Hydrological Simulation\nWe’re thrilled to have you at HydroSimul, your premier resource for delving into the realm of hydrological simulation. Here, we curate a vast collection of datasets and offer cutting-edge techniques to empower your exploration.\nAt HydroSimul, we believe in the power of collaboration. If you want to share your knowledge, experience, and skills in the field, please contact us at hydro.simul@gmail.com. We also welcome your advice.\nOur website serves as a comprehensive hub for your hydrological journey:"
  },
  {
    "objectID": "index.html#dataset-collection",
    "href": "index.html#dataset-collection",
    "title": "Welcom Page",
    "section": "1. Dataset Collection:",
    "text": "1. Dataset Collection:\nNumerous open-access datasets are readily available for use in hydrological modeling, including meteorological, hydrological, and various geophysical datasets. Within this dataset collection, we not only provide direct links to the datasets but also present essential information in a standardized format, simplifying your dataset selection process. Additionally, we strive to establish connections with research papers that have utilized these datasets and offer valuable feedback gleaned from these sources."
  },
  {
    "objectID": "index.html#data-processing",
    "href": "index.html#data-processing",
    "title": "Welcom Page",
    "section": "2. Data Processing:",
    "text": "2. Data Processing:\nOn this page, we provide you with a wealth of data processing tools and techniques. Discover how to adeptly clean, preprocess, and convert raw hydrological data into a valuable format suitable for analysis and modeling."
  },
  {
    "objectID": "index.html#data-analysis",
    "href": "index.html#data-analysis",
    "title": "Welcom Page",
    "section": "3. Data Analysis:",
    "text": "3. Data Analysis:\nUncover concealed statistical insights and trends within your data. Our comprehensive guides and tutorials are designed to empower you with the skills to effectively analyze hydrological data."
  },
  {
    "objectID": "index.html#hydrological-modeling",
    "href": "index.html#hydrological-modeling",
    "title": "Welcom Page",
    "section": "4. Hydrological Modeling:",
    "text": "4. Hydrological Modeling:\nTake your understanding to the next level with hydrological modeling. Explore various models and model frameworks and acquire hands-on experience in simulating complex hydrological processes. Additionally, we provide extensive resources on calibration algorithms and strategies to significantly improve your modeling outcomes."
  },
  {
    "objectID": "dataset/index.html",
    "href": "dataset/index.html",
    "title": "Dataset",
    "section": "",
    "text": "Numerous open-access datasets are readily available for use in hydrological modeling, including meteorological, hydrological, and various geophysical datasets. Within this dataset collection, we not only provide direct links to the datasets but also present essential information in a standardized format, simplifying your dataset selection process. Additionally, we strive to establish connections with research papers that have utilized these datasets and offer valuable feedback gleaned from these sources."
  },
  {
    "objectID": "dataset/geoph.html",
    "href": "dataset/geoph.html",
    "title": "Geophysical",
    "section": "",
    "text": "About this site test1\n\n1 + 1\n\n[1] 2\n\n\n\n\nAbout this site test2\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "dataset/geoph.html#h2",
    "href": "dataset/geoph.html#h2",
    "title": "Geophysical",
    "section": "",
    "text": "About this site test2\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "dataprocess/NetCDF.html",
    "href": "dataprocess/NetCDF.html",
    "title": "NetCDF",
    "section": "",
    "text": "About this site test1\n\n1 + 1\n\n[1] 2\n\n\n\n\nAbout this site test2\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "dataprocess/NetCDF.html#h2",
    "href": "dataprocess/NetCDF.html#h2",
    "title": "NetCDF",
    "section": "",
    "text": "About this site test2\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "dataprocess/basic_format.html",
    "href": "dataprocess/basic_format.html",
    "title": "Basic Data & File Format",
    "section": "",
    "text": "In the realm of hydrological modeling, various data structures and file formats are employed to effectively manage, analyze, and simulate hydrological processes. These structures and formats facilitate the representation of hydrological data, making it accessible for researchers, modelers, and decision-makers. Below are some of the common data structures and file formats used in hydrological modeling, along with their key features.\nOverview:"
  },
  {
    "objectID": "dataprocess/basic_format.html#ascii",
    "href": "dataprocess/basic_format.html#ascii",
    "title": "Basic format",
    "section": "ASCII",
    "text": "ASCII\nASCII (American Standard Code for Information Interchange) is a plain text format, making it human-readable.\n\nAdvantages\n\nHuman-Readable: Users can easily view, understand, and edit the data directly in a text editor.\nWidespread Support, Ease of Import/Export: ASCII is universally supported. Most programming languages, data analysis tools, and software applications can read and write ASCII files, ensuring high compatibility.\nLightweight: ASCII files are typically lightweight and do not consume excessive storage space, making them suitable for large datasets.\nSimple Structure: ASCII files have a straightforward structure, often using lines of text with fields separated by delimiters. This simplicity aids in data extraction and manipulation.\n\n\n\nDisadvantages\n\nLimited Data Types: ASCII primarily handles text-based data and is not suitable for complex data types such as images, multimedia, or hierarchical data.\nNo Inherent Data Validation: ASCII files lack built-in mechanisms for data validation or integrity checks, requiring users to ensure data conformity.\nLack of Compression: ASCII files do not inherently support data compression, potentially resulting in larger file sizes compared to binary formats.\nLimited Metadata: ASCII format lacks standardized metadata structures common in other formats, making comprehensive data documentation challenging.\nSlower Reading/Writing: Reading and writing data in ASCII format may be slower, especially for large datasets, due to additional parsing required to interpret text-based data.\n\n\n\nfile format for ASCII data\nWhen it comes to plain text formats, there is no universal standard, and it’s highly adaptable to specific needs. The initial step in loading a plain text table is to analyze the structure of the file.\nTypically, a text table can store 2D data, comprising columns and rows or a matrix. However, above the data body, there’s often metadata that describes the data. Metadata can vary widely between data body.\nDividing rows is usually straightforward and can be achieved by identifying row-end characters. However, dividing columns within each row presents multiple possibilities, such as spaces, tabs, commas, or semicolons.\nIn .txt files, any of these separators can be used, but in .csv files, commas or semicolons are commonly employed as separator characters.\n\n.txt: This is the most generic and widely used file extension for plain text files. It doesn’t imply any specific format or structure; it’s just a simple text file.\n.csv (Comma-Separated Values): While CSV files contain data separated by commas, they are still considered ASCII files because they use plain text characters to represent data values. Each line in a CSV file typically represents a record, with values separated by commas."
  },
  {
    "objectID": "dataprocess/basic_format.html#binary",
    "href": "dataprocess/basic_format.html#binary",
    "title": "Basic Data & File Format",
    "section": "Binary",
    "text": "Binary\nUnlike text-based files, binary files store data in a way that is optimized for computer processing and can represent a wide range of data types, from simple numbers to complex structures. These files are used in various applications, including programming, scientific research, and data storage, due to their efficiency in handling data.\n\nAdvantages of Binary Formats\n\nEfficiency: Binary formats are highly efficient for data storage and transmission because they represent data in a compact binary form. This can significantly reduce storage space and data transfer times, making them ideal for large datasets.\nData Integrity: Binary formats often include built-in mechanisms for data integrity and error checking. This helps ensure that data remains intact and accurate during storage and transmission.\nComplex Data: Binary formats can represent complex data structures, which makes them suitable for a wide range of data types.\nFaster I/O: Reading and writing data in binary format is generally faster than text-based formats like ASCII. This efficiency is particularly important for applications that require high-speed data processing.\nSecurity: Binary formats can provide a level of data security because they are not easily human-readable. This can be advantageous when dealing with sensitive information.\n\n\n\nDisadvantages of Binary Formats\n\nLack of Human-Readability: Binary formats are not human-readable, making it difficult to view or edit the data directly. This can be a disadvantage when data inspection or manual editing is required.\nCompatibility: Binary formats may not be universally compatible across different software platforms and programming languages. This can lead to issues when sharing or accessing data in various environments.\nLimited Metadata: Binary formats may not include comprehensive metadata structures, making it challenging to document and describe the data effectively.\nVersion Compatibility: Changes in the binary format’s structure or encoding can lead to compatibility issues when working with data created using different versions of software or hardware.\nPlatform Dependence: Binary formats can be platform-dependent, meaning they may not be easily transferable between different operating systems or hardware architectures.\n\nBinary formats are a valuable choice for certain applications, particularly when efficiency, data integrity, and complex data types are crucial. However, they may not be suitable for all scenarios, especially when human readability, compatibility, or ease of data inspection is essential."
  },
  {
    "objectID": "dataprocess/index.html",
    "href": "dataprocess/index.html",
    "title": "Dataset",
    "section": "",
    "text": "Numerous open-access datasets are readily available for use in hydrological modeling, including meteorological, hydrological, and various geophysical datasets. Within this dataset collection, we not only provide direct links to the datasets but also present essential information in a standardized format, simplifying your dataset selection process. Additionally, we strive to establish connections with research papers that have utilized these datasets and offer valuable feedback gleaned from these sources."
  },
  {
    "objectID": "dataprocess/spatial_data.html",
    "href": "dataprocess/spatial_data.html",
    "title": "Basic Manipulation",
    "section": "",
    "text": "The terra package in R is a powerful and versatile package for working with geospatial data, including vector and raster data. It provides a wide range of functionality for reading, processing, analyzing, and visualizing spatial data.\nFor more in-depth information and resources on the terra package and spatial data science in R, you can explore the original website Spatial Data Science.\nFirstly load the library to the R space:\n\n# load the library\nlibrary(terra)\nlibrary(tidyverse)"
  },
  {
    "objectID": "dataprocess/spatial_data.html#h2",
    "href": "dataprocess/spatial_data.html#h2",
    "title": "spatial data",
    "section": "",
    "text": "About this site test2\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "dataset/hydro.html",
    "href": "dataset/hydro.html",
    "title": "Hydro",
    "section": "",
    "text": "About this site test1\n\n1 + 1\n\n[1] 2\n\n\n\n\nAbout this site test2\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "dataset/hydro.html#h2",
    "href": "dataset/hydro.html#h2",
    "title": "Hydro",
    "section": "",
    "text": "About this site test2\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "dataset/meteo.html",
    "href": "dataset/meteo.html",
    "title": "Meteological",
    "section": "",
    "text": "About this site test1\n\n1 + 1\n\n[1] 2\n\n\n\n\nAbout this site test2\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "dataset/meteo.html#h2",
    "href": "dataset/meteo.html#h2",
    "title": "Meteological",
    "section": "",
    "text": "About this site test2\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Modelling/HM_0.html",
    "href": "Modelling/HM_0.html",
    "title": "Modelling",
    "section": "",
    "text": "About this site test1\n\n1 + 1\n\n[1] 2\n\n\n\n\nAbout this site test2\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Modelling/HM_0.html#h2",
    "href": "Modelling/HM_0.html#h2",
    "title": "Modelling",
    "section": "",
    "text": "About this site test2\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "dataprocess/data_workspace.html",
    "href": "dataprocess/data_workspace.html",
    "title": "Data in Workspace",
    "section": "",
    "text": "More Details in R for Data Science (2e)"
  },
  {
    "objectID": "dataprocess/data_workspace.html#coding-basics",
    "href": "dataprocess/data_workspace.html#coding-basics",
    "title": "Data in Workspace",
    "section": "Coding basics",
    "text": "Coding basics\n\nmath calculations:\n\n‘+’\n‘-’\n’*’\n‘/’\nTrigonometric functions\n\n\nRPython\n\n\n\n1 / 200 * 30\n\n[1] 0.15\n\n(59 + 73 - 2) / 3\n\n[1] 43.33333\n\n3^2\n\n[1] 9\n\nsin(pi / 2) # pi as Const number in R\n\n[1] 1\n\n\n\n\n\n\n\n\n\n\nCreate new objects\n\nRPython\n\n\nCreate new objects with the assignment operator &lt;-:\n\n# \"&lt;-\" special in R \n\na &lt;- 1 / 200 * 30\nb &lt;- a + 1\n\n\n\n\n\n\n\n\n\nNaming rules\n\nRPython\n\n\n\nmust start with a letter\ncan only contain letters, numbers, underscores _, and dot .\ncase-sensitive (age, Age and AGE are three different variables)\ncannot be any of the Reserved Words\n\nTRUE FALSE\nNULL Inf NaN NA NA_real NA_complex_ NA_character_\nif else\nfor while repeat\nnext break\nfunction\nin\n\n\n\n\n\n\n\n\n\n\nLegal\n\n\n\ni_use_snake_case\notherPeopleUseCamelCase\nsome.people.use.periods\naFew.People_RENOUNCEconvention6\n\n\n\n\n\n\n\n\n\n\n\nIllegal\n\n\n\n_start_with_underscores\n1_start_with_number\nif\ncontain sapce\ncontain-other+charater\n\n\n\n\nmore Reserved Words in:\nhelp(\"reserved\")\n\n\n\nmust start with a letter or the underscore character _\ncan only contain letters, numbers, and underscores _\ncase-sensitive (age, Age and AGE are three different variables)\ncannot be any of the Python keywords (35 keywors in Python 3.8)\n\nTrue False\nNone\nif else elif\nfor while repeat\ntry break continue finally\ndef\nin and or not\nreturn\n\n\n\n\n\n\n\n\n\n\nLegal\n\n\n\ni_use_snake_case\n_start_with_underscores\notherPeopleUseCamelCase\naFew_People_RENOUNCEconvention6\n\n\n\n\n\n\n\n\n\n\n\nIllegal\n\n\n\nwant.contain.dot\n1_start_with_number\nif\ncontain sapce\ncontain-other+charater\n\n\n\n\nMore Keywords in:\n&gt;&gt;&gt; help(\"keywords\")\n\n\n\n\nProgramming Naming Conventions\n\nCamel Case\n\nEach word, except the first, starts with a capital letter:\nmyVariableName\n\nPascal Case\n\nEach word starts with a capital letter:\nMyVariableName\n\nSnake Case\n\nEach word is separated by an underscore character:\nmy_variable_name"
  },
  {
    "objectID": "dataprocess/data_workspace.html#function",
    "href": "dataprocess/data_workspace.html#function",
    "title": "Data in Workspace",
    "section": "Function",
    "text": "Function\n\nCalling\n\nRPython\n\n\nCalling Syntax:\n\nfunction_name(argument1 = value1, argument2 = value2, ...)\n\nWe can also check the arguments and other information with:\n?seq\nTry using seq(), which makes regular sequences of numbers:\n\nseq(from = 1, to = 10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nWe often omit the names of the first several arguments in function calls, so we can rewrite this as follows:\n\nseq(1, 10)\n\n [1]  1  2  3  4  5  6  7  8  9 10"
  },
  {
    "objectID": "dataprocess/basic_r_python.html",
    "href": "dataprocess/basic_r_python.html",
    "title": "R & Python Basic",
    "section": "",
    "text": "More Details of R in R for Data Science (2e) and Advanced R\nMore Details of Python in Automate the Boring Stuff with Python and W3 School Python\nThis article serves as a brief introduction to the fundamental coding aspects of both R and Python. It provides a first impression of these scripting languages. For a more comprehensive understanding and in-depth techniques related to both languages, you are encouraged to explore the website mentioned above. The content here is primarily a condensed compilation of information from the provided links, aimed at facilitating a comparison between R and Python.\nData and Functions are the two essential components of every programming language, especially in the context of data science and data processing. They can be likened to nouns and verbs in natural languages. Data describes information, while Functions define actions for manipulating that data.\nThis article is divided into two main sections: Data (Section 1) and Coding (Section 2).\nIn the Data section, we will explore:\nIn the Coding section, we will delve into three key aspects:\nThe above five elements can be considered as the most fundamental elements of every scripting language. Additionally, we will explore object creation and naming in a section called ‘New Objects’ (Section 3). Objects can encompass functions and variables, further enriching our understanding of scripting.\nThis article will provide a solid introduction to the core concepts in programming, laying the groundwork for further exploration in both R and Python.\nOverview:"
  },
  {
    "objectID": "dataprocess/basic_r_python.html#naming-rules",
    "href": "dataprocess/basic_r_python.html#naming-rules",
    "title": "R & Python Basic",
    "section": "3.1 Naming rules",
    "text": "3.1 Naming rules\n\nRPython\n\n\n\nmust start with a letter\ncan only contain letters, numbers, underscores _, and dot .\ncase-sensitive (age, Age and AGE are three different variables)\ncannot be any of the Reserved Words\n\nTRUE FALSE\nNULL Inf NaN NA NA_real NA_complex_ NA_character_\nif else\nfor while repeat\nnext break\nfunction\nin\n\n\n\n\n\n\n\n\n\n\nLegal\n\n\n\ni_use_snake_case\notherPeopleUseCamelCase\nsome.people.use.periods\naFew.People_RENOUNCEconvention6\n\n\n\n\n\n\n\n\n\n\n\nIllegal\n\n\n\n_start_with_underscores\n1_start_with_number\nif\ncontain sapce\ncontain-other+charater\n\n\n\n\nmore Reserved Words in:\nhelp(\"reserved\")\n\n\n\nmust start with a letter or the underscore character _\ncan only contain letters, numbers, and underscores _\ncase-sensitive (age, Age and AGE are three different variables)\ncannot be any of the Python keywords (35 keywors in Python 3.8)\n\nTrue False\nNone\nif else elif\nfor while repeat\ntry break continue finally\ndef\nin and or not\nreturn\n\n\n\n\n\n\n\n\n\n\nLegal\n\n\n\ni_use_snake_case\n_start_with_underscores\notherPeopleUseCamelCase\naFew_People_RENOUNCEconvention6\n\n\n\n\n\n\n\n\n\n\n\nIllegal\n\n\n\nwant.contain.dot\n1_start_with_number\nif\ncontain sapce\ncontain-other+charater\n\n\n\n\nMore Keywords in:\n\nhelp(\"keywords\")"
  },
  {
    "objectID": "dataprocess/basic_r_python.html#programming-naming-conventions",
    "href": "dataprocess/basic_r_python.html#programming-naming-conventions",
    "title": "R & Python Basic",
    "section": "3.2 Programming Naming Conventions",
    "text": "3.2 Programming Naming Conventions\n\nCamel Case\n\nEach word, except the first, starts with a capital letter:\nmyVariableName\n\nPascal Case\n\nEach word starts with a capital letter:\nMyVariableName\n\nSnake Case\n\nEach word is separated by an underscore character:\nmy_variable_name"
  },
  {
    "objectID": "dataprocess/basic_r_python.html#calling",
    "href": "dataprocess/basic_r_python.html#calling",
    "title": "Coding basics",
    "section": "Calling",
    "text": "Calling\n\nRPython\n\n\nCalling Syntax:\n\nfunction_name(argument1 = value1, argument2 = value2, ...)\n\nTry using seq(), which makes regular sequences of numbers:\n\nseq(from = 1, to = 10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nWe often omit the names of the first several arguments in function calls, so we can rewrite this as follows:\n\nseq(1, 10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nWe can also check the arguments and other information with:\n?seq\nThe “help” windows shows as:\n\n\n\n\nsequence = list(range(1, 11))\nprint(sequence)\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
  },
  {
    "objectID": "dataprocess/basic_r_python.html#define",
    "href": "dataprocess/basic_r_python.html#define",
    "title": "Coding basics",
    "section": "Define",
    "text": "Define\n\nRPython\n\n\nuse the function() keyword:\n\nmy_add1 &lt;- function(x) { # create a function with the name my_function\n  x + 1\n}\n\ncalling the function my_add1:\n\nmy_add1(2)\n\n[1] 3"
  },
  {
    "objectID": "dataprocess/basic_r_python.html#choices",
    "href": "dataprocess/basic_r_python.html#choices",
    "title": "Coding basics",
    "section": "choices",
    "text": "choices\n\nBasic If-Else\n\nRPython\n\n\nThe basic form of an if statement in R is as follows:\n\nif (condition) true_action\nif (condition) true_action else false_action\n\nIf condition is TRUE, true_action is evaluated; if condition is FALSE, the optional false_action is evaluated.\nTypically the actions are compound statements contained within {:\nif returns a value so that you can assign the results:\n\na &lt;- 6\nb &lt;- 8\n\nif (b &gt; a) {\n  cat(\"b is greater than a\\n\")\n} else if (a == b) {\n  cat(\"a and b are equal\\n\")\n} else {\n  cat(\"a is greater than b\\n\")\n}\n\nb is greater than a\n\n\n\n\n\n# if statements\nif condition: \n  true_action\n  \n# if-else\nif condition: \n  true_action \nelse: \n  false_action\n\n\n# if-ifel-else\nif condition1: \n  true_action1 \nelif condition2: \n  true_action2 \nelse: \n  false_action\n\n\na = 6\nb = 8\nif b &gt; a:\n  print(\"b is greater than a\")\nelif a == b:\n  print(\"a and b are equal\")\nelse:\n  print(\"a is greater than b\")\n\nb is greater than a\n\n\n\n\n\n\n\nswitch\n\nRPython\n\n\nClosely related to if is the switch()-statement. It’s a compact, special purpose equivalent that lets you replace code like:\n\nx_option &lt;- function(x) {\n  if (x == \"a\") {\n    \"option 1\"\n  } else if (x == \"b\") {\n    \"option 2\" \n  } else if (x == \"c\") {\n    \"option 3\"\n  } else {\n    stop(\"Invalid `x` value\")\n  }\n}\n\nwith the more succinct:\n\nx_option &lt;- function(x) {\n  switch(x,\n    a = \"option 1\",\n    b = \"option 2\",\n    c = \"option 3\",\n    stop(\"Invalid `x` value\")\n  )\n}\nx_option(\"b\")\n\n[1] \"option 2\"\n\n\nThe last component of a switch() should always throw an error, otherwise unmatched inputs will invisibly return NULL:\n\n\n\nmatch subject:\n    case &lt;pattern_1&gt;:\n        &lt;action_1&gt;\n    case &lt;pattern_2&gt;:\n        &lt;action_2&gt;\n    case &lt;pattern_3&gt;:\n        &lt;action_3&gt;\n    case _:\n        &lt;action_wildcard&gt;\n\n\ndef x_option(x):\n    options = {\n        \"a\": \"option 1\",\n        \"b\": \"option 2\",\n        \"c\": \"option 3\"\n    }\n    return options.get(x, \"Invalid `x` value\")\n\nprint(x_option(\"b\"))\n\noption 2\n\n\n\n\n\n\n\nVectorised if\n\nRPython\n\n\nGiven that if only works with a single TRUE or FALSE, you might wonder what to do if you have a vector of logical values. Handling vectors of values is the job of ifelse(): a vectorised function with test, yes, and no vectors (that will be recycled to the same length):\n\nx &lt;- 1:10\nifelse(x %% 5 == 0, \"XXX\", as.character(x))\n\n [1] \"1\"   \"2\"   \"3\"   \"4\"   \"XXX\" \"6\"   \"7\"   \"8\"   \"9\"   \"XXX\"\n\nifelse(x %% 2 == 0, \"even\", \"odd\")\n\n [1] \"odd\"  \"even\" \"odd\"  \"even\" \"odd\"  \"even\" \"odd\"  \"even\" \"odd\"  \"even\"\n\n\nNote that missing values will be propagated into the output.\nI recommend using ifelse() only when the yes and no vectors are the same type as it is otherwise hard to predict the output type. See https://vctrs.r-lib.org/articles/stability.html#ifelse for additional discussion."
  },
  {
    "objectID": "dataprocess/basic_r_python.html#loops",
    "href": "dataprocess/basic_r_python.html#loops",
    "title": "Coding basics",
    "section": "Loops",
    "text": "Loops\n\nfor-Loops\nA for loop is used for iterating over a sequence (that is either a list, a tuple, a dictionary, a set, or a string). For each item in vector, perform_action is called once; updating the value of item each time.\n\nRPython\n\n\nIn R, for loops are used to iterate over items in a vector. They have the following basic form:\n\nfor (item in vector) perform_action\n\n\nfor (i in 1:3) {\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n\n\n\n\n\nfor item in vector \n  perform_action\n\n\nfor i in range(1, 3):\n  print(i)\n\n1\n2\n\n\n\n\n\n\n\nwhile-Loops\nWith the while loop we can execute a set of statements as long as a condition is TRUE:\n\nRPython\n\n\n\ni &lt;- 1\nwhile (i &lt; 6) {\n  print(i)\n  i &lt;- i + 1\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\n\n\n\ni = 1\nwhile i &lt; 6:\n  print(i)\n  i += 1\n\n1\n2\n3\n4\n5\n\n\n\n\n\n\n\nterminate\n\nRPython\n\n\nThere are two ways to terminate a for loop early:\n\nnext exits the current iteration.\nbreak exits the entire for loop.\n\n\nfor (i in 1:10) {\n  if (i &lt; 3) \n    next\n\n  print(i)\n  \n  if (i &gt;= 5)\n    break\n}\n\n[1] 3\n[1] 4\n[1] 5\n\n\n\n\n\nfor i in range(1, 10):\n    if i &lt; 3:\n        continue\n    \n    print(i)\n    \n    if i &gt;= 5:\n        break\n\n3\n4\n5"
  },
  {
    "objectID": "dataprocess/basic_r_python.html#creating",
    "href": "dataprocess/basic_r_python.html#creating",
    "title": "Coding basics",
    "section": "Creating",
    "text": "Creating\n\nRPython\n\n\nUse the function() keyword:\n\nmy_add1 &lt;- function(x) {\n  x + 1\n}\n\ncalling the function my_add1:\n\nmy_add1(2)\n\n[1] 3\n\n\n\n\n\n\n\n\nTip\n\n\n\nIn R, the return statement is not essential for a function to yield a value as its result. By default, R will return the result of the last command within the function as its output.\n\n\n\n\nIn Python a function is defined using the def keyword:\n\ndef my_add(x):\n  return x + 1\n\ncalling the function my_add1:\n\nprint(my_add(2))\n\n3\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe return statement is essential for a function to yield a value as its result."
  },
  {
    "objectID": "dataprocess/basic_r_python.html#datatype",
    "href": "dataprocess/basic_r_python.html#datatype",
    "title": "R & Python Basic",
    "section": "1.1 Datatype",
    "text": "1.1 Datatype\nA data type of a variable specifies the type of data that is stored inside that variable. In this context, we will just discuss Atomic Variables, which represent fundamental data types. There are six basic atomic data types:\n\nLogical (boolean data type)\n\ncan only have two values: TRUE and FALSE\n\nNumeric (double, float, lang)\n\nrepresents all real numbers with or without decimal values.\n\nInteger\n\nspecifies real values without decimal points.\n\nComplex\n\nis used to specify purely imaginary values\n\nCharacter (string)\n\ndata type is used to specify character or string values in a variable\n\nRaw (bytes)\n\nspecifies values as raw bytes\n\n\n\nRPython\n\n\nIn R, variables do not require explicit declaration with a particular data type. Instead, R is dynamically typed, allowing variables to adapt to the data they contain. You can use the following techniques to work with data types in R:\n\nChecking Data Types: To determine the data type of a variable, you can use the class() function.\nType Conversion: When needed, you can change the data type of a variable using R’s conversion functions, typically prefixed with as..\n\nR’s flexibility in data type handling simplifies programming tasks and allows for efficient data manipulation without the need for explicit type declarations.\n\n# Numeric\nx &lt;- 10.5\nclass(x)\n\n[1] \"numeric\"\n\n# Integer\nx &lt;- 1000L\nclass(x)\n\n[1] \"integer\"\n\n# Complex\nx &lt;- 9i + 3\nclass(x)\n\n[1] \"complex\"\n\n# Character/String\nx &lt;- \"R is exciting\"\nclass(x)\n\n[1] \"character\"\n\n# Logical/Boolean\nx &lt;- TRUE\nclass(x)\n\n[1] \"logical\"\n\n# Convert\ny &lt;- as.numeric(x)\nclass(y)\n\n[1] \"numeric\"\n\n# Raw (bytes)\nx &lt;- charToRaw(\"A\")\nx\n\n[1] 41\n\nclass(x)\n\n[1] \"raw\"\n\n\n\n\nIn Python, variables also do not require explicit declaration with a particular data type. Python is dynamically typed, allowing variables to adapt to the data they contain. You can use the following techniques to work with data types in Python:\n\nChecking Data Types: To determine the data type of a variable, you can use the type() function. It allows you to inspect the current data type of a variable.\nType Conversion: When needed, you can change the data type of a variable in Python using various conversion functions, like float().\n\nPython’s flexibility in data type handling simplifies programming tasks and allows for efficient data manipulation without the need for explicit type declarations.\n\n# Numeric\nx = 10.5\nprint(type(x))\n\n&lt;class 'float'&gt;\n\n# Integer\nx = 1000\nprint(type(x))\n\n&lt;class 'int'&gt;\n\n# Complex\nx = 9j + 3\nprint(type(x))\n\n&lt;class 'complex'&gt;\n\n# Character/String\nx = \"Python is exciting\"\nprint(type(x))\n\n&lt;class 'str'&gt;\n\n# Logical/Boolean\nx = True\nprint(type(x))\n\n&lt;class 'bool'&gt;\n\n# Convert to Numeric\ny = float(x)\nprint(type(y))\n\n&lt;class 'float'&gt;\n\n# Raw (bytes)\nx = b'A'\nprint(x)\n\nb'A'\n\nprint(type(x))\n\n&lt;class 'bytes'&gt;"
  },
  {
    "objectID": "dataprocess/basic_r_python.html#data-structure",
    "href": "dataprocess/basic_r_python.html#data-structure",
    "title": "R & Python Basic",
    "section": "1.2 Data Structure",
    "text": "1.2 Data Structure\nComparatively, data structures between R and Python tend to exhibit more differences than their data types. However, by incorporating additional libraries like NumPy and pandas, we can access shared data structures which play a vital role in the field of data science.\n\nVector: A set of multiple values (items)\n\nContains items of the same data type or structure\nIndexed: Allows you to get and change items using indices\nAllows duplicates\nChangeable: You can modify, add, and remove items after creation\n\nArray: A multi-dimensional extension of a vector\n\nMatrix: two dimensions\n\nList: A set of multiple values (items)\n\nContains items of different data types or structures\nIndexed: Allows you to get and change items using indices\nAllows duplicates\nChangeable: You can modify, add, and remove items after creation\n\nTable (Data Frame): Tabular data structure\n\nTwo-dimensional objects with rows and columns\nContains elements of several types\nEach column has the same data type\n\n\n\nRPython\n\n\nThe structure of R variable can be checked with str()ucture:\n\n# Create a vector\nvct_Test &lt;- c(1,5,7)\n# View the structure\nstr(vct_Test)\n\n num [1:3] 1 5 7\n\n# Create a array\nary_Test &lt;- array(1:24, c(2,3,4))\n# View the structure\nstr(ary_Test)\n\n int [1:2, 1:3, 1:4] 1 2 3 4 5 6 7 8 9 10 ...\n\n# Create a matrix\nmat_Test &lt;- matrix(1:24, 6, 4)\nmat_Test\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    7   13   19\n[2,]    2    8   14   20\n[3,]    3    9   15   21\n[4,]    4   10   16   22\n[5,]    5   11   17   23\n[6,]    6   12   18   24\n\n# View the structure\nstr(mat_Test)\n\n int [1:6, 1:4] 1 2 3 4 5 6 7 8 9 10 ...\n\n# Create a list\nlst_Test &lt;- list(c(1,3,5), \"abc\", FALSE)\n# View the structure\nstr(lst_Test)\n\nList of 3\n $ : num [1:3] 1 3 5\n $ : chr \"abc\"\n $ : logi FALSE\n\n# Create a table (data frame)\ndf_Test &lt;- data.frame(name = c(\"Bob\", \"Tom\"), age = c(12, 13))\ndf_Test\n\n  name age\n1  Bob  12\n2  Tom  13\n\n# View the structure\nstr(df_Test)\n\n'data.frame':   2 obs. of  2 variables:\n $ name: chr  \"Bob\" \"Tom\"\n $ age : num  12 13\n\n\n\n\nIn Python, the structure of a variable is treated as the data type, and you can confirm it using the type() function.\nIt’s important to note that some of the most commonly used data structures, such as arrays and data frames (tables), are not part of the core Python language itself. Instead, they are provided by two popular libraries: numpy and pandas.\n\nimport numpy as np\nimport pandas as pd\n\n# Create a vector (list in Python)\nvct_Test = [1, 5, 7]\n# View the structure\nprint(type(vct_Test))\n\n&lt;class 'list'&gt;\n\n# Create a 3D array (NumPy ndarray)\nary_Test = np.arange(1, 25).reshape((2, 3, 4))\n# View the structure\nprint(type(ary_Test))\n\n&lt;class 'numpy.ndarray'&gt;\n\n# Create a matrix (NumPy ndarray)\nmat_Test = np.arange(1, 25).reshape((6, 4))\nprint(type(mat_Test))\n\n&lt;class 'numpy.ndarray'&gt;\n\n# Create a list\nlst_Test = [[1, 3, 5], \"abc\", False]\n# View the structure\nprint(type(lst_Test))\n\n&lt;class 'list'&gt;\n\n# Create a table (pandas DataFrame)\ndf_Test = pd.DataFrame({\"name\": [\"Bob\", \"Tom\"], \"age\": [12, 13]})\nprint(type(df_Test))\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n\nprint(df_Test)\n\n  name  age\n0  Bob   12\n1  Tom   13\n\n\nPython offers several original data structures, including:\n\nTuples: Tuples are ordered collections of elements, similar to lists, but unlike lists, they are immutable, meaning their elements cannot be changed after creation. Tuples are often used to represent fixed collections of items.\nSets: Sets are unordered collections of unique elements. They are valuable for operations that require uniqueness, such as finding unique values in a dataset or performing set-based operations like unions and intersections.\nDictionaries: Dictionaries, also known as dicts, are collections of key-value pairs. They are used to store data in a structured and efficient manner, allowing quick access to values using their associated keys.\n\nWhile these data structures may not be as commonly used in data manipulation and calculations as arrays and data frames, they have unique features and use cases that can be valuable in various programming scenarios."
  },
  {
    "objectID": "dataprocess/basic_r_python.html#math-calculations",
    "href": "dataprocess/basic_r_python.html#math-calculations",
    "title": "R & Python Basic",
    "section": "2.1 math calculations",
    "text": "2.1 math calculations\n\n‘+’ ‘-’ ’*’ ‘/’\nExponent, Logarithm\nTrigonometric functions\nLinear algebra, Matrix multiplication\n\n\nRPython\n\n\n\n1 / 200 * 30\n\n[1] 0.15\n\n(59 + 73 - 2) / 3\n\n[1] 43.33333\n\n3^2\n\n[1] 9\n\nsin(pi / 2) # pi as Const number in R\n\n[1] 1\n\n\n\n\n\nprint(1 / 200 * 30)\n\n0.15\n\nprint((59 + 73 - 2) / 3)\n\n43.333333333333336\n\nprint(3**2)\n\n9\n\nimport math\nprint(math.sin(math.pi/2))\n\n1.0"
  },
  {
    "objectID": "dataprocess/basic_r_python.html#control-flow",
    "href": "dataprocess/basic_r_python.html#control-flow",
    "title": "R & Python Basic",
    "section": "2.2 Control flow",
    "text": "2.2 Control flow\nThere are two primary tools of control flow: choices and loops.\n\nChoices, like if statements calls, allow you to run different code depending on the input.\nLoops, like for and while, allow you to repeatedly run code, typically with changing options.\n\n\n2.2.1 choices\n\n2.2.1.1 Basic If-Else\n\nRPython\n\n\nThe basic form of an if statement in R is as follows:\n\nif (condition) {\n  true_action\n}\nif (condition) {\n  true_action\n} else {\n  false_action\n}\n\nIf condition is TRUE, true_action is evaluated; if condition is FALSE, the optional false_action is evaluated.\nTypically the actions are compound statements contained within {:\nif returns a value so that you can assign the results:\n\na &lt;- 6\nb &lt;- 8\n\nif (b &gt; a) {\n  cat(\"b is greater than a\\n\")\n} else if (a == b) {\n  cat(\"a and b are equal\\n\")\n} else {\n  cat(\"a is greater than b\\n\")\n}\n\nb is greater than a\n\n\n\n\n\n# if statements\nif condition: \n  true_action\n  \n# if-else\nif condition: \n  true_action \nelse: \n  false_action\n\n\n# if-ifel-else\nif condition1: \n  true_action1 \nelif condition2: \n  true_action2 \nelse: \n  false_action\n\n\na = 6\nb = 8\nif b &gt; a:\n  print(\"b is greater than a\")\nelif a == b:\n  print(\"a and b are equal\")\nelse:\n  print(\"a is greater than b\")\n\nb is greater than a\n\n\n\n\n\n\n\n2.2.1.2 switch\n\nRPython\n\n\nClosely related to if is the switch()-statement. It’s a compact, special purpose equivalent that lets you replace code like:\n\nx_option &lt;- function(x) {\n  if (x == \"a\") {\n    \"option 1\"\n  } else if (x == \"b\") {\n    \"option 2\" \n  } else if (x == \"c\") {\n    \"option 3\"\n  } else {\n    stop(\"Invalid `x` value\")\n  }\n}\n\nwith the more succinct:\n\nx_option &lt;- function(x) {\n  switch(x,\n    a = \"option 1\",\n    b = \"option 2\",\n    c = \"option 3\",\n    stop(\"Invalid `x` value\")\n  )\n}\nx_option(\"b\")\n\n[1] \"option 2\"\n\n\nThe last component of a switch() should always throw an error, otherwise unmatched inputs will invisibly return NULL:\n\n\n\nmatch subject:\n    case &lt;pattern_1&gt;:\n        &lt;action_1&gt;\n    case &lt;pattern_2&gt;:\n        &lt;action_2&gt;\n    case &lt;pattern_3&gt;:\n        &lt;action_3&gt;\n    case _:\n        &lt;action_wildcard&gt;\n\n\ndef x_option(x):\n    options = {\n        \"a\": \"option 1\",\n        \"b\": \"option 2\",\n        \"c\": \"option 3\"\n    }\n    return options.get(x, \"Invalid `x` value\")\n\nprint(x_option(\"b\"))\n\noption 2\n\n\n\n\n\n\n\n2.2.1.3 Vectorised if\n\nRPython\n\n\nGiven that if only works with a single TRUE or FALSE, you might wonder what to do if you have a vector of logical values. Handling vectors of values is the job of ifelse(): a vectorised function with test, yes, and no vectors (that will be recycled to the same length):\n\nx &lt;- 1:10\nifelse(x %% 5 == 0, \"XXX\", as.character(x))\n\n [1] \"1\"   \"2\"   \"3\"   \"4\"   \"XXX\" \"6\"   \"7\"   \"8\"   \"9\"   \"XXX\"\n\nifelse(x %% 2 == 0, \"even\", \"odd\")\n\n [1] \"odd\"  \"even\" \"odd\"  \"even\" \"odd\"  \"even\" \"odd\"  \"even\" \"odd\"  \"even\"\n\n\nNote that missing values will be propagated into the output.\nI recommend using ifelse() only when the yes and no vectors are the same type as it is otherwise hard to predict the output type. See https://vctrs.r-lib.org/articles/stability.html#ifelse for additional discussion.\n\n\n\n\n\n\n\n\n\n2.2.2 Loops\n\n2.2.2.1 for-Loops\nA for loop is used for iterating over a sequence (that is either a list, a tuple, a dictionary, a set, or a string). For each item in vector, perform_action is called once; updating the value of item each time.\n\nRPython\n\n\nIn R, for loops are used to iterate over items in a vector. They have the following basic form:\n\nfor (item in vector) perform_action\n\n\nfor (i in 1:3) {\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n\n\n\n\n\nfor item in vector \n  perform_action\n\n\nfor i in range(1, 3):\n  print(i)\n\n1\n2\n\n\n\n\n\n\n\n2.2.2.2 while-Loops\nWith the while loop we can execute a set of statements as long as a condition is TRUE:\n\nRPython\n\n\n\ni &lt;- 1\nwhile (i &lt; 6) {\n  print(i)\n  i &lt;- i + 1\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\n\n\n\ni = 1\nwhile i &lt; 6:\n  print(i)\n  i += 1\n\n1\n2\n3\n4\n5\n\n\n\n\n\n\n\n2.2.2.3 terminate\n\nRPython\n\n\nThere are two ways to terminate a for loop early:\n\nnext exits the current iteration.\nbreak exits the entire for loop.\n\n\nfor (i in 1:10) {\n  if (i &lt; 3) \n    next\n\n  print(i)\n  \n  if (i &gt;= 5)\n    break\n}\n\n[1] 3\n[1] 4\n[1] 5\n\n\n\n\n\nfor i in range(1, 10):\n    if i &lt; 3:\n        continue\n    \n    print(i)\n    \n    if i &gt;= 5:\n        break\n\n3\n4\n5"
  },
  {
    "objectID": "dataprocess/basic_r_python.html#function",
    "href": "dataprocess/basic_r_python.html#function",
    "title": "R & Python Basic",
    "section": "2.3 Function",
    "text": "2.3 Function\nMore details of in Advanced R Chapter 6\nA function is a block of code which only runs when it is called. It can be broken down into three components:\n\nThe formals(), the list of arguments that control how you call the function.\nThe body(), the code inside the function.\nThe environment(), the data structure that determines how the function finds the values associated with the names.\n\nWhile the formals and body are specified explicitly when you create a function, the environment is specified implicitly, based on where you defined the function. This location could be within another package or within the workspace (global environment).\n\nRPython\n\n\nThe function environment always exists, but it is only printed when the function isn’t defined in the global environment.\n\nfct_add &lt;- function(x, y) {\n  # A comment\n  x + y\n}\n\n# Get the formal arguments\nformals(fct_add)\n\n$x\n\n\n$y\n\n# Get the function's source code (body)\nbody(fct_add)\n\n{\n    x + y\n}\n\n# Get the function's global environment (module-level namespace)\nenvironment(fct_add)\n\n&lt;environment: R_GlobalEnv&gt;\n\n\n\n\n\ndef fct_add(x, y):\n    # A comment\n    return x + y\n\n# Get the formal arguments\nprint(fct_add.__code__.co_varnames)\n\n('x', 'y')\n\n# Get the function's source code (body)\nprint(fct_add.__code__.co_code)\n\nb'\\x97\\x00|\\x00|\\x01z\\x00\\x00\\x00S\\x00'\n\n# Get the function's global environment (module-level namespace)\nprint(fct_add.__globals__)\n\n{'__name__': '__main__', '__doc__': None, '__package__': None, '__loader__': &lt;class '_frozen_importlib.BuiltinImporter'&gt;, '__spec__': None, '__annotations__': {}, '__builtins__': &lt;module 'builtins' (built-in)&gt;, 'r': &lt;__main__.R object at 0x000001797F3A9650&gt;, 'x': [2.1, 4.2, 3.3, -5.6, 5.4, -1, 6.7, 7.9], 'y': 1.0, 'np': &lt;module 'numpy' from 'C:\\\\Users\\\\lei\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\PYTHON~1\\\\Lib\\\\site-packages\\\\numpy\\\\__init__.py'&gt;, 'pd': &lt;module 'pandas' from 'C:\\\\Users\\\\lei\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\PYTHON~1\\\\Lib\\\\site-packages\\\\pandas\\\\__init__.py'&gt;, 'vct_Test': [1, 5, 7], 'ary_Test': array([[[ 1,  2,  3,  4],\n        [ 5,  6,  7,  8],\n        [ 9, 10, 11, 12]],\n\n       [[13, 14, 15, 16],\n        [17, 18, 19, 20],\n        [21, 22, 23, 24]]]), 'mat_Test': array([[ 1,  2,  3,  4],\n       [ 5,  6,  7,  8],\n       [ 9, 10, 11, 12],\n       [13, 14, 15, 16],\n       [17, 18, 19, 20],\n       [21, 22, 23, 24]]), 'lst_Test': [[1, 3, 5], 'abc', False], 'df_Test':   name  age\n0  Bob   12\n1  Tom   13, 'a2': array([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]]), 'colnames': ['A', 'B', 'C'], 'a3': array([[[ 1,  2,  3,  4],\n        [ 5,  6,  7,  8],\n        [ 9, 10, 11, 12]],\n\n       [[13, 14, 15, 16],\n        [17, 18, 19, 20],\n        [21, 22, 23, 24]]]), 'df':    x  y  z\n0  1  3  a\n1  2  2  b\n2  3  1  c, 'a': 6, 'b': 8, 'x_a': array([2.1, 4.2, 3.3, 5.4]), 'idx': 0    False\n1     True\n2     True\nName: x, dtype: bool, 'idx_x_gt_1': 0    False\n1     True\n2     True\nName: x, dtype: bool, 'idx_z_eq_a': 0     True\n1    False\n2    False\nName: z, dtype: bool, 'math': &lt;module 'math' (built-in)&gt;, 'x_option': &lt;function x_option at 0x0000017917028CC0&gt;, 'i': 5, 'fct_add': &lt;function fct_add at 0x0000017917035260&gt;}\n\n\n\n\n\n\n2.3.1 Call\n\nRPython\n\n\nCalling Syntax:\n\nfunction_name(argument1 = value1, argument2 = value2, ...)\n\nTry using seq(), which makes regular sequences of numbers:\n\nseq(from = 1, to = 10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nWe often omit the names of the first several arguments in function calls, so we can rewrite this as follows:\n\nseq(1, 10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nWe can also check the arguments and other information with:\n?seq\nThe “help” windows shows as:\n\n\n\nCalling Syntax:\n\nfunction_name(argument1 = value1, argument2 = value2)\n\n\nsequence = list(range(1, 11))\nprint(sequence)\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n\n\n\n\n\n\n2.3.2 Define\n\nRPython\n\n\nUse the function() keyword:\n\nmy_add1 &lt;- function(x) {\n  x + 1\n}\n\ncalling the function my_add1:\n\nmy_add1(2)\n\n[1] 3\n\n\n\n\n\n\n\n\nTip\n\n\n\nIn R, the return statement is not essential for a function to yield a value as its result. By default, R will return the result of the last command within the function as its output.\n\n\n\n\nIn Python a function is defined using the def keyword:\n\ndef my_add(x):\n  return x + 1\n\ncalling the function my_add1:\n\nprint(my_add(2))\n\n3\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe return statement is essential for a function to yield a value as its result."
  },
  {
    "objectID": "dataprocess/basic_r_python.html#naming-conventions",
    "href": "dataprocess/basic_r_python.html#naming-conventions",
    "title": "R & Python Basic",
    "section": "3.2 Naming Conventions",
    "text": "3.2 Naming Conventions\n\nCamel Case\n\nEach word, except the first, starts with a capital letter:\nmyVariableName\n\nPascal Case\n\nEach word starts with a capital letter:\nMyVariableName\n\nSnake Case\n\nEach word is separated by an underscore character:\nmy_variable_name"
  },
  {
    "objectID": "dataprocess/basic_r_python.html#data-manipulate",
    "href": "dataprocess/basic_r_python.html#data-manipulate",
    "title": "R & Python Basic",
    "section": "1.3 Data Manipulate",
    "text": "1.3 Data Manipulate\nData manipulation is the art and science of transforming raw data into a more structured and useful format for analysis, interpretation, and decision-making. It’s a fundamental process in data science, analytics, and database management.\nOperations for creating and managing persistent data elements can be summarized as CRUD:\n\nCreate (Add): The creation of new data elements or records.\nRead: The retrieval and access of existing data elements for analysis or presentation.\nUpdate: The modification or editing of data elements to reflect changes or corrections.\nDelete: The removal or elimination of data elements that are no longer needed or relevant.\n\nAdditionally, subsetting plays a crucial role in data manipulation. Subsetting allows you to extract specific subsets of data based on conditions, criteria, or filters.\nCombining CRUD operations with subsetting provides a powerful toolkit for working with data, ensuring its accuracy, relevance, and utility in various applications, from database management to data analysis.\n\n1.3.1 Subsetting\n\nRPython\n\n\nMore Details in Advanced R: 4 Subsetting.\nR’s subsetting operators are fast and powerful. Mastering them allows you to succinctly perform complex operations in a way that few other languages can match. Subsetting in R is easy to learn but hard to master because you need to internalise a number of interrelated concepts:\n\nThere are six ways to subset atomic vectors.\nThere are three subsetting operators, [[, [, and $.\nSubsetting operators interact differently with different vector types (e.g., atomic vectors, lists, factors, matrices, and data frames).\n\nSubsetting is a natural complement to str(). While str() shows you all the pieces of any object (its structure).\n\n\n\n\n\n\n\n\nTip\n\n\n\nIn Python, indexing starts from 0, not 1.\n\n\n\n\n\n\n1.3.1.1 Vector\n\nRPython\n\n\n\nPositive integers return elements at the specified positions:\n\n\nx &lt;- c(2.1, 4.2, 3.3, 5.4)\n\n# One value\nx[1]\n\n[1] 2.1\n\n# More values\nx[c(1:2, 4)]\n\n[1] 2.1 4.2 5.4\n\n# Duplicate indices will duplicate values\nx[c(1, 1)]\n\n[1] 2.1 2.1\n\n# Real numbers are silently truncated to integers\nx[c(2.1, 2.9)]\n\n[1] 4.2 4.2\n\n\n\nNegative integers exclude elements at the specified positions:\n\n\n# Exclude elements\nx[-c(3, 1)]\n\n[1] 4.2 5.4\n\n\n\n\n\n\n\n\nError\n\n\n\nNote that you can’t mix positive and negative integers in a single subset:\n\n\n\nx[c(-1, 2)]\n\nError in x[c(-1, 2)]: nur Nullen dürfen mit negativen Indizes gemischt werden\n\n\n\n\n\nPositive integers return elements at the specified positions:\n\n\nimport numpy as np\nimport pandas as pd\n\n# Create a NumPy array\nx = np.array([2.1, 4.2, 3.3, 5.4])\n\n# One value\nprint(x[0])\n\n2.1\n\n# More values\nprint(x[np.array([0, 1, 3])])\n\n[2.1 4.2 5.4]\n\n# Duplicate indices will duplicate values\nprint(x[np.array([0, 0])])\n\n[2.1 2.1]\n\n\n\negative indexing to access an array from the end:\n\n\n# One value\nprint(x[-1])\n\n5.4\n\n# More values\nprint(x[-np.array([1, 3])])\n\n[5.4 4.2]\n\n\n\n\n\n\n\n1.3.1.2 Matrices and arrays\n\nRPython\n\n\nThe most common way of subsetting matrices (2D) and arrays (&gt;2D) is a simple generalisation of 1D subsetting: supply a 1D index for each dimension, separated by a comma. Blank subsetting is now useful because it lets you keep all rows or all columns.\n\n# Create a NumPy matrix\na2 &lt;- matrix(1:9, nrow = 3)\n# Rename the columns (equivalent to colnames in R)\ncolnames(a2) &lt;- c(\"A\", \"B\", \"C\")\n# Access a specific element using column name\na2[1, \"A\"]\n\nA \n1 \n\n# Select specific rows with all columns\na2[1:2, ]\n\n     A B C\n[1,] 1 4 7\n[2,] 2 5 8\n\n# columns which are excluded \na2[0, -2]\n\n     A C\n\n# Create a 3D array\na3 &lt;- array(1:24, c(2,3,4))\n# Access a specific element(s), in different dimensions\na3[1,2,2]\n\n[1] 9\n\na3[1,2,]\n\n[1]  3  9 15 21\n\na3[1,,]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    7   13   19\n[2,]    3    9   15   21\n[3,]    5   11   17   23\n\n\n\n\nIn Python, the : symbol is used to indicate all elements of a particular dimension or slice. It allows you to select or reference all items along that dimension in a sequence, array, or data structure.\n\nimport numpy as np\n\n# Create a NumPy matrix\na2 = np.array([[1, 2, 3],\n               [4, 5, 6],\n               [7, 8, 9]])\n\n# Rename the columns (equivalent to colnames in R)\ncolnames = [\"A\", \"B\", \"C\"]\n\n# Access a specific element using column name\nprint(a2[0, colnames.index(\"A\")])\n\n1\n\n# Select the first two rows\nprint(a2[0:2, :])\n\n[[1 2 3]\n [4 5 6]]\n\n# Create a NumPy 3D array\na3 = np.arange(1, 25).reshape((2, 3, 4))\n\n# Access a specific element in the 3D array\nprint(a3[0, 1, 1])\n\n6\n\nprint(a3[0, 1, :])\n\n[5 6 7 8]\n\nprint(a3[0, :, :])\n\n[[ 1  2  3  4]\n [ 5  6  7  8]\n [ 9 10 11 12]]\n\n\n\n\n\n\n\n1.3.1.3 Data frames and tibbles\n\nRPython\n\n\nData frames have the characteristics of both lists and matrices:\n\nWhen subsetting with a single index, they behave like lists and index the columns, so df[1:2] selects the first two columns.\nWhen subsetting with two indices, they behave like matrices, so df[1:3, ] selects the first three rows (and all the columns)[^python-dims].\n\n\n# Create a DataFrame\ndf &lt;- data.frame(x = 1:3, y = 3:1, z = letters[1:3])\n\n# Select rows\ndf[df$x == 2, ]\n\n  x y z\n2 2 2 b\n\ndf[c(1, 3), ]\n\n  x y z\n1 1 3 a\n3 3 1 c\n\n# There are two ways to select columns from a data frame\n# Like a list\ndf[c(\"x\", \"z\")]\n\n  x z\n1 1 a\n2 2 b\n3 3 c\n\n# Like a matrix\ndf[, c(\"x\", \"z\")]\n\n  x z\n1 1 a\n2 2 b\n3 3 c\n\n# There's an important difference if you select a single \n# column: matrix subsetting simplifies by default, list \n# subsetting does not.\nstr(df[\"x\"])\n\n'data.frame':   3 obs. of  1 variable:\n $ x: int  1 2 3\n\nstr(df[, \"x\"])\n\n int [1:3] 1 2 3\n\n\n\n\nMore detail about Function pandas.Seies.iloc() and pandas.Seies.loc() in pandas document\n\nloc gets rows (and/or columns) with particular labels.\niloc gets rows (and/or columns) at integer locations.\n\n\nimport pandas as pd\n\n# Create a DataFrame\ndf = pd.DataFrame({'x': range(1, 4), 'y': range(3, 0, -1), 'z': list('abc')})\n\n# Select rows\nprint(df[df['x'] == 2])\n\n   x  y  z\n1  2  2  b\n\nprint(df.iloc[[0, 2]])\n\n   x  y  z\n0  1  3  a\n2  3  1  c\n\n# Select columns\nprint(df[['x', 'z']])\n\n   x  z\n0  1  a\n1  2  b\n2  3  c\n\n# Select columns like a DataFrame\nprint(df.loc[:, ['x', 'z']])\n\n   x  z\n0  1  a\n1  2  b\n2  3  c\n\n# Select a single column as a Series (simplifies by default)\nprint(df['x'])\n\n0    1\n1    2\n2    3\nName: x, dtype: int64\n\n# Select a single column as a DataFrame (does not simplify)\nprint(df[['x']])\n\n   x\n0  1\n1  2\n2  3\n\n\n\n\n\n\n\n1.3.1.4 List\n\nRPython"
  },
  {
    "objectID": "dataprocess/basic_r_python.html#math",
    "href": "dataprocess/basic_r_python.html#math",
    "title": "R & Python Basic",
    "section": "2.1 Math",
    "text": "2.1 Math\n\n‘+’ ‘-’ ’*’ ‘/’\nExponent, Logarithm\nTrigonometric functions\nLinear algebra, Matrix multiplication\n\n\nRPython\n\n\n\n1 / 200 * 30\n\n[1] 0.15\n\n(59 + 73 - 2) / 3\n\n[1] 43.33333\n\n3^2\n\n[1] 9\n\nsin(pi / 2) # pi as Const number in R\n\n[1] 1\n\n\n\n\n\nprint(1 / 200 * 30)\n\n0.15\n\nprint((59 + 73 - 2) / 3)\n\n43.333333333333336\n\nprint(3**2)\n\n9\n\nimport math\nprint(math.sin(math.pi/2))\n\n1.0"
  },
  {
    "objectID": "dataprocess/basic_r_python.html#subsetting-und-idexing",
    "href": "dataprocess/basic_r_python.html#subsetting-und-idexing",
    "title": "R & Python Basic",
    "section": "1.3 Subsetting und idexing",
    "text": "1.3 Subsetting und idexing\nAdditionally, subsetting plays a crucial role in data manipulation. Subsetting allows you to extract specific subsets of data based on conditions, criteria, or filters.\n\nRPython\n\n\nMore Details in Advanced R: 4 Subsetting.\nR’s subsetting operators are fast and powerful. Mastering them allows you to succinctly perform complex operations in a way that few other languages can match. Subsetting in R is easy to learn but hard to master because you need to internalise a number of interrelated concepts:\n\nThere are six ways to subset atomic vectors.\nThere are three subsetting operators, [[, [, and $.\nSubsetting operators interact differently with different vector types (e.g., atomic vectors, lists, factors, matrices, and data frames).\n\nSubsetting is a natural complement to str(). While str() shows you all the pieces of any object (its structure).\n\n\n\n\n\n\n\n\nTip\n\n\n\nIn Python, indexing starts from 0, not 1.\n\n\n\n\n\n\n1.3.1 Vector\n\nRPython\n\n\n\nPositive integers return elements at the specified positions:\n\n\nx &lt;- c(2.1, 4.2, 3.3, 5.4)\n\n# One value\nx[1]\n\n[1] 2.1\n\n# More values\nx[c(1:2, 4)]\n\n[1] 2.1 4.2 5.4\n\n# Duplicate indices will duplicate values\nx[c(1, 1)]\n\n[1] 2.1 2.1\n\n# Real numbers are silently truncated to integers\nx[c(2.1, 2.9)]\n\n[1] 4.2 4.2\n\n\n\nNegative integers exclude elements at the specified positions:\n\n\n# Exclude elements\nx[-c(3, 1)]\n\n[1] 4.2 5.4\n\n\n\n\n\n\n\n\nError\n\n\n\nNote that you can’t mix positive and negative integers in a single subset:\n\n\n\nx[c(-1, 2)]\n\nError in x[c(-1, 2)]: nur Nullen dürfen mit negativen Indizes gemischt werden\n\n\n\n\n\nPositive integers return elements at the specified positions:\n\n\nimport numpy as np\nimport pandas as pd\n\n# Create a NumPy array\nx = np.array([2.1, 4.2, 3.3, 5.4])\n\n# One value\nprint(x[0])\n\n2.1\n\n# More values\nprint(x[np.array([0, 1, 3])])\n\n[2.1 4.2 5.4]\n\n# Duplicate indices will duplicate values\nprint(x[np.array([0, 0])])\n\n[2.1 2.1]\n\n\n\negative indexing to access an array from the end:\n\n\n# One value\nprint(x[-1])\n\n5.4\n\n# More values\nprint(x[-np.array([1, 3])])\n\n[5.4 4.2]\n\n\n\n\n\n\n\n1.3.2 Matrices and arrays\n\nRPython\n\n\nThe most common way of subsetting matrices (2D) and arrays (&gt;2D) is a simple generalisation of 1D subsetting: supply a 1D index for each dimension, separated by a comma. Blank subsetting is now useful because it lets you keep all rows or all columns.\n\n# Create a NumPy matrix\na2 &lt;- matrix(1:9, nrow = 3)\n# Rename the columns (equivalent to colnames in R)\ncolnames(a2) &lt;- c(\"A\", \"B\", \"C\")\n# Access a specific element using column name\na2[1, \"A\"]\n\nA \n1 \n\n# Select specific rows with all columns\na2[1:2, ]\n\n     A B C\n[1,] 1 4 7\n[2,] 2 5 8\n\n# columns which are excluded \na2[0, -2]\n\n     A C\n\n# Create a 3D array\na3 &lt;- array(1:24, c(2,3,4))\n# Access a specific element(s), in different dimensions\na3[1,2,2]\n\n[1] 9\n\na3[1,2,]\n\n[1]  3  9 15 21\n\na3[1,,]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    7   13   19\n[2,]    3    9   15   21\n[3,]    5   11   17   23\n\n\n\n\nIn Python, the : symbol is used to indicate all elements of a particular dimension or slice. It allows you to select or reference all items along that dimension in a sequence, array, or data structure.\n\nimport numpy as np\n\n# Create a NumPy matrix\na2 = np.array([[1, 2, 3],\n               [4, 5, 6],\n               [7, 8, 9]])\n\n# Rename the columns (equivalent to colnames in R)\ncolnames = [\"A\", \"B\", \"C\"]\n\n# Access a specific element using column name\nprint(a2[0, colnames.index(\"A\")])\n\n1\n\n# Select the first two rows\nprint(a2[0:2, :])\n\n[[1 2 3]\n [4 5 6]]\n\n# Create a NumPy 3D array\na3 = np.arange(1, 25).reshape((2, 3, 4))\n\n# Access a specific element in the 3D array\nprint(a3[0, 1, 1])\n\n6\n\nprint(a3[0, 1, :])\n\n[5 6 7 8]\n\nprint(a3[0, :, :])\n\n[[ 1  2  3  4]\n [ 5  6  7  8]\n [ 9 10 11 12]]\n\n\n\n\n\n\n\n1.3.3 Data frames and tibbles\n\nRPython\n\n\nData frames have the characteristics of both lists and matrices:\n\nWhen subsetting with a single index, they behave like lists and index the columns, so df[1:2] selects the first two columns.\nWhen subsetting with two indices, they behave like matrices, so df[1:3, ] selects the first three rows (and all the columns)[^python-dims].\n\n\n# Create a DataFrame\ndf &lt;- data.frame(x = 1:3, y = 3:1, z = letters[1:3])\n\n# Select rows\ndf[df$x == 2, ]\n\n  x y z\n2 2 2 b\n\ndf[c(1, 3), ]\n\n  x y z\n1 1 3 a\n3 3 1 c\n\n# There are two ways to select columns from a data frame\n# Like a list\ndf[c(\"x\", \"z\")]\n\n  x z\n1 1 a\n2 2 b\n3 3 c\n\n# Like a matrix\ndf[, c(\"x\", \"z\")]\n\n  x z\n1 1 a\n2 2 b\n3 3 c\n\n# There's an important difference if you select a single \n# column: matrix subsetting simplifies by default, list \n# subsetting does not.\nstr(df[\"x\"])\n\n'data.frame':   3 obs. of  1 variable:\n $ x: int  1 2 3\n\nstr(df[, \"x\"])\n\n int [1:3] 1 2 3\n\n\n\n\nMore detail about Function pandas.Seies.iloc() and pandas.Seies.loc() in pandas document\n\nloc gets rows (and/or columns) with particular labels.\niloc gets rows (and/or columns) at integer locations.\n\n\nimport pandas as pd\n\n# Create a DataFrame\ndf = pd.DataFrame({'x': range(1, 4), 'y': range(3, 0, -1), 'z': list('abc')})\n\n# Select rows\nprint(df[df['x'] == 2])\n\n   x  y  z\n1  2  2  b\n\nprint(df.iloc[[0, 2]])\n\n   x  y  z\n0  1  3  a\n2  3  1  c\n\n# Select columns\nprint(df[['x', 'z']])\n\n   x  z\n0  1  a\n1  2  b\n2  3  c\n\n# Select columns like a DataFrame\nprint(df.loc[:, ['x', 'z']])\n\n   x  z\n0  1  a\n1  2  b\n2  3  c\n\n# Select a single column as a Series (simplifies by default)\nprint(df['x'])\n\n0    1\n1    2\n2    3\nName: x, dtype: int64\n\n# Select a single column as a DataFrame (does not simplify)\nprint(df[['x']])\n\n   x\n0  1\n1  2\n2  3\n\n\n\n\n\n\n\n1.3.4 List\n\nRPython\n\n\nThere are two other subsetting operators: [[ and $. [[ is used for extracting single items, while x$y is a useful shorthand for x[[\"y\"]].\n[[ is most important when working with lists because subsetting a list with [ always returns a smaller list. To help make this easier to understand we can use a metaphor:\n[[ can return only a single item, you must use it with either a single positive integer or a single string.\n\nx &lt;- list(a = 1:3, b = \"a\", d = 4:6)\n\n# Get the subset \nx[1]\n\n$a\n[1] 1 2 3\n\nstr(x[1])\n\nList of 1\n $ a: int [1:3] 1 2 3\n\nx[1:2]\n\n$a\n[1] 1 2 3\n\n$b\n[1] \"a\"\n\n# Get the element\nx[[1]]\n\n[1] 1 2 3\n\nstr(x[1])\n\nList of 1\n $ a: int [1:3] 1 2 3\n\n# with Label\nx$a\n\n[1] 1 2 3\n\nx[[\"a\"]]\n\n[1] 1 2 3\n\n\n\n\nIn Python there are no effectiv ways to create a items named list. It can always get the element of the list but not a subset of the list.\nIn Python, there are no effective ways to create items with named elements in a list. While you can access individual elements by their positions, there isn’t a straightforward method to create a subset of the list with named elements.\n\n# Create a Python list with nested lists\nx = [list(range(1, 4)), \"a\", list(range(4, 7))]\n\n# Get the subset (Python list slice)\nprint([x[0]])\n\n[[1, 2, 3]]\n\n# Get the element using list indexing\nprint(x[0])\n\n[1, 2, 3]\n\nprint(type(x[0]))\n\n&lt;class 'list'&gt;\n\n\nHowever, dictionaries in Python excel in this regard, as they allow you to assign and access elements using user-defined keys, providing a more efficient way to work with named elements and subsets of data.\n\n# Create a dictionary with labels\nx = {\"a\": list(range(1, 4)), \"b\": \"a\", \"d\": list(range(4, 7))}\n\n\n# Get the element using dictionary indexing\nprint(x[\"a\"])\n\n[1, 2, 3]\n\n# Access an element with a label\nprint(x[\"a\"])\n\n[1, 2, 3]\n\nprint(x.get(\"a\"))\n\n[1, 2, 3]\n\nprint(type(x[\"a\"]))\n\n&lt;class 'list'&gt;"
  },
  {
    "objectID": "dataprocess/basic_r_python.html#data-crud",
    "href": "dataprocess/basic_r_python.html#data-crud",
    "title": "R & Python Basic",
    "section": "1.3 Data CRUD",
    "text": "1.3 Data CRUD\nData manipulation is the art and science of transforming raw data into a more structured and useful format for analysis, interpretation, and decision-making. It’s a fundamental process in data science, analytics, and database management.\nOperations for creating and managing persistent data elements can be summarized as CRUD:\n\nCreate (Add): The creation of new data elements or records.\nRead: The retrieval and access of existing data elements for analysis or presentation.\nUpdate: The modification or editing of data elements to reflect changes or corrections.\nDelete: The removal or elimination of data elements that are no longer needed or relevant.\n\nCombining CRUD operations with subsetting provides a powerful toolkit for working with data, ensuring its accuracy, relevance, and utility in various applications, from database management to data analysis.\n\n1.3.1 Create & Add\nMost of the original data we work with is often loaded from external data sources or files. This process will be discussed in detail in the article titled Data Load.\nIn this section, we will focus on the fundamental aspects of creating and adding data, which may have already been mentioned several times in the preceding text.\n\nRPython\n\n\nCreating new objects in R is commonly done using the assignment operator &lt;-.\nWhen it comes to vectors or list, there are two primary methods to append new elements:\n\nc(): allows you to combine the original vector with a new vector or element, effectively extending the vector.\nappend(): enables you to append a new vector or element at a specific location within the original vector.\n\n\n# Automic value\na &lt;- 1 / 200 * 30\n\n# vector\nx_v &lt;- c(2.1, 4.2, 3.3, 5.4)\n# List\nx_l &lt;- list(a = 1:3, b = \"a\", d = 4:6)\n# add new elements\nc(x_v, c(-1,-5.6))\n\n[1]  2.1  4.2  3.3  5.4 -1.0 -5.6\n\nc(x_l, list(e = c(TRUE, FALSE)))\n\n$a\n[1] 1 2 3\n\n$b\n[1] \"a\"\n\n$d\n[1] 4 5 6\n\n$e\n[1]  TRUE FALSE\n\n# append after 2. Element\nappend(x_v, c(-1,-5.6), 2)\n\n[1]  2.1  4.2 -1.0 -5.6  3.3  5.4\n\nappend(x_l, list(e = c(TRUE, FALSE)), 2)\n\n$a\n[1] 1 2 3\n\n$b\n[1] \"a\"\n\n$e\n[1]  TRUE FALSE\n\n$d\n[1] 4 5 6\n\n\nWhen working with 2D matrices or data frames in R, you can use the following functions to add new elements in the row or column dimensions:\n\ncbind(): to combine data frames or matrices by adding new columns.\nrbind(): to combine data frames or matrices by adding new rows.\n\n\n# Create a matrix\nx_m &lt;- matrix(1:9, nrow = 3)\n# data frame\ndf &lt;- data.frame(x = 1:3, y = 3:1, z = letters[1:3])\n# append in colum dimension\ncbind(x_m, -1:-3)\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   -1\n[2,]    2    5    8   -2\n[3,]    3    6    9   -3\n\ncbind(df, k = -1:-3)\n\n  x y z  k\n1 1 3 a -1\n2 2 2 b -2\n3 3 1 c -3\n\n# append in row dimension\nrbind(x_m, -1:-3)\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n[4,]   -1   -2   -3\n\nrbind(df, list(-1, -2, \"z\")) # try with rbind(df, c(-1, -2, \"z\"))\n\n   x  y z\n1  1  3 a\n2  2  2 b\n3  3  1 c\n4 -1 -2 z\n\n\nAdditionally, for both lists and data frames in R, you can use the $ &lt;- operator to add new elements:\n\n# Data frame\ndf &lt;- data.frame(x = 1:3, y = 3:1, z = letters[1:3])\ncbind(df, k = -1:-3)\n\n  x y z  k\n1 1 3 a -1\n2 2 2 b -2\n3 3 1 c -3\n\ndf$k &lt;- -1:-3 # same to df[['k']] &lt;- -1:-3\ndf\n\n  x y z  k\n1 1 3 a -1\n2 2 2 b -2\n3 3 1 c -3\n\n# List\nx_l &lt;- list(a = 1:3, b = \"a\", d = 4:6)\nc(x_l, list(e = c(TRUE, FALSE)))\n\n$a\n[1] 1 2 3\n\n$b\n[1] \"a\"\n\n$d\n[1] 4 5 6\n\n$e\n[1]  TRUE FALSE\n\nx_l$e &lt;- c(TRUE, FALSE) # same to x_l[['e']] &lt;- c(TRUE, FALSE)\nx_l\n\n$a\n[1] 1 2 3\n\n$b\n[1] \"a\"\n\n$d\n[1] 4 5 6\n\n$e\n[1]  TRUE FALSE\n\n\n\n\nCreating new objects in Python is often accomplished using the assignment operator =. When it comes to adding elements to list, there are three primary functions to consider:\n\nappend(): add a single element to the end of a list.\ninsert(): add an element at a specific position within a list.\nextend() same as +: append elements from an iterable (e.g., another list) to the end of an existing list, allowing for the expansion of the list with multiple elements.\n\n\n# Atomic element\na = 1 / 200 * 30\nb = a + 1\nprint(a)\n\n0.15\n\nprint(b)\n\n1.15\n\n# List\nx = [2.1, 4.2, 3.3, 5.4]\n\n# Append on element\nx.append(-1)\nprint(x)\n\n[2.1, 4.2, 3.3, 5.4, -1]\n\n# Insert on eelement\nx.insert(3, -5.6)\nprint(x)\n\n[2.1, 4.2, 3.3, -5.6, 5.4, -1]\n\n# Extend with new list\nx.extend([6.7, 7.9])\nprint(x)\n\n[2.1, 4.2, 3.3, -5.6, 5.4, -1, 6.7, 7.9]\n\n\nWhen working with numpy.array in Python, you can add elements in two primary ways:\n\nappend(): add element or a new numpy array to the end.\ninsert(): insert element or a new numpy array at specific locations within the original numpy array.\n\n\nimport numpy as np\n\n# Create a NumPy array\nx_a = np.array([2.1, 4.2, 3.3, 5.4])\n\nprint(np.append(x_a, -1))\n\n[ 2.1  4.2  3.3  5.4 -1. ]\n\nprint(np.append(x_a, np.array([6.7, 7.9])))\n\n[2.1 4.2 3.3 5.4 6.7 7.9]\n\nprint(np.insert(x_a, 3, -5.6))\n\n[ 2.1  4.2  3.3 -5.6  5.4]\n\nprint(np.insert(x_a, 3, np.array([6.7, 7.9])))\n\n[2.1 4.2 3.3 6.7 7.9 5.4]\n\n\n\n\n\n\n\n1.3.2 Read\nThe read process is essentially a form of subsetting, where you access specific elements or subsets of data using their indexes. The crucial aspect of this operation is how to obtain and utilize these indexes effectively.\n\nRPython\n\n\n\n# Create a DataFrame\ndf &lt;- data.frame(x = 1:3, y = 3:1, z = letters[1:3])\n\n# Access using integer index \ndf[1,2]\n\n[1] 3\n\n# Access using names index\ndf[,\"z\"]\n\n[1] \"a\" \"b\" \"c\"\n\ndf$z\n\n[1] \"a\" \"b\" \"c\"\n\n# Access with a value condition\nidx &lt;- which(df$x &gt; 1)\ndf[idx,]\n\n  x y z\n2 2 2 b\n3 3 1 c\n\ndf[idx, \"z\"]\n\n[1] \"b\" \"c\"\n\nidx &lt;- which(df$z == \"a\")\ndf[idx,]\n\n  x y z\n1 1 3 a\n\ndf[idx, 1:2]\n\n  x y\n1 1 3\n\n\n\n\n\nimport pandas as pd\n\n# Create a pandas DataFrame\ndf = pd.DataFrame({'x': range(1, 4), 'y': range(3, 0, -1), 'z': list('abc')})\n\n# Access using integer index (iloc)\nprint(df.iloc[0, 1])\n\n3\n\n# Access using column label\nprint(df['z'])\n\n0    a\n1    b\n2    c\nName: z, dtype: object\n\nprint(df.z)\n\n0    a\n1    b\n2    c\nName: z, dtype: object\n\n# Access with a value condition\nidx = df['x'] &gt; 1\nprint(df[idx])\n\n   x  y  z\n1  2  2  b\n2  3  1  c\n\nprint(df[df['z'] == 'a'])\n\n   x  y  z\n0  1  3  a\n\nprint(df[df['z'] == 'a'][['x', 'y']])\n\n   x  y\n0  1  3\n\n\n\n\n\n\n\n1.3.3 Update\nThe update operation builds upon the principles of reading. It involves replacing an existing value with a new one, but with certain constraints. The new value must have the same data type, size, and structure as the original value. This ensures data consistency and integrity when modifying data elements. About “data type” it is not so strength, somtimes it is chanable if you replace the whol e.g. colums in data frame.\nIt’s important to note that the concept of ‘data type’ isn’t always rigid. There are cases where data types can change, particularly when replacing entire columns in a data frame, for instance. While data types typically define the expected format and behavior of data, specific operations and transformations may lead to changes in data types to accommodate new values or structures.\n\nRPython\n\n\n\n# Create a DataFrame\ndf &lt;- data.frame(x = 1:3, y = 3:1, z = letters[1:3])\ndf\n\n  x y z\n1 1 3 a\n2 2 2 b\n3 3 1 c\n\n# Update using integer index \ndf[1,2] &lt;- 0\ndf\n\n  x y z\n1 1 0 a\n2 2 2 b\n3 3 1 c\n\n# Update using names index\ndf[2,\"z\"] &lt;- \"lk\"\ndf\n\n  x y  z\n1 1 0  a\n2 2 2 lk\n3 3 1  c\n\n# Update with a value condition\nidx &lt;- which(df$x &gt; 1)\ndf[idx, \"z\"] &lt;- \"bg1\"\ndf\n\n  x y   z\n1 1 0   a\n2 2 2 bg1\n3 3 1 bg1\n\nidx &lt;- which(df$z == \"a\")\ndf[idx,] &lt;- c(-1, -5, \"new_a\")\ndf\n\n   x  y     z\n1 -1 -5 new_a\n2  2  2   bg1\n3  3  1   bg1\n\n\n\n\n\nimport pandas as pd\n\n# Create a pandas DataFrame\ndf = pd.DataFrame({'x': range(1, 4), 'y': range(3, 0, -1), 'z': list('abc')})\nprint(df)\n\n   x  y  z\n0  1  3  a\n1  2  2  b\n2  3  1  c\n\n# Update using integer index\ndf.iat[0, 1] = 0\nprint(df)\n\n   x  y  z\n0  1  0  a\n1  2  2  b\n2  3  1  c\n\n# Update using column label and row index\ndf.at[1, 'z'] = \"lk\"\nprint(df)\n\n   x  y   z\n0  1  0   a\n1  2  2  lk\n2  3  1   c\n\n# Update with a value condition\nidx_x_gt_1 = df['x'] &gt; 1\ndf.loc[idx_x_gt_1, 'z'] = \"bg1\"\nprint(df)\n\n   x  y    z\n0  1  0    a\n1  2  2  bg1\n2  3  1  bg1\n\nidx_z_eq_a = df['z'] == 'a'\ndf.loc[idx_z_eq_a] = [-1, -5, \"new_a\"]\nprint(df)\n\n   x  y      z\n0 -1 -5  new_a\n1  2  2    bg1\n2  3  1    bg1\n\n\n\n\n\n\n\n1.3.4 Delete\n\nRPython\n\n\nDeletion in R can be accomplished relatively easily using methods like specifying negative integer indices or setting elements to NULL within a list. However, it’s essential to recognize that there are limitations to deletion operations. For instance, when dealing with multi-dimensional arrays, you cannot delete a single element in the same straightforward manner; instead, you can only delete entire sub-dimensions.\n\n# Create a DataFrame\ndf &lt;- data.frame(x = 1:3, y = 3:1, z = letters[1:3])\ndf\n\n  x y z\n1 1 3 a\n2 2 2 b\n3 3 1 c\n\n# Delete using negative integer index \ndf[,-2]\n\n  x z\n1 1 a\n2 2 b\n3 3 c\n\ndf[-2,]\n\n  x y z\n1 1 3 a\n3 3 1 c\n\n# Setting elements to `NULL`\ndf$y &lt;- NULL\ndf\n\n  x z\n1 1 a\n2 2 b\n3 3 c\n\n\n\n\nIn Python is to use the .drop() command to delete the elemnts in datatframe. More details in pandas document\n\ndf = pd.DataFrame({'x': range(1, 4), 'y': range(3, 0, -1), 'z': list('abc')})\nprint(df)\n\n   x  y  z\n0  1  3  a\n1  2  2  b\n2  3  1  c\n\n# Drop columns\nprint(df.drop(['x', 'z'], axis=1))\n\n   y\n0  3\n1  2\n2  1\n\nprint(df.drop(columns=['x', 'y']))\n\n   z\n0  a\n1  b\n2  c\n\n# Drop a row by index\nprint(df.drop([0, 1]))\n\n   x  y  z\n2  3  1  c"
  },
  {
    "objectID": "dataprocess/basic_r_python.html#index-subset",
    "href": "dataprocess/basic_r_python.html#index-subset",
    "title": "R & Python Basic",
    "section": "1.2 Index & subset",
    "text": "1.2 Index & subset\nAdditionally, subsetting plays a crucial role in data manipulation. Subsetting allows you to extract specific subsets of data based on conditions, criteria, or filters.\n\nRPython\n\n\nMore Details in Advanced R: 4 Subsetting.\nR’s subsetting operators are fast and powerful. Mastering them allows you to succinctly perform complex operations in a way that few other languages can match. Subsetting in R is easy to learn but hard to master because you need to internalise a number of interrelated concepts:\n\nThere are six ways to subset atomic vectors.\nThere are three subsetting operators, [[, [, and $.\nSubsetting operators interact differently with different vector types (e.g., atomic vectors, lists, factors, matrices, and data frames).\n\nSubsetting is a natural complement to str(). While str() shows you all the pieces of any object (its structure).\n\n\n\n\n\n\n\n\nTip\n\n\n\nIn Python, indexing starts from 0, not 1.\n\n\n\n\n\n\n1.2.1 Vector\n\nRPython\n\n\n\nPositive integers return elements at the specified positions:\n\n\nx &lt;- c(2.1, 4.2, 3.3, 5.4)\n\n# One value\nx[1]\n\n[1] 2.1\n\n# More values\nx[c(1:2, 4)]\n\n[1] 2.1 4.2 5.4\n\n# Duplicate indices will duplicate values\nx[c(1, 1)]\n\n[1] 2.1 2.1\n\n# Real numbers are silently truncated to integers\nx[c(2.1, 2.9)]\n\n[1] 4.2 4.2\n\n\n\nNegative integers exclude elements at the specified positions:\n\n\n# Exclude elements\nx[-c(3, 1)]\n\n[1] 4.2 5.4\n\n\n\n\n\n\n\n\nNOTE\n\n\n\nNote that you can’t mix positive and negative integers in a single subset:\n\n\n\nx[c(-1, 2)]\n\nError in x[c(-1, 2)]: nur Nullen dürfen mit negativen Indizes gemischt werden\n\n\n\n\n\nPositive integers return elements at the specified positions:\n\n\nimport numpy as np\nimport pandas as pd\n\n# Create a NumPy array\nx = np.array([2.1, 4.2, 3.3, 5.4])\n\n# One value\nprint(x[0])\n\n2.1\n\n# More values\nprint(x[np.array([0, 1, 3])])\n\n[2.1 4.2 5.4]\n\n# Duplicate indices will duplicate values\nprint(x[np.array([0, 0])])\n\n[2.1 2.1]\n\n\n\negative indexing to access an array from the end:\n\n\n# One value\nprint(x[-1])\n\n5.4\n\n# More values\nprint(x[-np.array([1, 3])])\n\n[5.4 4.2]\n\n\n\n\n\n\n\n1.2.2 Matrices and arrays\n\nRPython\n\n\nThe most common way of subsetting matrices (2D) and arrays (&gt;2D) is a simple generalisation of 1D subsetting: supply a 1D index for each dimension, separated by a comma. Blank subsetting is now useful because it lets you keep all rows or all columns.\n\n# Create a matrix\na2 &lt;- matrix(1:9, nrow = 3)\n# Rename the columns (equivalent to colnames in R)\ncolnames(a2) &lt;- c(\"A\", \"B\", \"C\")\n# Access a specific element using column name\na2[1, \"A\"]\n\nA \n1 \n\n# Select specific rows with all columns\na2[1:2, ]\n\n     A B C\n[1,] 1 4 7\n[2,] 2 5 8\n\n# columns which are excluded \na2[0, -2]\n\n     A C\n\n# Create a 3D array\na3 &lt;- array(1:24, c(2,3,4))\n# Access a specific element(s), in different dimensions\na3[1,2,2]\n\n[1] 9\n\na3[1,2,]\n\n[1]  3  9 15 21\n\na3[1,,]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    7   13   19\n[2,]    3    9   15   21\n[3,]    5   11   17   23\n\n\n\n\nIn Python, the : symbol is used to indicate all elements of a particular dimension or slice. It allows you to select or reference all items along that dimension in a sequence, array, or data structure.\n\nimport numpy as np\n\n# Create a NumPy matrix\na2 = np.array([[1, 2, 3],\n               [4, 5, 6],\n               [7, 8, 9]])\n\n# Rename the columns (equivalent to colnames in R)\ncolnames = [\"A\", \"B\", \"C\"]\n\n# Access a specific element using column name\nprint(a2[0, colnames.index(\"A\")])\n\n1\n\n# Select the first two rows\nprint(a2[0:2, :])\n\n[[1 2 3]\n [4 5 6]]\n\n# Create a NumPy 3D array\na3 = np.arange(1, 25).reshape((2, 3, 4))\n\n# Access a specific element in the 3D array\nprint(a3[0, 1, 1])\n\n6\n\nprint(a3[0, 1, :])\n\n[5 6 7 8]\n\nprint(a3[0, :, :])\n\n[[ 1  2  3  4]\n [ 5  6  7  8]\n [ 9 10 11 12]]\n\n\n\n\n\n\n\n1.2.3 Data frames\n\nRPython\n\n\nData frames have the characteristics of both lists and matrices:\n\nWhen subsetting with a single index, they behave like lists and index the columns, so df[1:2] selects the first two columns.\nWhen subsetting with two indices, they behave like matrices, so df[1:3, ] selects the first three rows (and all the columns)[^python-dims].\n\n\n# Create a DataFrame\ndf &lt;- data.frame(x = 1:3, y = 3:1, z = letters[1:3])\n\n# Select rows\ndf[df$x == 2, ]\n\n  x y z\n2 2 2 b\n\ndf[c(1, 3), ]\n\n  x y z\n1 1 3 a\n3 3 1 c\n\n# There are two ways to select columns from a data frame\n# Like a list\ndf[c(\"x\", \"z\")]\n\n  x z\n1 1 a\n2 2 b\n3 3 c\n\n# Like a matrix\ndf[, c(\"x\", \"z\")]\n\n  x z\n1 1 a\n2 2 b\n3 3 c\n\n# There's an important difference if you select a single \n# column: matrix subsetting simplifies by default, list \n# subsetting does not.\nstr(df[\"x\"])\n\n'data.frame':   3 obs. of  1 variable:\n $ x: int  1 2 3\n\nstr(df[, \"x\"])\n\n int [1:3] 1 2 3\n\n\n\n\nMore detail about Function pandas.Seies.iloc() and pandas.Seies.loc() in pandas document\n\nloc gets rows (and/or columns) with particular labels.\niloc gets rows (and/or columns) at integer locations.\n\n\nimport pandas as pd\n\n# Create a DataFrame\ndf = pd.DataFrame({'x': range(1, 4), 'y': range(3, 0, -1), 'z': list('abc')})\n\n# Select rows\nprint(df[df['x'] == 2])\n\n   x  y  z\n1  2  2  b\n\nprint(df.iloc[[0, 2]])\n\n   x  y  z\n0  1  3  a\n2  3  1  c\n\n# Select columns\nprint(df[['x', 'z']])\n\n   x  z\n0  1  a\n1  2  b\n2  3  c\n\n# Select columns like a DataFrame\nprint(df.loc[:, ['x', 'z']])\n\n   x  z\n0  1  a\n1  2  b\n2  3  c\n\n# Select a single column as a Series (simplifies by default)\nprint(df['x'])\n\n0    1\n1    2\n2    3\nName: x, dtype: int64\n\n# Select a single column as a DataFrame (does not simplify)\nprint(df[['x']])\n\n   x\n0  1\n1  2\n2  3\n\n\n\n\n\n\n\n1.2.4 List\n\nRPython\n\n\nThere are two other subsetting operators: [[ and $. [[ is used for extracting single items, while x$y is a useful shorthand for x[[\"y\"]].\n[[ is most important when working with lists because subsetting a list with [ always returns a smaller list. To help make this easier to understand we can use a metaphor:\n[[ can return only a single item, you must use it with either a single positive integer or a single string.\n\nx &lt;- list(a = 1:3, b = \"a\", d = 4:6)\n\n# Get the subset \nx[1]\n\n$a\n[1] 1 2 3\n\nstr(x[1])\n\nList of 1\n $ a: int [1:3] 1 2 3\n\nx[1:2]\n\n$a\n[1] 1 2 3\n\n$b\n[1] \"a\"\n\n# Get the element\nx[[1]]\n\n[1] 1 2 3\n\nstr(x[1])\n\nList of 1\n $ a: int [1:3] 1 2 3\n\n# with Label\nx$a\n\n[1] 1 2 3\n\nx[[\"a\"]]\n\n[1] 1 2 3\n\n\n\n\nIn Python there are no effectiv ways to create a items named list. It can always get the element of the list but not a subset of the list.\nIn Python, there are no effective ways to create items with named elements in a list. While you can access individual elements by their positions, there isn’t a straightforward method to create a subset of the list with named elements.\n\n# Create a Python list with nested lists\nx = [list(range(1, 4)), \"a\", list(range(4, 7))]\n\n# Get the subset (Python list slice)\nprint([x[0]])\n\n[[1, 2, 3]]\n\n# Get the element using list indexing\nprint(x[0])\n\n[1, 2, 3]\n\nprint(type(x[0]))\n\n&lt;class 'list'&gt;\n\n\nHowever, dictionaries in Python excel in this regard, as they allow you to assign and access elements using user-defined keys, providing a more efficient way to work with named elements and subsets of data.\n\n# Create a dictionary with labels\nx = {\"a\": list(range(1, 4)), \"b\": \"a\", \"d\": list(range(4, 7))}\n\n\n# Get the element using dictionary indexing\nprint(x[\"a\"])\n\n[1, 2, 3]\n\n# Access an element with a label\nprint(x[\"a\"])\n\n[1, 2, 3]\n\nprint(x.get(\"a\"))\n\n[1, 2, 3]\n\nprint(type(x[\"a\"]))\n\n&lt;class 'list'&gt;"
  },
  {
    "objectID": "dataprocess/basic_r_python.html#datatypes-structure",
    "href": "dataprocess/basic_r_python.html#datatypes-structure",
    "title": "R & Python Basic",
    "section": "1.1 Datatypes & Structure",
    "text": "1.1 Datatypes & Structure\nIn programming, the concept of datatypes is fundamental. It forms the basis for how we handle and manipulate information in software. The most basic data types, such as integers, numerics, booleans, characters, and bytes, are supported by almost all programming languages. Additionally, there are more complex data types built upon these basics, like strings, which are sequences of characters, and dates, which can be represented as variables of integers and more.\nData structures are equally important, as they determine the organization of data, whether it involves the same data types in multiple dimensions or combinations of different types. Data types and structures are intertwined, serving as the cornerstone for our programming endeavors.\nVariables play a pivotal role in storing data of different types. The choice of data type and structure is critical, as different types and structures enable various operations and functionalities. Therefore, understanding data types and structures is paramount before embarking on data manipulation tasks.\n\n1.1.1 Datatypes\nA data type of a variable specifies the type of data that is stored inside that variable. In this context, we will just discuss Atomic Variables, which represent fundamental data types. There are six basic atomic data types:\n\nLogical (boolean data type)\n\ncan only have two values: TRUE and FALSE\n\nNumeric (double, float, lang)\n\nrepresents all real numbers with or without decimal values.\n\nInteger\n\nspecifies real values without decimal points.\n\nComplex\n\nis used to specify purely imaginary values\n\nCharacter (string)\n\ndata type is used to specify character or string values in a variable\n\nRaw (bytes)\n\nspecifies values as raw bytes\n\n\n\nRPython\n\n\nIn R, variables do not require explicit declaration with a particular data type. Instead, R is dynamically typed, allowing variables to adapt to the data they contain. You can use the following techniques to work with data types in R:\n\nChecking Data Types: To determine the data type of a variable, you can use the class() function.\nType Conversion: When needed, you can change the data type of a variable using R’s conversion functions, typically prefixed with as..\n\nR’s flexibility in data type handling simplifies programming tasks and allows for efficient data manipulation without the need for explicit type declarations.\n\n# Numeric\nx &lt;- 10.5\nclass(x)\n\n[1] \"numeric\"\n\n# Integer\nx &lt;- 1000L\nclass(x)\n\n[1] \"integer\"\n\n# Complex\nx &lt;- 9i + 3\nclass(x)\n\n[1] \"complex\"\n\n# Character/String\nx &lt;- \"R is exciting\"\nclass(x)\n\n[1] \"character\"\n\n# Logical/Boolean\nx &lt;- TRUE\nclass(x)\n\n[1] \"logical\"\n\n# Convert\ny &lt;- as.numeric(x)\nclass(y)\n\n[1] \"numeric\"\n\n# Raw (bytes)\nx &lt;- charToRaw(\"A\")\nx\n\n[1] 41\n\nclass(x)\n\n[1] \"raw\"\n\n\n\n\nIn Python, variables also do not require explicit declaration with a particular data type. Python is dynamically typed, allowing variables to adapt to the data they contain. You can use the following techniques to work with data types in Python:\n\nChecking Data Types: To determine the data type of a variable, you can use the type() function. It allows you to inspect the current data type of a variable.\nType Conversion: When needed, you can change the data type of a variable in Python using various conversion functions, like float().\n\nPython’s flexibility in data type handling simplifies programming tasks and allows for efficient data manipulation without the need for explicit type declarations.\n\n# Numeric\nx = 10.5\nprint(type(x))\n\n&lt;class 'float'&gt;\n\n# Integer\nx = 1000\nprint(type(x))\n\n&lt;class 'int'&gt;\n\n# Complex\nx = 9j + 3\nprint(type(x))\n\n&lt;class 'complex'&gt;\n\n# Character/String\nx = \"Python is exciting\"\nprint(type(x))\n\n&lt;class 'str'&gt;\n\n# Logical/Boolean\nx = True\nprint(type(x))\n\n&lt;class 'bool'&gt;\n\n# Convert to Numeric\ny = float(x)\nprint(type(y))\n\n&lt;class 'float'&gt;\n\n# Raw (bytes)\nx = b'A'\nprint(x)\n\nb'A'\n\nprint(type(x))\n\n&lt;class 'bytes'&gt;\n\n\n\n\n\n\n\n1.1.2 Data Structure\nComparatively, data structures between R and Python tend to exhibit more differences than their data types. However, by incorporating additional libraries like NumPy and pandas, we can access shared data structures which play a vital role in the field of data science.\n\nVector: A set of multiple values (items)\n\nContains items of the same data type or structure\nIndexed: Allows you to get and change items using indices\nAllows duplicates\nChangeable: You can modify, add, and remove items after creation\n\nArray: A multi-dimensional extension of a vector\n\nMatrix: two dimensions\n\nList: A set of multiple values (items)\n\nContains items of different data types or structures\nIndexed: Allows you to get and change items using indices\nAllows duplicates\nChangeable: You can modify, add, and remove items after creation\n\nTable (Data Frame): Tabular data structure\n\nTwo-dimensional objects with rows and columns\nContains elements of several types\nEach column has the same data type\n\n\n\nRPython\n\n\nThe structure of R variable can be checked with str()ucture:\n\n# Create a vector\nvct_Test &lt;- c(1,5,7)\n# View the structure\nstr(vct_Test)\n\n num [1:3] 1 5 7\n\n# Create a array\nary_Test &lt;- array(1:24, c(2,3,4))\n# View the structure\nstr(ary_Test)\n\n int [1:2, 1:3, 1:4] 1 2 3 4 5 6 7 8 9 10 ...\n\n# Create a matrix\nmat_Test &lt;- matrix(1:24, 6, 4)\nmat_Test\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    7   13   19\n[2,]    2    8   14   20\n[3,]    3    9   15   21\n[4,]    4   10   16   22\n[5,]    5   11   17   23\n[6,]    6   12   18   24\n\n# View the structure\nstr(mat_Test)\n\n int [1:6, 1:4] 1 2 3 4 5 6 7 8 9 10 ...\n\n# Create a list\nlst_Test &lt;- list(c(1,3,5), \"abc\", FALSE)\n# View the structure\nstr(lst_Test)\n\nList of 3\n $ : num [1:3] 1 3 5\n $ : chr \"abc\"\n $ : logi FALSE\n\n# Create a table (data frame)\ndf_Test &lt;- data.frame(name = c(\"Bob\", \"Tom\"), age = c(12, 13))\ndf_Test\n\n  name age\n1  Bob  12\n2  Tom  13\n\n# View the structure\nstr(df_Test)\n\n'data.frame':   2 obs. of  2 variables:\n $ name: chr  \"Bob\" \"Tom\"\n $ age : num  12 13\n\n\n\n\nIn Python, the structure of a variable is treated as the data type, and you can confirm it using the type() function.\nIt’s important to note that some of the most commonly used data structures, such as arrays and data frames (tables), are not part of the core Python language itself. Instead, they are provided by two popular libraries: numpy and pandas.\n\nimport numpy as np\nimport pandas as pd\n\n# Create a vector (list in Python)\nvct_Test = [1, 5, 7]\n# View the structure\nprint(type(vct_Test))\n\n&lt;class 'list'&gt;\n\n# Create a 3D array (NumPy ndarray)\nary_Test = np.arange(1, 25).reshape((2, 3, 4))\n# View the structure\nprint(type(ary_Test))\n\n&lt;class 'numpy.ndarray'&gt;\n\n# Create a matrix (NumPy ndarray)\nmat_Test = np.arange(1, 25).reshape((6, 4))\nprint(type(mat_Test))\n\n&lt;class 'numpy.ndarray'&gt;\n\n# Create a list\nlst_Test = [[1, 3, 5], \"abc\", False]\n# View the structure\nprint(type(lst_Test))\n\n&lt;class 'list'&gt;\n\n# Create a table (pandas DataFrame)\ndf_Test = pd.DataFrame({\"name\": [\"Bob\", \"Tom\"], \"age\": [12, 13]})\nprint(type(df_Test))\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n\nprint(df_Test)\n\n  name  age\n0  Bob   12\n1  Tom   13\n\n\nPython offers several original data structures, including:\n\nTuples: Tuples are ordered collections of elements, similar to lists, but unlike lists, they are immutable, meaning their elements cannot be changed after creation. Tuples are often used to represent fixed collections of items.\nSets: Sets are unordered collections of unique elements. They are valuable for operations that require uniqueness, such as finding unique values in a dataset or performing set-based operations like unions and intersections.\nDictionaries: Dictionaries, also known as dicts, are collections of key-value pairs. They are used to store data in a structured and efficient manner, allowing quick access to values using their associated keys.\n\nWhile these data structures may not be as commonly used in data manipulation and calculations as arrays and data frames, they have unique features and use cases that can be valuable in various programming scenarios."
  },
  {
    "objectID": "docs/dataprocess/data_load.html",
    "href": "docs/dataprocess/data_load.html",
    "title": "load data",
    "section": "",
    "text": "About this site test1\n\n1 + 1\n\n[1] 2\n\n\n\n\nAbout this site test2\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "docs/dataprocess/data_load.html#h2",
    "href": "docs/dataprocess/data_load.html#h2",
    "title": "load data",
    "section": "",
    "text": "About this site test2\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "dataprocess/data_load.html",
    "href": "dataprocess/data_load.html",
    "title": "Data Loading",
    "section": "",
    "text": "This Aritcl will show the process to load data from other files. I t will divide into four paties: plain text (read able ASCII), Excel, NetCDF and spatial data.\nOverview:"
  },
  {
    "objectID": "dataprocess/data_load.html#example-file",
    "href": "dataprocess/data_load.html#example-file",
    "title": "Data Loading",
    "section": "1.1 Example File",
    "text": "1.1 Example File\nLet’s start with an example CSV file named Bachum_2763190000100.csv. This file contains pegel discharge data and is sourced from open data available at ELWAS-WEB NRW. You can also access it directly from the internet via Github, just like you would access a local file.\nTake a look:"
  },
  {
    "objectID": "dataprocess/data_load.html#r-library-and-functions",
    "href": "dataprocess/data_load.html#r-library-and-functions",
    "title": "Data Loading",
    "section": "2.2 R library and functions",
    "text": "2.2 R library and functions\nTo load the necessary library, readxl, and access its help documentation, you can visit this link. The readxl::read_excel() function is versatile, as it can read both .xls and .xlsx files and automatically detects the format based on the file extension. Additionally, you have the options of using read_xls() for .xls files and read_xlsx() for .xlsx files. More details in the Page.\n\n# load the library\nlibrary(readxl)\n# The Excel file cannot be read directly from GitHub. You will need to download it to your local machine first\nfn_Pegeln &lt;- \"C:\\\\Lei\\\\HS_Web\\\\data_share/Pegeln_NRW.xlsx\""
  },
  {
    "objectID": "dataprocess/data_load.html#metadata-handel",
    "href": "dataprocess/data_load.html#metadata-handel",
    "title": "Data Loading",
    "section": "1.3 Metadata Handel",
    "text": "1.3 Metadata Handel\nMetadata can vary widely between datasets, so it’s handled separately from the data body.\nThere are three ways to deal with metadata:\n\nDirectly Ignore: This approach involves ignoring metadata when it’s redundant or readily available from other data sources, such as file names or external references.\nExtract from Text: When metadata is crucial but not in table form, you can extract information from text strings. For more information, refer to the section on string manipulation Section 4.\nRead as a Second Table: If metadata is well-organized in a tabular format, it can be read as a separate table to facilitate its use.\n\nIn the Bachum_2763190000100.csv file, you will find that there are 10 lines of metadata, which are well-organized in a tabular format. However, it’s important to note that the consistency in values column varies.\n\n1.3.1 Directly Ignore use grguments skip\n\n# skip = 10\nread_csv2(fn_Bachum, skip = 10, n_max = 10, col_names = FALSE)\n\n# A tibble: 10 × 2\n   X1            X2\n   &lt;chr&gt;      &lt;dbl&gt;\n 1 01.01.1990  20.6\n 2 02.01.1990  19.0\n 3 03.01.1990  17.9\n 4 04.01.1990  16.8\n 5 05.01.1990  16.0\n 6 06.01.1990  14.8\n 7 07.01.1990  14.3\n 8 08.01.1990  14.0\n 9 09.01.1990  14.4\n10 10.01.1990  14.5\n\n\n\n\n1.3.2 Read metadata as table\nWhen directly reading all metadata into one table, you may encounter mixed data types. In the metadata, there are three data types:\n\nNumeric: Examples include Pegelnullpunkt and Einzugsgebiet.\nString: This category covers fields like Name, Pegelnummer, and others.\nDate: Date values are present in columns like Datum von and Datum bis.\n\nIn a data frame (tibble), columns must have the same data type. Consequently, R will automatically convert them to a single data type, which is typically string.\nTo address this situation, you should specify the data type you want to read. For example, to read the date values in lines 4 and 5, you can use the following settings: 1. skip = 3 to skip the first three lines of metadata. 2. n_max = 2 to read the next two lines (lines 4 and 5) as date values.\n\nRPython\n\n\n\n# skip = 3\nread_csv2(fn_Bachum, skip = 3, n_max = 2, col_names = FALSE)\n\n# A tibble: 2 × 2\n  X1        X2        \n  &lt;chr&gt;     &lt;chr&gt;     \n1 Datum von 01.01.1990\n2 Datum bis 31.12.2022\n\n\n\n\n\ndf_bach = pd.read_csv(fn_Bachum, skiprows=3, nrows=2, header=None, delimiter=';', encoding='latin-1')\nprint(df_bach)\n\n           0           1\n0  Datum von  01.01.1990\n1  Datum bis  31.12.2022\n\n\n\n\n\nUnfortunately, R may not always recognize date values correctly, so you may need to perform additional steps for conversion:\n\nAfter Reading: This involves transforming the data from its initial format to the desired date format within your R environment.\nSet the Data Type by Reading: Another approach is to set the data type while reading the data.\n\nMore details in the next section:"
  },
  {
    "objectID": "dataprocess/data_load.html#load-tabular-data",
    "href": "dataprocess/data_load.html#load-tabular-data",
    "title": "Data Loading",
    "section": "1.4 Load tabular data",
    "text": "1.4 Load tabular data\n\nRPython\n\n\nTo read the first 10 lines of metadata, you can use the n_max setting with a value of n_max = 10 in the read_csv2() function.\n\nread_csv2(fn_Bachum, n_max = 10, col_names = FALSE)\n\n# A tibble: 10 × 2\n   X1                          X2             \n   &lt;chr&gt;                       &lt;chr&gt;          \n 1 \"Name\"                      \"Bachum\"       \n 2 \"Pegelnummer\"               \"2763190000100\"\n 3 \"Gew\\xe4sser\"               \"Ruhr\"         \n 4 \"Datum von\"                 \"01.01.1990\"   \n 5 \"Datum bis\"                 \"31.12.2022\"   \n 6 \"Parameter\"                 \"Abfluss\"      \n 7 \"Q Einheit\"                 \"m\\xb3/s\"      \n 8 \"Tagesmittelwerte\"           &lt;NA&gt;          \n 9 \"Pegelnullpunkt [m\\xfcNHN]\" \"146,83\"       \n10 \"Einzugsgebiet [km\\xb2]\"    \"1.532,02\"     \n\n\nAfter dealing with the metadata, we can proceed to load the data body using the readr::read_*() function cluster. Plain text files typically store data in a tabular or matrix format, both of which have at most two dimensions. When using the readr::read_() function, it automatically returns a tibble. If your data in the text file is in matrix format, you can use conversion functions like as.matrix() to transform it into other data structures.\n\n# 1. load\ntb_Read &lt;- read_csv2(fn_Bachum, skip = 10, n_max = 10, col_names = FALSE)\ntb_Read\n\n# A tibble: 10 × 2\n   X1            X2\n   &lt;chr&gt;      &lt;dbl&gt;\n 1 01.01.1990  20.6\n 2 02.01.1990  19.0\n 3 03.01.1990  17.9\n 4 04.01.1990  16.8\n 5 05.01.1990  16.0\n 6 06.01.1990  14.8\n 7 07.01.1990  14.3\n 8 08.01.1990  14.0\n 9 09.01.1990  14.4\n10 10.01.1990  14.5\n\n# 2. convert\ndf_Read &lt;- as.data.frame(tb_Read)\nmat_Read &lt;- as.matrix(tb_Read)\n\ndf_Read\n\n           X1     X2\n1  01.01.1990 20.640\n2  02.01.1990 18.994\n3  03.01.1990 17.949\n4  04.01.1990 16.779\n5  05.01.1990 16.019\n6  06.01.1990 14.817\n7  07.01.1990 14.296\n8  08.01.1990 13.952\n9  09.01.1990 14.403\n10 10.01.1990 14.500\n\nmat_Read\n\n      X1           X2      \n [1,] \"01.01.1990\" \"20.640\"\n [2,] \"02.01.1990\" \"18.994\"\n [3,] \"03.01.1990\" \"17.949\"\n [4,] \"04.01.1990\" \"16.779\"\n [5,] \"05.01.1990\" \"16.019\"\n [6,] \"06.01.1990\" \"14.817\"\n [7,] \"07.01.1990\" \"14.296\"\n [8,] \"08.01.1990\" \"13.952\"\n [9,] \"09.01.1990\" \"14.403\"\n[10,] \"10.01.1990\" \"14.500\"\n\n\n\n\n\ntb_Read = pd.read_csv(fn_Bachum, skiprows=10, nrows=10, header=None, delimiter=';', decimal=',', encoding='latin-1')\nprint(tb_Read)\n\n            0       1\n0  01.01.1990  20.640\n1  02.01.1990  18.994\n2  03.01.1990  17.949\n3  04.01.1990  16.779\n4  05.01.1990  16.019\n5  06.01.1990  14.817\n6  07.01.1990  14.296\n7  08.01.1990  13.952\n8  09.01.1990  14.403\n9  10.01.1990  14.500"
  },
  {
    "objectID": "dataprocess/data_load.html#example-file-1",
    "href": "dataprocess/data_load.html#example-file-1",
    "title": "Data Loading",
    "section": "2.1 Example File",
    "text": "2.1 Example File\nLet’s begin with an example Excel file named Pegeln_NRW.xlsx. This file contains information about measurement stations in NRW (Nordrhein-Westfalen, Germany) and is sourced from open data available at ELWAS-WEB NRW. You can also access it directly from Github.\nTake a look:"
  },
  {
    "objectID": "dataprocess/data_load.html#r-library-and-functions-1",
    "href": "dataprocess/data_load.html#r-library-and-functions-1",
    "title": "Data Loading",
    "section": "2.2 R library and functions",
    "text": "2.2 R library and functions\nTo load the necessary library, readxl, and access its help documentation, you can visit this link. The readxl::read_excel() function is versatile, as it can read both .xls and .xlsx files and automatically detects the format based on the file extension. Additionally, you have the options of using read_xls() for .xls files and read_xlsx() for .xlsx files. More details in the Page.\n\n# load the library\nlibrary(readxl)\n# The Excel file cannot be read directly from GitHub. You will need to download it to your local machine first\nfn_Pegeln &lt;- \"C:\\\\Lei\\\\HS_Web\\\\data_share/Pegeln_NRW.xlsx\""
  },
  {
    "objectID": "dataprocess/data_load.html#load-the-data-body",
    "href": "dataprocess/data_load.html#load-the-data-body",
    "title": "Data Loading",
    "section": "Load the data body",
    "text": "Load the data body\n\n# \ntb_Pegeln &lt;- read_excel(fn_Pegeln)\ntb_Pegeln\n\n# A tibble: 277 × 16\n   Suchergebnisse Pegel.…¹ ...2  ...3  ...4  ...5  ...6  ...7  ...8  ...9  ...10\n   &lt;chr&gt;                   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n 1 \"Suchkriterien:\\n -- \\… &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; \n 2  &lt;NA&gt;                   &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; \n 3  &lt;NA&gt;                   &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; \n 4 \"Name\"                  Pege… Gewä… Betr… Pege… Einz… Q von Q bis NQ    MNQ  \n 5 \"Ahlen\"                 3211… Werse LANU… 73,47 46,62 1975  2013  0     0,07 \n 6 \"Ahmsen\"                4639… Werre LANU… 64,28 593   1963  2022  1,21  2,22 \n 7 \"Ahrhütte-Neuhof\"       2718… Ahr   LANU… 340,… 124   1986  2011  0,22  0,36 \n 8 \"Albersloh\"             3259… Werse LANU… 48,68 321,… 1973  2020  0,12  0,24 \n 9 \"Altena\"                2766… Lenne LANU… 154,… 1.190 1950  2021  1,36  6,48 \n10 \"Altena_Rahmedestraße\"  2766… Rahm… LANU… 157,… 29,6  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; \n# ℹ 267 more rows\n# ℹ abbreviated name: ¹​`Suchergebnisse Pegel.xlsx 14.09.2023 10:01`\n# ℹ 6 more variables: ...11 &lt;chr&gt;, ...12 &lt;chr&gt;, ...13 &lt;chr&gt;, ...14 &lt;chr&gt;,\n#   ...15 &lt;chr&gt;, ...16 &lt;chr&gt;"
  },
  {
    "objectID": "dataprocess/data_load.html#sec-datatype",
    "href": "dataprocess/data_load.html#sec-datatype",
    "title": "Data Loading",
    "section": "1.5 Data type",
    "text": "1.5 Data type\nIn this section, we will work with a custom-made text file that contains various data types and formats. The file consists of three rows, with one of them serving as the header containing column names, and six columns in total.\nLet’s take a look:\n\nActually the function will always guse the dattype for each column, when the data really normally format the function will return the right datatype for the data:\n\nRPython\n\n\n\nread_table(fn_Datatype)\n\n# A tibble: 2 × 6\n    int float_en float_de date_en    date_de    str  \n  &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;date&gt;     &lt;chr&gt;      &lt;chr&gt;\n1     1      0.1 0,1      2023-09-15 15.09.2023 en   \n2     9      9.6 9,6      2023-09-16 16.09.2023 de   \n\n\n\n\n\ndf = pd.read_table(fn_Datatype)\nprint(df)\n\n   int  float_en float_de     date_en     date_de str\n0    1       0.1      0,1  2023-09-15  15.09.2023  en\n1    9       9.6      9,6  2023-09-16  16.09.2023  de\n\nprint(df.dtypes)\n\nint           int64\nfloat_en    float64\nfloat_de     object\ndate_en      object\ndate_de      object\nstr          object\ndtype: object\n\n\n\n\n\nBy default, functions like readr::read_table() in R and pandas.read_table() in Python will attempt to guess data types automatically when reading data. Here’s how this guessing typically works:\n\nIf a column contains only numbers and decimal dots (periods), it will be recognized as numeric (double in R and int or float in Python).\nIf a date is formatted in “Y-M-D” (e.g., “2023-08-27”) or “h:m:s” (e.g., “15:30:00”) formats, it may be recognized as a date or time type. Nur in R\nIf the data type cannot be confidently determined, it is often treated as a string (str in R and object in Python).\n\nThis automatic guessing is convenient, but it’s essential to verify the inferred data types, especially when working with diverse datasets.\n\n1.5.1 Set the Data Type by Reading\nExplicitly setting data types using the col_types (in R) or dtype (in Python) argument can help ensure correct data handling.\n\nRPython\n\n\nTo address the issue of date recognition, you can set the col_types argument, you can use a compact string representation where each character represents one column:\n\nc: Character\ni: Integer\nn: Number\nd: Double\nl: Logical\nf: Factor\nD: Date\nT: Date Time\nt: Time\n?: Guess\n_ or -: Skip\n\nto \"cD\" when reading the data. This informs the function that the first column contains characters (c) and the second column contains Dates (D).\n\nread_table(fn_Datatype, col_types = \"iddDDc\")\n\n# A tibble: 2 × 6\n    int float_en float_de date_en    date_de str  \n  &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;date&gt;     &lt;date&gt;  &lt;chr&gt;\n1     1      0.1       NA 2023-09-15 NA      en   \n2     9      9.6       NA 2023-09-16 NA      de   \n\n\n\nread_table(fn_Datatype, col_types = \"idd?Dc\")\n\n# A tibble: 2 × 6\n    int float_en float_de date_en    date_de str  \n  &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;date&gt;     &lt;date&gt;  &lt;chr&gt;\n1     1      0.1       NA 2023-09-15 NA      en   \n2     9      9.6       NA 2023-09-16 NA      de   \n\n\n\n\nTo set data types when reading data using functions pandas.read_*, you have three main choices by using the dtype parameter:\n\nstr: Specify the data type as a string.\nint: Specify the data type as an integer.\nfloat: Specify the data type as a floating-point number.\n\nHowever, you can also use the dtype parameter with a callable function to perform more advanced type conversions. Some commonly used functions include:\n\npd.to_datetime: Converts a column to datetime format.\npd.to_numeric: Converts a column to numeric (integer or float) format.\npd.to_timedelta: Converts a column to timedelta format.\n\n\n# Define column names and types as a dictionary\ncol_types = {\"X1\": str, \"X2\": pd.to_datetime}\n# Read the CSV file, skip 3 rows, read 2 rows, and specify column names and types\ndf = pd.read_csv(fn_Bachum, skiprows=3, nrows=2, header=None, delimiter=';', names=[\"X1\", \"X2\"], dtype=col_types, encoding='latin-1')\n\n# Display the loaded data\nprint(df)\n\n\nDON’T RUN Error, because data doesn’t match the default format of ‘Y-m-d’.\n\n\n\n\n\nUnfortunately, the default date format in R and Python may not work for German-style dates like “d.m.Y” as R and Python primarily recognizes the “Y-m-d” format.\n\n\n\n1.5.2 After Reading\nTo address this issue, you can perform date conversions after reading the data:\n\nRPython\n\n\nUsing function as.Date() and specify the date format using the format argument, such as format = \"%d.%m.%Y\".\n\ndf_Date &lt;- read_csv2(fn_Bachum, skip = 3, n_max = 2, col_names = FALSE)\ndf_Date$X2 &lt;- df_Date$X2 |&gt; as.Date(format = \"%d.%m.%Y\")\ndf_Date\n\n# A tibble: 2 × 2\n  X1        X2        \n  &lt;chr&gt;     &lt;date&gt;    \n1 Datum von 1990-01-01\n2 Datum bis 2022-12-31\n\n\n\n\n\ndf_Date = pd.read_csv(fn_Bachum, skiprows=3, nrows=2, header=None, delimiter=';', encoding='latin-1')\n\n# Display the loaded data\nprint(df_Date)\n\n           0           1\n0  Datum von  01.01.1990\n1  Datum bis  31.12.2022\n\n# 2. Convert the second column (X2) to a date format\ndf_Date[1] = pd.to_datetime(df_Date[1], format='%d.%m.%Y')\n\n# Display the DataFrame with the second column converted to date format\nprint(df_Date)\n\n           0          1\n0  Datum von 1990-01-01\n1  Datum bis 2022-12-31"
  },
  {
    "objectID": "dataprocess/data_load.html#load-tabular-data-1",
    "href": "dataprocess/data_load.html#load-tabular-data-1",
    "title": "Data Loading",
    "section": "2.3 Load tabular data",
    "text": "2.3 Load tabular data\nSimilar to plain text files, metadata is often provided before the data body in Excel files. In Excel, each cell can be assigned a specific data type, while in R tables (data.frame or tibble), every column must have the same data type. This necessitates separate handling of metadata and data body to ensure that the correct data types are maintained.\nUnlike plain text files where we can only select lines to load, Excel allows us to define coordinates to access a specific celles-box wherever they are located.\n\n2.3.1 First try without any setting\n\n# try without setting\ntb_Pegeln &lt;- read_excel(fn_Pegeln)\ntb_Pegeln\n\n# A tibble: 277 × 16\n   Suchergebnisse Pegel.…¹ ...2  ...3  ...4  ...5  ...6  ...7  ...8  ...9  ...10\n   &lt;chr&gt;                   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n 1 \"Suchkriterien:\\n -- \\… &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; \n 2  &lt;NA&gt;                   &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; \n 3  &lt;NA&gt;                   &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; \n 4 \"Name\"                  Pege… Gewä… Betr… Pege… Einz… Q von Q bis NQ    MNQ  \n 5 \"Ahlen\"                 3211… Werse LANU… 73,47 46,62 1975  2013  0     0,07 \n 6 \"Ahmsen\"                4639… Werre LANU… 64,28 593   1963  2022  1,21  2,22 \n 7 \"Ahrhütte-Neuhof\"       2718… Ahr   LANU… 340,… 124   1986  2011  0,22  0,36 \n 8 \"Albersloh\"             3259… Werse LANU… 48,68 321,… 1973  2020  0,12  0,24 \n 9 \"Altena\"                2766… Lenne LANU… 154,… 1.190 1950  2021  1,36  6,48 \n10 \"Altena_Rahmedestraße\"  2766… Rahm… LANU… 157,… 29,6  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; \n# ℹ 267 more rows\n# ℹ abbreviated name: ¹​`Suchergebnisse Pegel.xlsx 14.09.2023 10:01`\n# ℹ 6 more variables: ...11 &lt;chr&gt;, ...12 &lt;chr&gt;, ...13 &lt;chr&gt;, ...14 &lt;chr&gt;,\n#   ...15 &lt;chr&gt;, ...16 &lt;chr&gt;\n\n\nWhen we provide only the file name to the function, we will always retrieve all the content from the first sheet. However, due to the limitations in R tables, every column will be recognized as the same data type, typically character.\n\n\n2.3.2 Give a range\n\n# using the range argument\ntb_Pegeln_Range &lt;- read_excel(fn_Pegeln, range = \"Suchergebnisse Pegel!A5:P10\")\ntb_Pegeln_Range\n\n# A tibble: 5 × 16\n  Name            Pegelnummer   Gewässername Betreiber  `Pegelnullpunkt [müNHN]`\n  &lt;chr&gt;           &lt;chr&gt;         &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;                   \n1 Ahlen           3211000000300 Werse        LANUV, NRW 73,47                   \n2 Ahmsen          4639000000100 Werre        LANUV, NRW 64,28                   \n3 Ahrhütte-Neuhof 2718193000100 Ahr          LANUV, NRW 340,58                  \n4 Albersloh       3259000000100 Werse        LANUV, NRW 48,68                   \n5 Altena          2766930000100 Lenne        LANUV, NRW 154,22                  \n# ℹ 11 more variables: `Einzugsgebiet [km²]` &lt;chr&gt;, `Q von` &lt;chr&gt;,\n#   `Q bis` &lt;chr&gt;, NQ &lt;chr&gt;, MNQ &lt;chr&gt;, MQ &lt;chr&gt;, MHQ &lt;chr&gt;, HQ &lt;chr&gt;,\n#   `Q Einheit` &lt;chr&gt;, `Ostwert in UTM` &lt;chr&gt;, `Nordwert in UTM` &lt;chr&gt;\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe data type of “Pegelnullpunkt [müNHN]” appears to be incorrect due to improper settings in Excel."
  },
  {
    "objectID": "dataprocess/data_load.html#data-type",
    "href": "dataprocess/data_load.html#data-type",
    "title": "Data Loading",
    "section": "2.4 Data type",
    "text": "2.4 Data type\nCompared to plain text files, Excel data already contains data type information for each cell. Therefore, the data type will be directly determined by the data type specified in Excel.\nHowever, there are instances where the data type in Excel is not correctly set, so manual data type conversion may be necessary. For more details, refer to Section 1.5."
  },
  {
    "objectID": "dataprocess/data_load.html#library-and-functions",
    "href": "dataprocess/data_load.html#library-and-functions",
    "title": "Data Loading",
    "section": "1.2 Library and functions",
    "text": "1.2 Library and functions\n\nRPython\n\n\nFirst, we need to load the necessary library tidyverse. This library collection includes readr for reading files and dplyr for data manipulation, among others.\nAnd, we set the URL address as the file path (including the file name).\n\n# load the library\nlibrary(tidyverse)\nfn_Bachum &lt;- \"https://raw.githubusercontent.com/HydroSimul/Web/main/data_share/Bachum_2763190000100.csv\"\nfn_Datatype &lt;- \"https://raw.githubusercontent.com/HydroSimul/Web/main/data_share/load_Datatype.txt\"\n\nThe documentation for the readr library is available online and can be accessed at https://readr.tidyverse.org.\nOf particular interest are the following functions:\n\nreadr::read_csv()\nreadr::read_table()\n\nWe can observe that the CSV file is divided by semicolons. Therefore, it’s more appropriate to use read_csv2() rather than read_csv().\nThe difference between read_*() functions in the readr package is determined by the delimiter character used in the files:\n\n\n\nCHEAT SHEET from Rstudio\n\n\n\n\n\n# load the library\nimport pandas as pd\nfn_Bachum = \"https://raw.githubusercontent.com/HydroSimul/Web/main/data_share/Bachum_2763190000100.csv\"\nfn_Datatype = \"https://raw.githubusercontent.com/HydroSimul/Web/main/data_share/load_Datatype.txt\"\n\nThe documentation for the pandas library is available online and can be accessed at https://pandas.pydata.org/docs/index.html.\nOf particular interest are the following functions:\n\npandas.read_csv()\npandas.read_table()"
  },
  {
    "objectID": "spatialdata/extract_raster.html",
    "href": "spatialdata/extract_raster.html",
    "title": "Extract values from Raster Data",
    "section": "",
    "text": "Raster Data is actually a kind of Sample data of a area, the area will be divided in regular grids (equally sized rectangles). The typical raster data like elevation is already the most important data for many spatial-based research fields. The raster form is also the important form for meteorological data, in order the area-value (e.g. Temperature and Presentation) to represent. But for the application e.g. in the Hydrology need we some statistical values for specific research regions, especially the Average.\nTherefore we need the operate EXTRACT."
  },
  {
    "objectID": "spatialdata/extract_raster.html#rough-with-original-resolution",
    "href": "spatialdata/extract_raster.html#rough-with-original-resolution",
    "title": "Extract values from Raster Data",
    "section": "2.1 Rough with original resolution",
    "text": "2.1 Rough with original resolution\nThe first method we just use the original Raster with original resolution. But when the resolution not so fine, it will occur to that the selected grids have the big difference than the region. This is one very typical Problem in meteorological data, they have not so gut space resolution because the time resolution is always finer than the common geological data. In order to balance the data size, we must reduce the space resolution.\n\n\n\nIllustration of Mask with “Touch” oder “Centerpoint”\n\n\nFor the SELECT, there are two familiar methods Touch and Center-point:\n\nTouch: all the grids, who touched by the region, will be selected\nCenter-point: only the grids, who’s Center-point is with in the region, will be selected\n\nFor the both SELECT methods there some implausible cases:\n\nwhen we use Touch method, it will select some grids, who has only a little area within the region, like Cell 4\nCell 5: only an eighth of the area within the region, but it counts as a “whole cell” just because its center is in the region\nCell 18: with three quarters of the area in the region, but is not selected, just because the center is not in the region\n\nSummary we can say: the original resolution can be used, only when the deviation between the grids and region is not so big and\n\nTouch includes all grid cells that are touched, so can be used for some extreme statistical value (e.g. Max or Min)\nCenter-point can be used for the average value and actually the deviation maybe reduced, due to the surplus of selected grids and deficit of not selected grids in the boundary."
  },
  {
    "objectID": "spatialdata/extract_raster.html#refine-resolution",
    "href": "spatialdata/extract_raster.html#refine-resolution",
    "title": "Extract values from Raster Data",
    "section": "2.2 Refine resolution",
    "text": "2.2 Refine resolution\nThe second method is one simplest method, we need only refine our data in higher resolution, like resolution in 10 times finer and the grids will in 100 times more.\nEssentially there is no difference as 1. method, but the problem will be solved. This method is pointed, just because I must use Matlab processing the data, but there is no spatial Analyse Toolbox in Matlab. Therefore this is fast the only Answer, just because the Refine needs no supply from spatial Analyse Toolbox, we can easily repeat the data 10 more in row and 10 more in column.\n\n\n\nIllustration of Mask with “Touch” oder “Centerpoint” after Disaggregation\n\n\nLike the figure shows: the accuracy is gut improvement, the deviation should lay under the 1%."
  },
  {
    "objectID": "spatialdata/extract_raster.html#exact-with-area",
    "href": "spatialdata/extract_raster.html#exact-with-area",
    "title": "Extract values from Raster Data",
    "section": "2.3 Exact with Area",
    "text": "2.3 Exact with Area\nThe weighted mean is always exacter than the numerical average. The important Point for the weighted mean is the weights, in the spatial analyse it’s the portion of the area. So, the main task in third method is calculate the area of every value, that within one region.\nIn order to calculate the area, we need actually convert the raster grids into shape, for the convert we have also two methods:\n\nthe same value as one Polygon (this method should more convenient for the categories data with only several value)\nevery grid as a rectangle polygon, then calculate the portion of the area, where is located within the region (this method is use in R::terra package, but there is also a small deviation, when the CRS lie in lon-lat. The portion of one grid will be not equal to the portion of the region, because the grid area one to one is already different.)\n\n\n\n\nIllustration von Extract mit Wert-Polygon\n\n\nIn the Illustration is every value as the same polygon converted."
  },
  {
    "objectID": "spatialdata/extract_raster.html#exact-with-scale-product",
    "href": "spatialdata/extract_raster.html#exact-with-scale-product",
    "title": "Extract values from Raster Data",
    "section": "2.4 Exact with scale product",
    "text": "2.4 Exact with scale product\nThis method is designed only for the meteorological data, those have the big mange on time scalar. It’s also the most effective method in the practice.\nThe theory and the formal is just like:\n\\[\n\\vec{\\Omega}_{[time,region]} = \\vec{A}_{[time,grid]} \\cdot \\vec{W}_{[grid,region]}\n\\]\n\\(\\vec{\\Omega}_{[time,region]}\\) = Region-value of every region\n\\(\\vec{A}_{[time,grid]}\\) = all Value in the matrix [time, grid]\n\\(\\vec{W}_{[grid,region]}\\) = Weights of every grid to every region in the matrix [grid,region]\n\n2.4.1 Weight-Matrix\nFor the Weight-Matrix calculate we just the portion of the grid area only that within the region to the whole region area (but not the whole grid), then divide the area of the region.\nOne example weight_grid:\n            [R1]      [R2]\n [G1]      0.000      0.00\n [G2] 134364.119 189431.77\n [G3] 212464.416      0.00\n [G4]   2747.413      0.00\n [G5] 150176.618      0.00\n [G6]      0.000  45011.22\nG for Grid and R for Region\n\n\n2.4.2 Value-Matrix\nOne example mat_value:\n     [G1] [G2] [G3] [G4] [G5] [G6] \n[T1]    2    1    3    4    1    1  \n[T2]    3    1    2    4    1    1  \nT for Time\nThe end."
  },
  {
    "objectID": "spatialdata/extract_vector.html",
    "href": "spatialdata/extract_vector.html",
    "title": "Extract values from Vector (Shape) Data",
    "section": "",
    "text": "Compared to the raster data are Vector data describe the data use mange Shapes, the basic (or we can say the only three) Shape form are Points, Lines and Polygons. In all cases, the geometry of these data structures consists of sets of coordinate pairs (x, y). (From terra The geometry just define the location and topology, the other important part of vector data is the attributes of every shape, in that store we the data.\nIn our Hydrology or Meteorology fields have we usually the point-data (e.g. data from measuring station) and area-data (e.g. land use or soil). There may be also some line-data (e.g. isoline), but they are always not used for storing the data, maybe just show the data. So, the operate EXTRACT is actually the statistic of the Vector data for the specific regions.\nAnd in the Blog we will only take the forms POINTS and POLYGONs into account."
  },
  {
    "objectID": "spatialdata/extract_vector.html#numerical-mean",
    "href": "spatialdata/extract_vector.html#numerical-mean",
    "title": "Extract values from Vector (Shape) Data",
    "section": "3.1 Numerical Mean",
    "text": "3.1 Numerical Mean\nThe moooost simple and direct method is the Numerical Mean of points in the region:\n\n\n\nIllustration of Extract with Point-Value: Numerical Mean of points in the region\n\n\n\nInterset with regions, then select points which in the region\nCalculate mean value of points\n\nThe weakness are also obviously, many points, who lay just near the boundary of region, will be ignored. It’s also familiar that in some regions there are no points laying in.\nSo, we need maybe convert the point-data to polygon- or raster-data"
  },
  {
    "objectID": "spatialdata/extract_vector.html#tiessen-dirichlet-polygon",
    "href": "spatialdata/extract_vector.html#tiessen-dirichlet-polygon",
    "title": "Extract values from Vector (Shape) Data",
    "section": "3.2 Tiessen (Dirichlet) Polygon",
    "text": "3.2 Tiessen (Dirichlet) Polygon\nActually the convert to the polygon is the most popular and typical method specially with the Tiessen (Dirichlet) Polygon in meteorological fields.\n\n\n\nIllustration of Extract mit Punkt-Wert\n\n\n\nConvert point data to Tiessen polygon data\nuse the method of polygon like above"
  },
  {
    "objectID": "spatialdata/extract_vector.html#interpolate-as-raster",
    "href": "spatialdata/extract_vector.html#interpolate-as-raster",
    "title": "Extract values from Vector (Shape) Data",
    "section": "3.3 Interpolate as Raster",
    "text": "3.3 Interpolate as Raster\nThe second convert idea is convert to the raster: Interpolation\n\n\n\nIllustration der Interpolation mit Punkt-Werten\n\n\nThe Interpolation is also one important issue, and it will be discussed in th near future. Here will just show the three most impotent methods: Nearest neighbor, IDW (Inverse distance weighted) and Kringing\nThe three methods are also very easy processed in R::terra, that will be showed in the next Blog.\nThe end."
  },
  {
    "objectID": "dataprocess/basic_format.html#nectcdf",
    "href": "dataprocess/basic_format.html#nectcdf",
    "title": "Basic Data & File Format",
    "section": "NectCDF",
    "text": "NectCDF\nNetCDF (Network Common Data Form) is a versatile data format widely used in scientific and environmental applications. It is primarily a binary data format, but it includes structured elements for efficient data storage and management. Here are some key characteristics of NetCDF:\n\nBinary Representation: NetCDF data files are primarily stored in binary format, which enables efficient storage and handling of numerical data, particularly floating-point numbers.\nSelf-Describing: NetCDF files are self-describing, meaning they include metadata alongside the data. This metadata provides essential information about the data’s structure, dimensions, units, and other attributes.\nHierarchical Structure: NetCDF supports a hierarchical structure capable of representing complex data types, including multi-dimensional arrays and groups of data variables.\nData Compression: NetCDF allows for data compression, which can reduce the storage space required for large datasets while maintaining data integrity.\nLanguage Support: NetCDF libraries and tools are available for multiple programming languages, making it accessible to a wide range of scientific and data analysis applications.\n\nNetCDF’s combination of binary efficiency and structured metadata makes it an invaluable choice for storing and sharing scientific data, particularly in fields such as meteorology, oceanography, and environmental science."
  },
  {
    "objectID": "dataprocess/basic_format.html#excel-files",
    "href": "dataprocess/basic_format.html#excel-files",
    "title": "Basic Data & File Format",
    "section": "Excel Files",
    "text": "Excel Files\nExcel files, often denoted with the extensions .xls or .xlsx, are a common file format used for storing structured data in tabular form. These files are not to be confused with the Microsoft Excel software itself but are the data containers created and manipulated using spreadsheet software like Excel.\nExcel files are widely used in various applications, including data storage, analysis, reporting, and sharing. They consist of rows and columns, where each cell can contain text, numbers, formulas, or dates. These files are versatile and can hold different types of data, making them a popular choice for managing information.\n\nAdvantages:\n\nUser-Friendly Interface: Excel’s user-friendly interface makes it accessible to users with varying levels of expertise. Its familiar grid layout simplifies data input and manipulation.\nVersatility: Excel can handle various types of data, from simple lists to complex calculations.\nFormulas and Functions: Excel provides an extensive library of built-in formulas and functions, allowing users to automate calculations and streamline data processing.\nData Visualization: Creating charts and graphs in Excel is straightforward. It helps in visualizing data trends and patterns, making complex information more accessible.\nData Validation: Excel allows you to set rules and validation criteria for data entry, reducing errors and ensuring data accuracy.\n\n\n\nDisadvantages:\n\nLimited Data Handling: Excel has limitations in handling very large datasets. Performance may degrade, and it’s not suitable for big data analytics.\nLack of Version Control: Excel lacks robust version control features, making it challenging to track changes and manage document versions in collaborative environments.\n\nIn conclusion, Excel is a valuable tool for various data-related tasks but comes with limitations in terms of scalability, data integrity, and security. Careful consideration of its strengths and weaknesses is essential when deciding whether it’s the right choice for your data management needs."
  },
  {
    "objectID": "dataprocess/basic_format.html#spatial-data-file-formats",
    "href": "dataprocess/basic_format.html#spatial-data-file-formats",
    "title": "Basic Data & File Format",
    "section": "Spatial Data File Formats",
    "text": "Spatial Data File Formats\nSpatial data files are a specialized type of data format designed for storing geographic or location-based information. Unlike standard data files that store text, numbers, or other types of data, spatial data files are tailored for representing the geographical features of our world.\nSpatial data comes in various file formats, each tailored for specific types of geographic data and applications. Here are some commonly used formats and their key differences:\n\nRaster Data Formats\nMore Raster file-formats in GDAL\n\nTIFF (Tagged Image File Format):\n\nA widely used raster format for storing high-quality images and raster datasets.\nSupports georeferencing and metadata, making it suitable for spatial applications.\n\nASC (Arc/Info ASCII Grid):\n\nA plain text format used to represent raster data in a grid format.\nContains elevation or other continuous data with rows and columns of values.\n\nJPEG (Joint Photographic Experts Group), PNG (Portable Network Graphics):\n\nCommonly used for photographs and images, but not ideal for spatial analysis due to lossy compression.\n\n\n\n\nVector Data Formats\nMore Vector file-formats in GDAL\n\nShapefile (SHP):\n\nOne of the most common vector formats used in GIS applications.\nConsists of multiple files (.shp, .shx, .dbf, etc.) to store point, line, or polygon geometries and associated attributes.\n\nGeoPackage (GPKG):\n\nAn open, standards-based platform-independent format for spatial data.\nCan store multiple layers, attributes, and geometries in a single file.\n\nKML (Keyhole Markup Language):\n\nXML-based format used for geographic visualization in Earth browsers like Google Earth.\nSuitable for storing points, lines, polygons, and related attributes.\n\nGeoJSON:\n\nA lightweight format for encoding geographic data structures using JSON (JavaScript Object Notation).\nIdeal for web applications due to its simplicity and ease of use."
  },
  {
    "objectID": "dataprocess/basic_format.html#database-systems",
    "href": "dataprocess/basic_format.html#database-systems",
    "title": "Basic Data & File Format",
    "section": "Database Systems",
    "text": "Database Systems\nDatabase Systems, such as SQL and NoSQL databases, are crucial for efficiently managing and querying large, structured datasets. They provide structured data storage, ensuring data integrity and consistency. SQL databases like MySQL and PostgreSQL are well-suited for relational data, while NoSQL databases like MongoDB excel in handling semi-structured or unstructured data. These systems are commonly used for storing long-term observational data, model outputs, and sensor data in scientific research and various enterprise applications.\n\nAdvantages\n\nEfficient Data Retrieval: Databases are optimized for querying and retrieving data, making it quick and efficient to access information.\nData Integrity: Databases enforce data integrity rules, ensuring that data remains consistent and reliable over time.\nStructured Storage: They provide a structured way to store data, making it easier to organize and manage large datasets.\nConcurrent Access: Multiple users or applications can access the database simultaneously, enabling collaboration and scalability.\nSecurity: Database systems offer security features like user authentication and authorization to protect sensitive data.\nBackup and Recovery: They often include mechanisms for automated data backup and recovery, reducing the risk of data loss.\n\n\n\nDisadvantages\n\nComplexity: Setting up and maintaining a database can be complex and requires specialized knowledge.\nCost: Licensing, hardware, and maintenance costs can be significant, especially for enterprise-grade database systems.\nScalability Challenges: Some database systems may face scalability limitations as data volume grows.\nLearning Curve: Users and administrators need to learn query languages (e.g., SQL) and database management tools.\nOverhead: Databases can introduce overhead due to indexing, data normalization, and transaction management.\nVendor Lock-In: Depending on the chosen database system, there may be vendor lock-in, making it challenging to switch to another system.\nResource Intensive: Databases consume computing resources, such as CPU and RAM, which can affect system performance.\n\nThe choice of using a database system depends on specific requirements, such as data volume, complexity, security, and scalability needs. It’s essential to carefully evaluate the advantages and disadvantages in the context of your project."
  },
  {
    "objectID": "dataprocess/basic_format.html#array",
    "href": "dataprocess/basic_format.html#array",
    "title": "Basic Data & File Format",
    "section": "Array",
    "text": "Array\nArrays are collections of elements, typically of the SAME data type, organized in a linear or multi-dimensional fashion. They provide efficient data storage and manipulation, making them essential for numerical computations."
  },
  {
    "objectID": "dataprocess/basic_format.html#table-dataframe",
    "href": "dataprocess/basic_format.html#table-dataframe",
    "title": "Basic Data & File Format",
    "section": "Table (Dataframe)",
    "text": "Table (Dataframe)\nTabular data structures, often referred to as tables, are a fundamental way of organizing and representing data in a structured format. They consist of rows and columns, where each row typically represents a single observation or record, and each column represents a specific attribute or variable associated with those observations. Tabular structures are highly versatile and are widely used for storing and analyzing various types of data, ranging from simple lists to complex datasets. This characteristic of tables enables them to represent and manage a wide range of information efficiently.\nCompare to array, in a table, columns are allowed to have different data types, but all values within a specific column must share the same data type."
  },
  {
    "objectID": "dataprocess/basic_format.html#spatial-vector",
    "href": "dataprocess/basic_format.html#spatial-vector",
    "title": "Basic format",
    "section": "Spatial Vector",
    "text": "Spatial Vector\nSpatial vector data structures represent geometric shapes like points, lines, and polygons in space. They are widely used in geographic information systems (GIS) for mapping and analyzing spatial data, such as city boundaries or river networks."
  },
  {
    "objectID": "dataprocess/basic_format.html#spatial-raster",
    "href": "dataprocess/basic_format.html#spatial-raster",
    "title": "Basic format",
    "section": "Spatial Raster",
    "text": "Spatial Raster\nSpatial raster data structures are grid-based representations of spatial data, where each cell holds a value. They are commonly used for storing continuous data, like satellite imagery or elevation models. Rasters enable efficient spatial operations and analysis."
  },
  {
    "objectID": "dataprocess/basic_format.html#time-series",
    "href": "dataprocess/basic_format.html#time-series",
    "title": "Basic Data & File Format",
    "section": "Time Series",
    "text": "Time Series\nTime series data structures are specifically designed to capture and represent information recorded over a period of time. They play a crucial role in analyzing trends, patterns, and dependencies within sequences of data. Time series data, by definition, have a temporal dimension, making time an essential component of these structures.\nIn comparison to spatial information, time information is relatively straightforward. When the time dimension progresses in uniform steps, it can be efficiently described using the start time and step intervals. However, when the time intervals are irregular or non-uniform, additional time-related details are necessary. This can include specifying the year, month, and day for date-based time data or the hour, minute, and second for time-based information.\nIt’s worth noting that while most time series data adheres to the standard calendar system, some datasets may use alternative calendar systems such as the Julian calendar. Additionally, time zone information is crucial when working with time data, as it ensures accurate temporal references across different geographical regions."
  },
  {
    "objectID": "dataprocess/extract_vector.html",
    "href": "dataprocess/extract_vector.html",
    "title": "Extract values from Vector (Shape) Data",
    "section": "",
    "text": "Compared to the raster data are Vector data describe the data use mange Shapes, the basic (or we can say the only three) Shape form are Points, Lines and Polygons. In all cases, the geometry of these data structures consists of sets of coordinate pairs (x, y). (From terra The geometry just define the location and topology, the other important part of vector data is the attributes of every shape, in that store we the data.\nIn our Hydrology or Meteorology fields have we usually the point-data (e.g. data from measuring station) and area-data (e.g. land use or soil). There may be also some line-data (e.g. isoline), but they are always not used for storing the data, maybe just show the data. So, the operate EXTRACT is actually the statistic of the Vector data for the specific regions.\nAnd in the Blog we will only take the forms POINTS and POLYGONs into account."
  },
  {
    "objectID": "dataprocess/extract_vector.html#numerical-mean",
    "href": "dataprocess/extract_vector.html#numerical-mean",
    "title": "Extract values from Vector (Shape) Data",
    "section": "3.1 Numerical Mean",
    "text": "3.1 Numerical Mean\nThe moooost simple and direct method is the Numerical Mean of points in the region:\n\n\n\nIllustration of Extract with Point-Value: Numerical Mean of points in the region\n\n\n\nInterset with regions, then select points which in the region\nCalculate mean value of points\n\nThe weakness are also obviously, many points, who lay just near the boundary of region, will be ignored. It’s also familiar that in some regions there are no points laying in.\nSo, we need maybe convert the point-data to polygon- or raster-data"
  },
  {
    "objectID": "dataprocess/extract_vector.html#tiessen-dirichlet-polygon",
    "href": "dataprocess/extract_vector.html#tiessen-dirichlet-polygon",
    "title": "Extract values from Vector (Shape) Data",
    "section": "3.2 Tiessen (Dirichlet) Polygon",
    "text": "3.2 Tiessen (Dirichlet) Polygon\nActually the convert to the polygon is the most popular and typical method specially with the Tiessen (Dirichlet) Polygon in meteorological fields.\n\n\n\nIllustration of Extract mit Punkt-Wert\n\n\n\nConvert point data to Tiessen polygon data\nuse the method of polygon like above"
  },
  {
    "objectID": "dataprocess/extract_vector.html#interpolate-as-raster",
    "href": "dataprocess/extract_vector.html#interpolate-as-raster",
    "title": "Extract values from Vector (Shape) Data",
    "section": "3.3 Interpolate as Raster",
    "text": "3.3 Interpolate as Raster\nThe second convert idea is convert to the raster: Interpolation\n\n\n\nIllustration der Interpolation mit Punkt-Werten\n\n\nThe Interpolation is also one important issue, and it will be discussed in th near future. Here will just show the three most impotent methods: Nearest neighbor, IDW (Inverse distance weighted) and Kringing\nThe three methods are also very easy processed in R::terra, that will be showed in the next Blog.\nThe end."
  },
  {
    "objectID": "dataprocess/extract_raster.html",
    "href": "dataprocess/extract_raster.html",
    "title": "Extract values from Raster Data",
    "section": "",
    "text": "Raster Data is actually a kind of Sample data of a area, the area will be divided in regular grids (equally sized rectangles). The typical raster data like elevation is already the most important data for many spatial-based research fields. The raster form is also the important form for meteorological data, in order the area-value (e.g. Temperature and Presentation) to represent. But for the application e.g. in the Hydrology need we some statistical values for specific research regions, especially the Average.\nTherefore we need the operate EXTRACT."
  },
  {
    "objectID": "dataprocess/extract_raster.html#rough-with-original-resolution",
    "href": "dataprocess/extract_raster.html#rough-with-original-resolution",
    "title": "Extract values from Raster Data",
    "section": "2.1 Rough with original resolution",
    "text": "2.1 Rough with original resolution\nThe first method we just use the original Raster with original resolution. But when the resolution not so fine, it will occur to that the selected grids have the big difference than the region. This is one very typical Problem in meteorological data, they have not so gut space resolution because the time resolution is always finer than the common geological data. In order to balance the data size, we must reduce the space resolution.\n\n\n\nIllustration of Mask with “Touch” oder “Centerpoint”\n\n\nFor the SELECT, there are two familiar methods Touch and Center-point:\n\nTouch: all the grids, who touched by the region, will be selected\nCenter-point: only the grids, who’s Center-point is with in the region, will be selected\n\nFor the both SELECT methods there some implausible cases:\n\nwhen we use Touch method, it will select some grids, who has only a little area within the region, like Cell 4\nCell 5: only an eighth of the area within the region, but it counts as a “whole cell” just because its center is in the region\nCell 18: with three quarters of the area in the region, but is not selected, just because the center is not in the region\n\nSummary we can say: the original resolution can be used, only when the deviation between the grids and region is not so big and\n\nTouch includes all grid cells that are touched, so can be used for some extreme statistical value (e.g. Max or Min)\nCenter-point can be used for the average value and actually the deviation maybe reduced, due to the surplus of selected grids and deficit of not selected grids in the boundary."
  },
  {
    "objectID": "dataprocess/extract_raster.html#refine-resolution",
    "href": "dataprocess/extract_raster.html#refine-resolution",
    "title": "Extract values from Raster Data",
    "section": "2.2 Refine resolution",
    "text": "2.2 Refine resolution\nThe second method is one simplest method, we need only refine our data in higher resolution, like resolution in 10 times finer and the grids will in 100 times more.\nEssentially there is no difference as 1. method, but the problem will be solved. This method is pointed, just because I must use Matlab processing the data, but there is no spatial Analyse Toolbox in Matlab. Therefore this is fast the only Answer, just because the Refine needs no supply from spatial Analyse Toolbox, we can easily repeat the data 10 more in row and 10 more in column.\n\n\n\nIllustration of Mask with “Touch” oder “Centerpoint” after Disaggregation\n\n\nLike the figure shows: the accuracy is gut improvement, the deviation should lay under the 1%."
  },
  {
    "objectID": "dataprocess/extract_raster.html#exact-with-area",
    "href": "dataprocess/extract_raster.html#exact-with-area",
    "title": "Extract values from Raster Data",
    "section": "2.3 Exact with Area",
    "text": "2.3 Exact with Area\nThe weighted mean is always exacter than the numerical average. The important Point for the weighted mean is the weights, in the spatial analyse it’s the portion of the area. So, the main task in third method is calculate the area of every value, that within one region.\nIn order to calculate the area, we need actually convert the raster grids into shape, for the convert we have also two methods:\n\nthe same value as one Polygon (this method should more convenient for the categories data with only several value)\nevery grid as a rectangle polygon, then calculate the portion of the area, where is located within the region (this method is use in R::terra package, but there is also a small deviation, when the CRS lie in lon-lat. The portion of one grid will be not equal to the portion of the region, because the grid area one to one is already different.)\n\n\n\n\nIllustration von Extract mit Wert-Polygon\n\n\nIn the Illustration is every value as the same polygon converted."
  },
  {
    "objectID": "dataprocess/extract_raster.html#exact-with-scale-product",
    "href": "dataprocess/extract_raster.html#exact-with-scale-product",
    "title": "Extract values from Raster Data",
    "section": "2.4 Exact with scale product",
    "text": "2.4 Exact with scale product\nThis method is designed only for the meteorological data, those have the big mange on time scalar. It’s also the most effective method in the practice.\nThe theory and the formal is just like:\n\\[\n\\vec{\\Omega}_{[time,region]} = \\vec{A}_{[time,grid]} \\cdot \\vec{W}_{[grid,region]}\n\\]\n\\(\\vec{\\Omega}_{[time,region]}\\) = Region-value of every region\n\\(\\vec{A}_{[time,grid]}\\) = all Value in the matrix [time, grid]\n\\(\\vec{W}_{[grid,region]}\\) = Weights of every grid to every region in the matrix [grid,region]\n\n2.4.1 Weight-Matrix\nFor the Weight-Matrix calculate we just the portion of the grid area only that within the region to the whole region area (but not the whole grid), then divide the area of the region.\nOne example weight_grid:\n            [R1]      [R2]\n [G1]      0.000      0.00\n [G2] 134364.119 189431.77\n [G3] 212464.416      0.00\n [G4]   2747.413      0.00\n [G5] 150176.618      0.00\n [G6]      0.000  45011.22\nG for Grid and R for Region\n\n\n2.4.2 Value-Matrix\nOne example mat_value:\n     [G1] [G2] [G3] [G4] [G5] [G6] \n[T1]    2    1    3    4    1    1  \n[T2]    3    1    2    4    1    1  \nT for Time\nThe end."
  },
  {
    "objectID": "dataprocess/basic_format.html#spatial-data",
    "href": "dataprocess/basic_format.html#spatial-data",
    "title": "Basic Data & File Format",
    "section": "Spatial Data",
    "text": "Spatial Data\nMore Details in Spatial Data Science\nSpatial data refers to data that has a geographic or spatial component, representing the locations and shapes of physical objects on the Earth’s surface. This type of data is essential in various fields, including geography, environmental science, urban planning, and more. One of the key elements in spatial data is its association with coordinate systems, which allow precise location referencing.\n\nSpatial Vector\nSpatial vector data structures represent geometric shapes like points, lines, and polygons in space. They are widely used in geographic information systems (GIS) for mapping and analyzing spatial data, such as landuse boundaries or river networks.\n\nThe term “Vector” is used because spatial vector data is essentially stored as a vector of points, lines, or polygons (which are composed of lines). The data structure for geographic shapes is divided into two key components:\n\nGeometry: Geometry represents the spatial shape or location of the geographic feature. It defines the boundaries, points, lines, or polygons that make up the feature. These geometric elements are used to precisely describe the geometric feature.\nAttributes: Attributes are associated with the geographic feature and provide additional information about it. These attributes can include data such as the feature’s name, population, temperature, or any other relevant details. Attributes are typically organized and stored in a tabular format, making it easy to perform data analysis and visualization.\n\nThe data structure of points in geospatial data is relatively simple. The geometry of one point is described by its coordinates, typically represented as X (or longitude) and Y (or latitude) values.\n\nOn the other hand, lines and polygons are more complex geometric shapes. The geometry of a line or polygon is defined by a sequence of multiple points. These points are connected in a specific order to form the shape of the line or polygon. In other words, the geometry of every line (or polygon) is composed of a series of coordinates of points.\n\n\n\nSpatial Raster\nSpatial raster data structures are grid-based representations of spatial data, where each cell holds a value. They are commonly used for storing continuous data, like satellite imagery or elevation models.\nThe datastructure of raster data is quite simple. In a raster, each row shares the same X value, and each column shares the same Y value. Additionally, in most situations, the resolution in each dimension remains constant. This means that specifying the starting point and the resolutions is usually sufficient to describe the coordinates of every grid cell. A single raster layer indeed resembles a 2D matrix.\n\n\n\nCoordinate Reference System (CRS)\nIn addition to Geometry (of Vector) and Koordinate (of aster), another essential component of spatial data is the Coordinate Reference System (CRS). The CRS plays a crucial role in geospatial data by providing a framework for translating the Earth’s 3D surface into a 2D coordinate system.\nKey points about the Coordinate Reference System (CRS) include:\n\nProjection: The CRS defines how the Earth’s curved surface is projected onto a 2D plane, enabling the representation of geographic features on maps and in geographic information systems (GIS). Different projection methods exist, each with its own strengths and weaknesses depending on the region and purpose of the map.\nUnits: CRS specifies the units of measurement for coordinates. Common units include degrees (for latitude and longitude), meters, and feet, among others.\nReference Point: It establishes a reference point (usually the origin) and orientation for the coordinate system.\nEPSG Code: Many CRS are identified by an EPSG (European Petroleum Survey Group) code, which is a unique numeric identifier that facilitates data sharing and standardization across GIS systems.\n\nThe CRS is fundamental for correctly interpreting and analyzing spatial data, as it ensures that geographic features are accurately represented in maps and GIS applications. Different CRSs are used for different regions and applications to minimize distortion and provide precise geospatial information.\nThe use of EPSG (European Petroleum Survey Group) codes is highly recommended for defining Coordinate Reference Systems (CRS) in spatial data. These codes consist of a string of numbers that uniquely identify a specific CRS. By using EPSG codes, you can easily access comprehensive definitions of different CRSs, which include details about their coordinate systems, datums, projections, and other parameters. Many software applications and libraries support EPSG codes, making it a standardized and convenient way to specify CRS information in spatial data.\nYou can obtain information about EPSG codes from the EPSG website. This website serves as a valuable resource for accessing detailed information associated with EPSG codes, including coordinate reference system (CRS) definitions and specifications."
  },
  {
    "objectID": "dataprocess/basic_format.html#plain-text-ascii",
    "href": "dataprocess/basic_format.html#plain-text-ascii",
    "title": "Basic Data & File Format",
    "section": "Plain text (ASCII)",
    "text": "Plain text (ASCII)\nASCII (American Standard Code for Information Interchange) is a plain text format, making it human-readable.\n\nAdvantages\n\nHuman-Readable: Users can easily view, understand, and edit the data directly in a text editor.\nWidespread Support, Ease of Import/Export: ASCII is universally supported. Most programming languages, data analysis tools, and software applications can read and write ASCII files, ensuring high compatibility.\nLightweight: ASCII files are typically lightweight and do not consume excessive storage space, making them suitable for large datasets.\nSimple Structure: ASCII files have a straightforward structure, often using lines of text with fields separated by delimiters. This simplicity aids in data extraction and manipulation.\n\n\n\nDisadvantages\n\nLimited Data Types: ASCII primarily handles text-based data and is not suitable for complex data types such as images, multimedia, or hierarchical data.\nNo Inherent Data Validation: ASCII files lack built-in mechanisms for data validation or integrity checks, requiring users to ensure data conformity.\nLack of Compression: ASCII files do not inherently support data compression, potentially resulting in larger file sizes compared to binary formats.\nSlower Reading/Writing: Reading and writing data in ASCII format may be slower, especially for large datasets, due to additional parsing required to interpret text-based data.\n\n\n\nFile format for ASCII data\nWhen it comes to plain text formats, there is no universal standard, and it’s highly adaptable to specific needs. The initial step in loading a plain text table is to analyze the structure of the file.\nTypically, a text table can store 2D data, comprising columns and rows or a matrix. However, above the data body, there’s often metadata that describes the data. Metadata can vary widely between data body.\nDividing rows is usually straightforward and can be achieved by identifying row-end characters. However, dividing columns within each row presents multiple possibilities, such as spaces, tabs, commas, or semicolons.\n\n.txt: This is the most generic and widely used file extension for plain text files. It doesn’t imply any specific format or structure; it’s just a simple text file.\n.csv (Comma-Separated Values): While CSV files contain data separated by commas, they are still considered ASCII files because they use plain text characters to represent data values. Each line in a CSV file typically represents a record, with values separated by commas.\n\nIn .txt files, any of these separators can be used, but in .csv files, commas or semicolons are commonly employed as separator characters."
  },
  {
    "objectID": "dataprocess/spatial_data.html#vector",
    "href": "dataprocess/spatial_data.html#vector",
    "title": "Basic Manipulation",
    "section": "2.1 Vector",
    "text": "2.1 Vector\nAs introduced in the section, spatial vector data typically consists of three main components:\n\nGeometry: Describes the spatial location and shape of features.\nAttributes: Non-spatial properties associated with features.\nCRS (Coordinate Reference System): Defines the spatial reference framework.\n\n\n# Define the coordinate reference system (CRS) with EPSG codes\ncrs_31468 &lt;- \"EPSG:31468\"\n\n# Define coordinates for the first polygon\nx_polygon_1 &lt;- c(4484566, 4483922, 4483002, 4481929, 4481222, 4482500, 4483000, 4484666, 4484233)\ny_polygon_1 &lt;- c(5554566, 5554001, 5553233, 5554933, 5550666, 5551555, 5550100, 5551711, 5552767)\nxy_polygon_1 &lt;- cbind(id=1, part=1, x_polygon_1, y_polygon_1)\n# Define coordinates for the second polygon\nx_polygon_2 &lt;- c(4481929, 4481222, 4480500)\ny_polygon_2 &lt;- c(5554933, 5550666, 5552555)\nxy_polygon_2 &lt;- cbind(id=2, part=1, x_polygon_2, y_polygon_2)\n# Combine the two polygons into one data frame\nxy_polygon &lt;- rbind(xy_polygon_1, xy_polygon_2)\n\n# Create a vector layer for the polygons, specifying their type, attributes, CRS, and additional attributes\nvect_Test &lt;- vect(xy_polygon, type=\"polygons\", atts = data.frame(ID_region = 1:2, Name = c(\"a\", \"b\")), crs = crs_31468)\nvect_Test$region_area &lt;- expanse(vect_Test)"
  },
  {
    "objectID": "dataprocess/spatial_data.html#raster",
    "href": "dataprocess/spatial_data.html#raster",
    "title": "Basic Manipulation",
    "section": "2.2 Raster",
    "text": "2.2 Raster\nFor raster data, the geometry is relatively simple and can be defined by the following components:\n\nCoordinate of Original Point (X0, Y0) plus Resolutions (X and Y)\nBoundaries (Xmin, Xmax, Ymin, Ymax) plus Number of Rows and Columns\n\nOne of the most critical aspects of raster data is the values stored within its cells. You can set or modify these values using the values()&lt;- function in R.\n\nrast_Test &lt;- rast(ncol=10, nrow=10, xmin=-150, xmax=-80, ymin=20, ymax=60)\nvalues(rast_Test) &lt;- runif(ncell(rast_Test))\n\nCertainly, you can directly create a data file like an ASC (ASCII) file for raster data."
  },
  {
    "objectID": "dataprocess/basic_format.html#sec-spatialData",
    "href": "dataprocess/basic_format.html#sec-spatialData",
    "title": "Basic Data & File Format",
    "section": "Spatial Data",
    "text": "Spatial Data\nMore Details in Spatial Data Science\nSpatial data refers to data that has a geographic or spatial component, representing the locations and shapes of physical objects on the Earth’s surface. This type of data is essential in various fields, including geography, environmental science, urban planning, and more. One of the key elements in spatial data is its association with coordinate systems, which allow precise location referencing.\n\nSpatial Vector\nSpatial vector data structures represent geometric shapes like points, lines, and polygons in space. They are widely used in geographic information systems (GIS) for mapping and analyzing spatial data, such as landuse boundaries or river networks.\n\nThe term “Vector” is used because spatial vector data is essentially stored as a vector of points, lines, or polygons (which are composed of lines). The data structure for geographic shapes is divided into two key components:\n\nGeometry: Geometry represents the spatial shape or location of the geographic feature. It defines the boundaries, points, lines, or polygons that make up the feature. These geometric elements are used to precisely describe the geometric feature.\nAttributes: Attributes are associated with the geographic feature and provide additional information about it. These attributes can include data such as the feature’s name, population, temperature, or any other relevant details. Attributes are typically organized and stored in a tabular format, making it easy to perform data analysis and visualization.\n\nThe data structure of points in geospatial data is relatively simple. The geometry of one point is described by its coordinates, typically represented as X (or longitude) and Y (or latitude) values.\n\nOn the other hand, lines and polygons are more complex geometric shapes. The geometry of a line or polygon is defined by a sequence of multiple points. These points are connected in a specific order to form the shape of the line or polygon. In other words, the geometry of every line (or polygon) is composed of a series of coordinates of points.\n\n\n\nSpatial Raster\nSpatial raster data structures are grid-based representations of spatial data, where each cell holds a value. They are commonly used for storing continuous data, like satellite imagery or elevation models.\nThe datastructure of raster data is quite simple. In a raster, each row shares the same X value, and each column shares the same Y value. Additionally, in most situations, the resolution in each dimension remains constant. This means that specifying the starting point and the resolutions is usually sufficient to describe the coordinates of every grid cell. A single raster layer indeed resembles a 2D matrix.\n\n\n\nCoordinate Reference System (CRS)\nIn addition to Geometry (of Vector) and Koordinate (of aster), another essential component of spatial data is the Coordinate Reference System (CRS). The CRS plays a crucial role in geospatial data by providing a framework for translating the Earth’s 3D surface into a 2D coordinate system.\nKey points about the Coordinate Reference System (CRS) include:\n\nAngular coordinates: The earth has an irregular spheroid-like shape. The natural coordinate reference system for geographic data is longitude/latitude.\nProjection: The CRS defines how the Earth’s curved surface is projected onto a 2D plane, enabling the representation of geographic features on maps and in geographic information systems (GIS). Different projection methods exist, each with its own strengths and weaknesses depending on the region and purpose of the map.\nUnits: CRS specifies the units of measurement for coordinates. Common units include degrees (for latitude and longitude), meters, and feet, among others.\nReference Point: It establishes a reference point (usually the origin) and orientation for the coordinate system.\nEPSG Code: Many CRS are identified by an EPSG (European Petroleum Survey Group) code, which is a unique numeric identifier that facilitates data sharing and standardization across GIS systems.\n\nThe CRS is fundamental for correctly interpreting and analyzing spatial data, as it ensures that geographic features are accurately represented in maps and GIS applications. Different CRSs are used for different regions and applications to minimize distortion and provide precise geospatial information.\nThe use of EPSG (European Petroleum Survey Group) codes is highly recommended for defining Coordinate Reference Systems (CRS) in spatial data. These codes consist of a string of numbers that uniquely identify a specific CRS. By using EPSG codes, you can easily access comprehensive definitions of different CRSs, which include details about their coordinate systems, datums, projections, and other parameters. Many software applications and libraries support EPSG codes, making it a standardized and convenient way to specify CRS information in spatial data.\nYou can obtain information about EPSG codes from the EPSG website. This website serves as a valuable resource for accessing detailed information associated with EPSG codes, including coordinate reference system (CRS) definitions and specifications."
  },
  {
    "objectID": "dataprocess/spatial_data.html#assigning-a-crs",
    "href": "dataprocess/spatial_data.html#assigning-a-crs",
    "title": "Basic Manipulation",
    "section": "4.1 Assigning a CRS",
    "text": "4.1 Assigning a CRS\nIn cases where the Coordinate Reference System (CRS) information is not included in the data file’s content, you can assign it manually using the crs() function. This situation often occurs when working with raster data in formats like ASC (Arc/Info ASCII Grid) or other file formats that may not store CRS information.\n\ncrs(rast_Test) &lt;- \"EPSG:31468\"\nrast_Test\n\nclass       : SpatRaster \ndimensions  : 5, 5, 1  (nrow, ncol, nlyr)\nresolution  : 1000, 1000  (x, y)\nextent      : 4480000, 4485000, 5550000, 5555000  (xmin, xmax, ymin, ymax)\ncoord. ref. : DHDN / 3-degree Gauss-Kruger zone 4 (EPSG:31468) \nsource      : minibeispiel_raster.asc \nname        : minibeispiel_raster \n\n\nAs the results showed, the CRS information has been filled with the necessary details in line coord. ref..\nThe use of EPSG (European Petroleum Survey Group) codes is highly recommended for defining Coordinate Reference Systems (CRS) in spatial data. You can obtain information about EPSG codes from the EPSG website.\n\n\n\n\n\n\nNOTE\n\n\n\nYou should not use this approach to change the CRS of a data set from what it is to what you want it to be. Assigning a CRS is like labeling something."
  },
  {
    "objectID": "dataprocess/spatial_data.html#transforming-vector-data",
    "href": "dataprocess/spatial_data.html#transforming-vector-data",
    "title": "Basic Manipulation",
    "section": "4.2 Transforming vector data",
    "text": "4.2 Transforming vector data\nThe transformation of vector data is relatively simple, as it involves applying a mathematical formula to the coordinates of each point to obtain their new coordinates. This transformation can be considered as without loss of precision.\nThe project() function can be utilized to reproject both vector and raster data.\n\n# New CRS\ncrs_New &lt;- \"EPSG:4326\"\n# Reproject\nvect_Test_New &lt;- project(vect_Test, crs_New)\n\n# Info and Plot of vector layer\nvect_Test_New\n\n class       : SpatVector \n geometry    : polygons \n dimensions  : 2, 3  (geometries, attributes)\n extent      : 11.72592, 11.78419, 50.08692, 50.13034  (xmin, xmax, ymin, ymax)\n coord. ref. : lon/lat WGS 84 (EPSG:4326) \n names       : ID_region  Name region_area\n type        :     &lt;int&gt; &lt;chr&gt;       &lt;num&gt;\n values      :         1     a   8.853e+06\n                       2     b   2.208e+06"
  },
  {
    "objectID": "dataprocess/spatial_data.html#transforming-raster-data",
    "href": "dataprocess/spatial_data.html#transforming-raster-data",
    "title": "Basic Manipulation",
    "section": "4.3 Transforming raster data",
    "text": "4.3 Transforming raster data\nVector data can be transformed from lon/lat coordinates to planar and back without loss of precision. This is not the case with raster data. A raster consists of rectangular cells of the same size (in terms of the units of the CRS; their actual size may vary). It is not possible to transform cell by cell. For each new cell, values need to be estimated based on the values in the overlapping old cells. If the values are categorical data, the “nearest neighbor” method is commonly used. Otherwise some sort of interpolation is employed (e.g. “bilinear”). (From Spatial Data Science)\n\n\n\n\n\n\nNote\n\n\n\nBecause projection of rasters affects the cell values, in most cases you will want to avoid projecting raster data and rather project vector data.\n\n\n\n4.3.1 With CRS\nThe simplest approach is to provide a new CRS:\n\n# New CRS\ncrs_New &lt;- \"EPSG:4326\"\n# Reproject\nrast_Test_New &lt;- project(rast_Test, crs_New)\n\n# Info and Plot of vector layer\nrast_Test_New\n\nclass       : SpatRaster \ndimensions  : 4, 6, 1  (nrow, ncol, nlyr)\nresolution  : 0.01176853, 0.01176853  (x, y)\nextent      : 11.7188, 11.78941, 50.08395, 50.13102  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource(s)   : memory\nname        : minibeispiel_raster \nmin value   :            1.000000 \nmax value   :            2.586555 \n\n\n\nplot(rast_Test)\nplot(rast_Test_New)\n\n\n\n\n\n\nOriginal\n\n\n\n\n\n\n\nNew\n\n\n\n\n\n\n\n\n4.3.2 With Mask Raster\nA second way is provide an existing SpatRaster with the geometry you desire, with special boundary and resolution, this is a better way.\n\n# New CRS\nrast_Mask &lt;- rast(ncol=10, nrow=10, xmin=265000, xmax=270000, ymin=5553000, ymax=5558000)\ncrs(rast_Mask) &lt;- \"EPSG:25833\"\nvalues(rast_Mask) &lt;- 1\n# Reproject\nrast_Test_New &lt;- project(rast_Test, rast_Mask)\n\n# Info and Plot of vector layer\nrast_Test_New\n\nclass       : SpatRaster \ndimensions  : 10, 10, 1  (nrow, ncol, nlyr)\nresolution  : 500, 500  (x, y)\nextent      : 265000, 270000, 5553000, 5558000  (xmin, xmax, ymin, ymax)\ncoord. ref. : ETRS89 / UTM zone 33N (EPSG:25833) \nsource(s)   : memory\nname        : minibeispiel_raster \nmin value   :                   1 \nmax value   :                   3 \n\n\n\nplot(rast_Test)\nplot(rast_Test_New)\n\n\n\n\n\n\nOriginal\n\n\n\n\n\n\n\nNew"
  },
  {
    "objectID": "dataprocess/spatial_data.html#attributes-manipulation",
    "href": "dataprocess/spatial_data.html#attributes-manipulation",
    "title": "Basic Manipulation",
    "section": "5.1 Attributes manipulation",
    "text": "5.1 Attributes manipulation\n\n5.1.1 Extract all Attributes\n\nas.data.frame()\n\n\ndf_Attr &lt;- as.data.frame(vect_Test)\ndf_Attr\n\n  ID_region Name region_area\n1         1    a     8853404\n2         2    b     2208109\n\n\n\n\n5.1.2 Extract one with attribute name\n\n$name\n[, \"name\"]\n\n\nvect_Test$ID_region\n\n[1] 1 2\n\nvect_Test[,\"ID_region\"]\n\n class       : SpatVector \n geometry    : polygons \n dimensions  : 2, 1  (geometries, attributes)\n extent      : 4480500, 4484666, 5550100, 5554933  (xmin, xmax, ymin, ymax)\n source      : minibeispiel_polygon.geojson\n coord. ref. : DHDN / 3-degree Gauss-Kruger zone 4 (EPSG:31468) \n names       : ID_region\n type        :     &lt;int&gt;\n values      :         1\n                       2\n\n\n\n\n5.1.3 Add a new attribute\n\n$name &lt;-\n[, \"name\"] &lt;-\n\n\nvect_Test$New_Attr &lt;- c(\"n1\", \"n2\")\nvect_Test[,\"New_Attr\"] &lt;- c(\"n1\", \"n2\")\n\n\n\n5.1.4 Merge several attributes\n\nsame order\n\ncbind()\n\ncommon (key-)attributes\n\nmerge()\n\n\n\ndf_New_Attr &lt;- data.frame(Name = c(\"a\", \"b\"), new_Attr2 = c(9, 6))\n\ncbind(vect_Test, df_New_Attr)\n\n class       : SpatVector \n geometry    : polygons \n dimensions  : 2, 6  (geometries, attributes)\n extent      : 4480500, 4484666, 5550100, 5554933  (xmin, xmax, ymin, ymax)\n source      : minibeispiel_polygon.geojson\n coord. ref. : DHDN / 3-degree Gauss-Kruger zone 4 (EPSG:31468) \n names       : ID_region  Name region_area New_Attr  Name new_Attr2\n type        :     &lt;int&gt; &lt;chr&gt;       &lt;num&gt;    &lt;chr&gt; &lt;chr&gt;     &lt;num&gt;\n values      :         1     a   8.853e+06       n1     a         9\n                       2     b   2.208e+06       n2     b         6\n\nmerge(vect_Test, df_New_Attr, by = \"Name\")\n\n class       : SpatVector \n geometry    : polygons \n dimensions  : 2, 5  (geometries, attributes)\n extent      : 4480500, 4484666, 5550100, 5554933  (xmin, xmax, ymin, ymax)\n coord. ref. : DHDN / 3-degree Gauss-Kruger zone 4 (EPSG:31468) \n names       :  Name ID_region region_area New_Attr new_Attr2\n type        : &lt;chr&gt;     &lt;int&gt;       &lt;num&gt;    &lt;chr&gt;     &lt;num&gt;\n values      :     a         1   8.853e+06       n1         9\n                   b         2   2.208e+06       n2         6\n\n\n\n\n5.1.5 Delete a attribute\n\n$name &lt;- NULL\n\n\nvect_Test$New_Attr &lt;- c(\"n1\", \"n2\")\nvect_Test[,\"New_Attr\"] &lt;- c(\"n1\", \"n2\")"
  },
  {
    "objectID": "dataprocess/spatial_data.html#object-append-and-aggregate",
    "href": "dataprocess/spatial_data.html#object-append-and-aggregate",
    "title": "Basic Manipulation",
    "section": "5.2 Object Append and aggregate",
    "text": "5.2 Object Append and aggregate\n\n5.2.1 Append new Objects\n\nrbind()\n\n\n# New Vect\n# Define the coordinate reference system (CRS) with EPSG codes\ncrs_31468 &lt;- \"EPSG:31468\"\n\n# Define coordinates for the first polygon\nx_polygon_3 &lt;- c(4480400, 4481222, 4480500)\ny_polygon_3 &lt;- c(5551000, 5550666, 5552555)\nxy_polygon_3 &lt;- cbind(id=3, part=1, x_polygon_3, y_polygon_3)\n\n# Create a vector layer for the polygons, specifying their type, attributes, CRS, and additional attributes\nvect_New &lt;- vect(xy_polygon_3, type=\"polygons\", atts = data.frame(ID_region = 3, Name = c(\"b\")), crs = crs_31468)\nvect_New$region_area &lt;- expanse(vect_New)\n\n# Append the objects\nvect_Append &lt;- rbind(vect_Test, vect_New)\nvect_Append\n\n class       : SpatVector \n geometry    : polygons \n dimensions  : 3, 4  (geometries, attributes)\n extent      : 4480400, 4484666, 5550100, 5554933  (xmin, xmax, ymin, ymax)\n source      : minibeispiel_polygon.geojson\n coord. ref. : DHDN / 3-degree Gauss-Kruger zone 4 (EPSG:31468) \n names       : ID_region  Name region_area New_Attr\n type        :     &lt;int&gt; &lt;chr&gt;       &lt;num&gt;    &lt;chr&gt;\n values      :         1     a   8.853e+06       n1\n                       2     b   2.208e+06       n2\n                       3     b   6.558e+05       NA\n\n\n\n\n5.2.2 Aggregate / Dissolve\nIt is common to aggregate (“dissolve”) polygons that have the same value for an attribute of interest.\n\naggregate()\n\n\nvect_Aggregated &lt;- terra::aggregate(vect_Append, by = \"Name\")\nvect_Aggregated\n\n class       : SpatVector \n geometry    : polygons \n dimensions  : 2, 5  (geometries, attributes)\n extent      : 4480400, 4484666, 5550100, 5554933  (xmin, xmax, ymin, ymax)\n coord. ref. : DHDN / 3-degree Gauss-Kruger zone 4 (EPSG:31468) \n names       :  Name mean_ID_region mean_region_area New_Attr agg_n\n type        : &lt;chr&gt;          &lt;num&gt;            &lt;num&gt;    &lt;chr&gt; &lt;int&gt;\n values      :     a              1        8.853e+06       n1     1\n                   b            2.5        1.432e+06       NA     2\n\n\n\nplot(vect_Append, \"ID_region\")\nplot(vect_Aggregated, \"Name\")\n\n\n\n\n\n\nOriginal\n\n\n\n\n\n\n\nAggregated"
  },
  {
    "objectID": "dataprocess/spatial_data.html#overlap",
    "href": "dataprocess/spatial_data.html#overlap",
    "title": "Basic Manipulation",
    "section": "5.3 Overlap",
    "text": "5.3 Overlap\nTo perform operations that involve overlap between two vector datasets, we will create a new vector dataset:\n\nvect_Overlap &lt;- as.polygons(rast_Test)[1,]\nnames(vect_Overlap) &lt;- \"ID_Rast\"\n\nplot(vect_Overlap, \"ID_Rast\")\n\n\n\n\n\n5.3.1 Erase\n\nerase()\n\n\nvect_Erase &lt;- erase(vect_Test, vect_Overlap)\nplot(vect_Erase, \"ID_region\")\n\n\n\n\n\n\n5.3.2 Intersect\n\nintersect()\n\n\nvect_Intersect &lt;- terra::intersect(vect_Test, vect_Overlap)\nplot(vect_Intersect, \"ID_region\")\n\n\n\n\n\n\n5.3.3 Union\nAppends the geometries and attributes of the input.\n\nunion()\n\n\nvect_Union &lt;- terra::union(vect_Test, vect_Overlap)\nplot(vect_Union, \"ID_region\")\n\n\n\n\n\n\n5.3.4 Cover\ncover() is a combination of intersect() and union(). intersect returns new (intersected) geometries with the attributes of both input datasets. union appends the geometries and attributes of the input. cover returns the intersection and appends the other geometries and attributes of both datasets.\n\ncover()\n\n\nvect_Cover &lt;- terra::cover(vect_Test, vect_Overlap)\nplot(vect_Cover, \"ID_region\")\n\n\n\n\n\n\n5.3.5 Difference\n\nsymdif()\n\n\nvect_Difference &lt;- terra::symdif(vect_Test, vect_Overlap)\nplot(vect_Difference, \"ID_region\")"
  },
  {
    "objectID": "dataprocess/spatial_data.html#raster-algebra",
    "href": "dataprocess/spatial_data.html#raster-algebra",
    "title": "Basic Manipulation",
    "section": "6.1 Raster algebra",
    "text": "6.1 Raster algebra\nMany generic functions that allow for simple and elegant raster algebra have been implemented for Raster objects, including the normal algebraic operators such as +, -, *, /, logical operators such as &gt;, &gt;=, &lt;, ==, !, and functions like abs, round, ceiling, floor, trunc, sqrt, log, log10, exp, cos, sin, atan, tan, max, min, range, prod, sum, any, all. In these functions, you can mix raster objects with numbers, as long as the first argument is a raster object. (Spatial Data Science)\n\nrast_Add &lt;- rast_Test + 10\nplot(rast_Add)"
  },
  {
    "objectID": "dataprocess/spatial_data.html#replace-with-condition",
    "href": "dataprocess/spatial_data.html#replace-with-condition",
    "title": "Basic Manipulation",
    "section": "6.2 Replace with Condition",
    "text": "6.2 Replace with Condition\n\nrast[condition] &lt;-\n\n\n# Copy to a new raster\nrast_Replace &lt;- rast_Test\n\n# Replace\nrast_Replace[rast_Replace &gt; 1] &lt;- 10\nplot(rast_Replace)"
  },
  {
    "objectID": "dataprocess/spatial_data.html#summary-of-multi-layers",
    "href": "dataprocess/spatial_data.html#summary-of-multi-layers",
    "title": "Basic Manipulation",
    "section": "6.3 Summary of multi-layers",
    "text": "6.3 Summary of multi-layers\n\nrast_Mean &lt;- mean(rast_Test, rast_Replace)\nplot(rast_Mean)"
  },
  {
    "objectID": "dataprocess/spatial_data.html#aggregate-and-disaggregate",
    "href": "dataprocess/spatial_data.html#aggregate-and-disaggregate",
    "title": "Basic Manipulation",
    "section": "6.4 Aggregate and disaggregate",
    "text": "6.4 Aggregate and disaggregate\n\naggregate()\ndisagg()\n\n\nrast_Aggregate &lt;- aggregate(rast_Test, 2)\nplot(rast_Aggregate)\n\n\n\n\n\nrast_Disagg &lt;- disagg(rast_Test, 2)\nrast_Disagg\n\nclass       : SpatRaster \ndimensions  : 10, 10, 1  (nrow, ncol, nlyr)\nresolution  : 500, 500  (x, y)\nextent      : 4480000, 4485000, 5550000, 5555000  (xmin, xmax, ymin, ymax)\ncoord. ref. : DHDN / 3-degree Gauss-Kruger zone 4 (EPSG:31468) \nsource(s)   : memory\nname        : minibeispiel_raster \nmin value   :                   1 \nmax value   :                   3 \n\nplot(rast_Disagg)"
  },
  {
    "objectID": "dataprocess/spatial_data.html#crop",
    "href": "dataprocess/spatial_data.html#crop",
    "title": "Basic Manipulation",
    "section": "6.5 Crop",
    "text": "6.5 Crop\nThe crop function lets you take a geographic subset of a larger raster object with an extent. But you can also use other spatial object, in them an extent can be extracted.\n\ncrop()\n\nwith extention\nwith rster\nwith vector\n\n\n\nrast_Crop &lt;- crop(rast_Test, vect_Test[1,])\nplot(rast_Crop)"
  },
  {
    "objectID": "dataprocess/spatial_data.html#trim",
    "href": "dataprocess/spatial_data.html#trim",
    "title": "Basic Manipulation",
    "section": "6.6 Trim",
    "text": "6.6 Trim\n\ntrim()\n\nTrim (shrink) a SpatRaster by removing outer rows and columns that are NA or another value.\n\nrast_Trim0 &lt;- rast_Test\nrast_Trim0[21:25] &lt;- NA\nrast_Trim &lt;- trim(rast_Trim0)\n\n\nplot(rast_Trim0)\nplot(rast_Trim)\n\n\n\n\n\n\nwith NA\n\n\n\n\n\n\n\nTrimed"
  },
  {
    "objectID": "dataprocess/spatial_data.html#mask",
    "href": "dataprocess/spatial_data.html#mask",
    "title": "Basic Manipulation",
    "section": "6.7 Mask",
    "text": "6.7 Mask\n\nmask()\ncrop(mask = TRUE) = mask() + trim()\n\nWhen you use mask manipulation in spatial data analysis, it involves setting the cells that are not covered by a mask to NA (Not Available) values. If you apply the crop(mask = TRUE) operation, it means that not only will the cells outside of the mask be set to NA, but the resulting raster will also be cropped to match the extent of the mask.\n\nrast_Mask &lt;- mask(rast_Disagg, vect_Test[1,])\nrast_CropMask &lt;- crop(rast_Disagg, vect_Test[1,], mask = TRUE)\n\n\nplot(rast_Mask)\nplot(rast_CropMask)\n\n\n\n\n\n\nMask\n\n\n\n\n\n\n\nMask + Crop (Trim)"
  }
]