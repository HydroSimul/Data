[
  {
    "objectID": "modelling/index.html",
    "href": "modelling/index.html",
    "title": "Hydrological Modelling",
    "section": "",
    "text": "Hydrological modeling is a scientific approach aimed at simulating and comprehending the dynamics of the Earth’s water cycle. On this page, we will compile information on hydrological models, as well as the processes involved in their application. This includes running the model, evaluating its performance, calibrating and conducting sensitivity analyses, and ultimately validating the model’s results. Through this comprehensive exploration, we aim to provide valuable insights into the world of hydrological modeling."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcom Page",
    "section": "",
    "text": "Welcome to HydroSimul: Your Gateway to Understanding Hydrological Simulation\nWe’re thrilled to have you at HydroSimul, your premier resource for delving into the realm of hydrological simulation. Here, we curate a vast collection of datasets and offer cutting-edge techniques to empower your exploration.\nAt HydroSimul, we believe in the power of collaboration. If you want to share your knowledge, experience, and skills in the field, please contact us at hydro.simul@gmail.com. We also welcome your advice.\nOur website serves as a comprehensive hub for your hydrological journey:"
  },
  {
    "objectID": "index.html#dataset-collection",
    "href": "index.html#dataset-collection",
    "title": "Welcom Page",
    "section": "1. Dataset Collection:",
    "text": "1. Dataset Collection:\nNumerous open-access datasets are readily available for use in hydrological modeling, including meteorological, hydrological, and various geophysical datasets. Within this dataset collection, we not only provide direct links to the datasets but also present essential information in a standardized format, simplifying your dataset selection process. Additionally, we strive to establish connections with research papers that have utilized these datasets and offer valuable feedback gleaned from these sources."
  },
  {
    "objectID": "index.html#data-processing",
    "href": "index.html#data-processing",
    "title": "Welcom Page",
    "section": "2. Data Processing:",
    "text": "2. Data Processing:\nOn this page, we provide you with a wealth of data processing tools and techniques. Discover how to adeptly clean, preprocess, and convert raw hydrological data into a valuable format suitable for analysis and modeling."
  },
  {
    "objectID": "index.html#data-analysis",
    "href": "index.html#data-analysis",
    "title": "Welcom Page",
    "section": "3. Data Analysis:",
    "text": "3. Data Analysis:\nUncover concealed statistical insights and trends within your data. Our comprehensive guides and tutorials are designed to empower you with the skills to effectively analyze hydrological data."
  },
  {
    "objectID": "index.html#hydrological-modeling",
    "href": "index.html#hydrological-modeling",
    "title": "Welcom Page",
    "section": "4. Hydrological Modeling:",
    "text": "4. Hydrological Modeling:\nTake your understanding to the next level with hydrological modeling. Explore various models and model frameworks and acquire hands-on experience in simulating complex hydrological processes. Additionally, we provide extensive resources on calibration algorithms and strategies to significantly improve your modeling outcomes."
  },
  {
    "objectID": "dataset/index.html",
    "href": "dataset/index.html",
    "title": "Dataset",
    "section": "",
    "text": "Numerous open-access datasets are readily available for use in hydrological modeling, including meteorological, hydrological, and various geophysical datasets. Within this dataset collection, we not only provide direct links to the datasets but also present essential information in a standardized format, simplifying your dataset selection process. Additionally, we strive to establish connections with research papers that have utilized these datasets and offer valuable feedback gleaned from these sources."
  },
  {
    "objectID": "dataset/geoph.html",
    "href": "dataset/geoph.html",
    "title": "Geophysical",
    "section": "",
    "text": "About this site test1\n\n1 + 1\n\n[1] 2\n\n\n\n\nAbout this site test2\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "dataset/geoph.html#h2",
    "href": "dataset/geoph.html#h2",
    "title": "Geophysical",
    "section": "",
    "text": "About this site test2\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "dataprocess/timeserises_analyse.html",
    "href": "dataprocess/timeserises_analyse.html",
    "title": "Time Series Analyse",
    "section": "",
    "text": "A time series is often adequately described as a function of four components: trend, seasonality, dependent stochastic component and independent residual component (Machiwal and Jha 2012). It can be mathematically expressed as (Shahin, Oorschot, and Lange 1993):\n\\[\nx_{\\mathrm{t}}=T_{\\mathrm{t}}+S_{\\mathrm{t}}+\\varepsilon_{\\mathrm{t}}+\\eta_{\\mathrm{t}}\n\\]\nwhere\n\n\\(T_{\\mathrm{t}}\\) = trend component,\n\\(S_{\\mathrm{t}}\\) = seasonality,\n\\(\\varepsilon_{\\mathrm{t}}\\) = dependent stochastic component, and\n\\(\\eta_{\\mathrm{t}}\\) = independent residual component.\n\nThe first two components can be treat as systematic pattern, which are deterministic in nature, whereas the stochastic component accounts for the random error.\n\n\nThe term ‘homogeneity’ implies that the data in the series belong to one population, and therefore have a time invariant mean (Machiwal and Jha 2012). Homogeneity in the time dimension is one aspect of stationarity. It means that the statistical characteristics of the time series, like its mean and variance, don’t change significantly across different time periods.\nA time series is said to be strictly stationary if its statistical properties do not vary with changes of time origin. A less strict type of stationarity, called weak stationarity or second-order stationarity, is that in which the first- and secondorder moments depend only on time differences (Chen and Rao 2002). In nature, strictly stationary time series does not exist, and weakly stationary time series is practically considered as stationary time series (Machiwal and Jha 2012).\nA ‘trend’ is defined as “a unidirectional and gradual change (falling or rising) in the mean value of a variable” (Shahin, Oorschot, and Lange 1993).\nA time series is said to have trends, if there is a significant correlation (positive or negative) between the observed values and time. Trends and shifts in a hydrologic time series are usually introduced due to gradual natural or human-induced changes in the hydrologic environment producing the time series (Haan 1977).\n\n\n\n‘Periodicity’ represents a regular or oscillatory form of movement that is recurring over a fixed interval of time (Shahin, Oorschot, and Lange 1993). It generally occurs due to astronomic cycles such as earth’s rotation around the sun (Haan 1977).\n\nAnnual: Precipetation, evapotranspiration\nWeekly: water-use data of domestic, industrial\n\n\n\n\nthe phenomenon of ‘persistence’ is highly relevant to the hydrologic time series, which means that the successive members of a time series are linked in some dependent manner (Shahin, Oorschot, and Lange 1993). In other words, ‘persistence’ denotes the tendency for the magnitude of an event to be dependent on the magnitude of previous event(s), i.e., a memory effect (Machiwal and Jha 2012)."
  },
  {
    "objectID": "dataprocess/timeserises_analyse.html#stationarity-homogeneity-and-trend",
    "href": "dataprocess/timeserises_analyse.html#stationarity-homogeneity-and-trend",
    "title": "Time Series Analyse",
    "section": "",
    "text": "The term ‘homogeneity’ implies that the data in the series belong to one population, and therefore have a time invariant mean (Machiwal and Jha 2012). Homogeneity in the time dimension is one aspect of stationarity. It means that the statistical characteristics of the time series, like its mean and variance, don’t change significantly across different time periods.\nA time series is said to be strictly stationary if its statistical properties do not vary with changes of time origin. A less strict type of stationarity, called weak stationarity or second-order stationarity, is that in which the first- and secondorder moments depend only on time differences (Chen and Rao 2002). In nature, strictly stationary time series does not exist, and weakly stationary time series is practically considered as stationary time series (Machiwal and Jha 2012).\nA ‘trend’ is defined as “a unidirectional and gradual change (falling or rising) in the mean value of a variable” (Shahin, Oorschot, and Lange 1993).\nA time series is said to have trends, if there is a significant correlation (positive or negative) between the observed values and time. Trends and shifts in a hydrologic time series are usually introduced due to gradual natural or human-induced changes in the hydrologic environment producing the time series (Haan 1977)."
  },
  {
    "objectID": "dataprocess/timeserises_analyse.html#periodicity",
    "href": "dataprocess/timeserises_analyse.html#periodicity",
    "title": "Time Series Analyse",
    "section": "",
    "text": "‘Periodicity’ represents a regular or oscillatory form of movement that is recurring over a fixed interval of time (Shahin, Oorschot, and Lange 1993). It generally occurs due to astronomic cycles such as earth’s rotation around the sun (Haan 1977).\n\nAnnual: Precipetation, evapotranspiration\nWeekly: water-use data of domestic, industrial"
  },
  {
    "objectID": "dataprocess/timeserises_analyse.html#persistence",
    "href": "dataprocess/timeserises_analyse.html#persistence",
    "title": "Time Series Analyse",
    "section": "",
    "text": "the phenomenon of ‘persistence’ is highly relevant to the hydrologic time series, which means that the successive members of a time series are linked in some dependent manner (Shahin, Oorschot, and Lange 1993). In other words, ‘persistence’ denotes the tendency for the magnitude of an event to be dependent on the magnitude of previous event(s), i.e., a memory effect (Machiwal and Jha 2012)."
  },
  {
    "objectID": "dataprocess/statistic_basic.html",
    "href": "dataprocess/statistic_basic.html",
    "title": "Statistic Basic",
    "section": "",
    "text": "One of the most important tasks while analyzing any time series is to describe and summarize the time series data in forms, which easily convey their important characteristics.\nKey statistical characteristics often described include: a measure of the central tendency of the data, a measure of spread or variability, a measure of the symmetry of the data distribution, and perhaps estimates of extremes such as some large or small percentile (Snedecor and Cochran 1980).\n\n\nAccording to Helsel and Hirsch (2020), the data about which a statement or summary is to be made are called ‘population’ or sometimes ‘target population’. It may be impossible both physically and economically to collect all data of interest. Alternatively, a subset of the entire data called ‘sample’ is selected and measured in such a way that conclusions about the sample may be extended to the entire population.\n\n\n\nIn statistics, measures of location or central tendency are used to summarize and describe the central or typical value in a dataset. Here are the six common measures of location (Machiwal and Jha 2012):\n\nMean: The mean, often referred to as the average, is calculated by summing all the values in a dataset and dividing by the number of values. It represents the balance point of the data.\nMedian: The median is the middle value when the data is sorted in ascending order. It’s less sensitive to extreme values (outliers) than the mean and is a good measure of the central value when the data is skewed.\nMode: The mode is the value that appears most frequently in the dataset. There can be multiple modes in a dataset, and it’s useful for categorical or discrete data.\nGeometric Mean: The geometric mean is used for data that is not normally distributed, such as financial returns or growth rates. It’s calculated by taking the nth root of the product of n values.\nTrimmed Mean: The trimmed mean is a variation of the mean that removes a certain percentage of extreme values (usually a specified percentage from both tails of the distribution) before calculating the mean. This makes it more robust to outliers.\n\nAmong these measures, the mean and median are the most widely used for summarizing data.\n\n\nThe arithmetic mean (\\(\\overline{{x}}\\)) is calculated by summing up of all data values, \\(x_{\\mathrm{i}}\\) and dividing the sum by the sample size \\(n\\):\n\\[\n{\\overline{{x}}}=\\sum_{i=1}^{n}{\\frac{x_{\\mathrm{i}}}{n}}\n\\]\n\n\n\nThe median is the middle value in a dataset when the data is ordered from smallest to largest. It’s a robust measure of central tendency that is not influenced by extreme values (outliers).\nFor an ordered dataset with ‘n’ values:\n\nIf ‘n’ is odd, the median is the middle value: \\[\n\\text{M} = x_{\\frac{n+1}{2}}\n\\]\nIf ‘n’ is even, the median is the average of the two middle values: \\[\n\\text{M} = \\frac{x_{\\frac{n}{2}} + x_{\\frac{n}{2}+1}}{2}\n\\]\n\n\n\n\nThe geometric mean (GM) is often used to compute summary statistic for positively skewed datasets (Machiwal and Jha 2012).\n\\[\n{\\mathrm{GM}}={\\mathrm{exp}}\\left[\\sum_{i=1}^{n}{\\frac{\\ln\\left(x_{\\mathrm{i}}\\right)}{n}}\\right]\n\\]\nFor the positively skewed data series, the GM is usually fairly close to the median of the series. In fact, the GM is an unbiased estimate of the median when the logarithms of the datasets are symmetric (Helsel et al. 2020).\n\n\n\n\n\n\nThe ‘sample variance’ and ‘sample standard deviation’ (square root of sample variance) are classical measures of spread (dispersion), which are the most common measures of dispersion (Machiwal and Jha 2012).\n\\[\ns^{2}=\\sum_{i=1}^{n}\\frac{\\left(x_{\\mathrm{i}}-{\\overline{{x}}}\\,\\right)^{2}}{\\left(n-1\\right)}\n\\]\n\\[\ns={\\sqrt{\\sum_{i=1}^{n}{\\frac{\\left(x_{i}-{\\overline{{x}}}\\,\\right)^{2}}{\\left(n-1\\right)}}}}\n\\]\n\n\n\nRobust measures of spreading about the mean include ‘range’, ‘interquartile range’, ‘coefficient of variation’ and ‘median absolute deviation’ (Machiwal and Jha 2012).\n\n\nQuantiles are values that divide a dataset into equally sized subsets. Common quantiles include quartiles (dividing data into four parts), quintiles (dividing into five parts), deciles (dividing into ten parts), and percentiles (dividing into one hundred parts).\n\nSort the dataset in ascending order.\nCompute the index ‘i’ as\n\n\\[\ni = \\text{round}((n+1) \\cdot q)\n\\]\n\nIf ‘i’ is an integer, the quantile is\n\n\\[\n\\text{Q}(q) = x_i\n\\] - If ‘i’ is not an integer, the quantile is interpolated as\n\\[\n\\text{Q}(q) = x_{\\lfloor i \\rfloor} + (i - \\lfloor i \\rfloor) \\cdot (x_{\\lfloor i \\rfloor + 1} - x_{\\lfloor i \\rfloor})\n\\]\nQuantiles are used to understand the spread and distribution of data and are often used in box plots and histograms to visualize data distribution.\n\n\n\nThe coefficient of variation (CV) gives a normalized measure of spreading about the mean, and is estimated as (Machiwal and Jha 2012):\n\\[\n\\mathbf{C}\\mathbf{V}(\\vartheta_{0})={\\frac{s}{\\bar{x}}}\\times100\n\\]\nHydrologic variables with larger CV values are more variable than those with smaller values. Wilding (in (Nielsen and Bouma 1985)) suggested a classification scheme for identifying the extent of variability for soil properties based on their CV values, where CV values of 0-15, 16-35 and &gt;36 indicate little, moderate and high variability, respectively.\n\n\n\nQuartile coefficient (QC) of dispersion is another descriptive statistic which measures dispersion and is used to make comparison within and between datasets. The test-statistic is computed using the first (P25) and third (P75) quartiles for each data set. The quartile coefficient of dispersion (QC) is given as (Machiwal and Jha 2012):\n\\[\n\\text{QC}={\\frac{P_{75}-P_{25}}{P_{75}+P_{25}}}\n\\]\n\n\n\n\n\nHydrologic time series data are usually skewed, which means that data in the time series are not symmetric around the mean or median, with extreme values extending out longer in one direction (Machiwal and Jha 2012).\n\n\nIt is defined as the adjusted third moment about the mean divided by the cube of the standard deviation (s), and is mathematically expressed as follows:\n\\[\ng={\\frac{n}{\\left(n-1\\right)\\,\\left(n-2\\right)}}\\sum_{i=1}^{n}{\\frac{\\left(x_{i}-{\\overline{{x}}}\\,\\right)^{3}}{s^{3}}}\n\\]\nA positively skewed distribution of hydrologic time series with right extended tail has a positive coefficient of skewness, whereas a time series with negative-skewed distribution with left extended tail has a negative coefficient of skewness (Machiwal and Jha 2012).\n\n\n\nA robust measure of skewness is the ‘quartile skew coefficient (QS)’, which is defined as the difference in distances of the upper and lower quartiles from the median, divided by the IQR (Kenney John F 1939). Mathematically, it is expressed as:\n\\[\n\\text{QS}=\\frac{\\left(P_{75}-P_{50}\\,\\right)-\\left(P_{50}-P_{25}\\,\\right)}{P_{75}-P_{25}}\n\\]"
  },
  {
    "objectID": "dataprocess/statistic_basic.html#population-and-sample",
    "href": "dataprocess/statistic_basic.html#population-and-sample",
    "title": "Statistic Basic",
    "section": "",
    "text": "According to Helsel and Hirsch (2020), the data about which a statement or summary is to be made are called ‘population’ or sometimes ‘target population’. It may be impossible both physically and economically to collect all data of interest. Alternatively, a subset of the entire data called ‘sample’ is selected and measured in such a way that conclusions about the sample may be extended to the entire population."
  },
  {
    "objectID": "dataprocess/statistic_basic.html#measures-of-location",
    "href": "dataprocess/statistic_basic.html#measures-of-location",
    "title": "Statistic Basic",
    "section": "",
    "text": "In statistics, measures of location or central tendency are used to summarize and describe the central or typical value in a dataset. Here are the six common measures of location (Machiwal and Jha 2012):\n\nMean: The mean, often referred to as the average, is calculated by summing all the values in a dataset and dividing by the number of values. It represents the balance point of the data.\nMedian: The median is the middle value when the data is sorted in ascending order. It’s less sensitive to extreme values (outliers) than the mean and is a good measure of the central value when the data is skewed.\nMode: The mode is the value that appears most frequently in the dataset. There can be multiple modes in a dataset, and it’s useful for categorical or discrete data.\nGeometric Mean: The geometric mean is used for data that is not normally distributed, such as financial returns or growth rates. It’s calculated by taking the nth root of the product of n values.\nTrimmed Mean: The trimmed mean is a variation of the mean that removes a certain percentage of extreme values (usually a specified percentage from both tails of the distribution) before calculating the mean. This makes it more robust to outliers.\n\nAmong these measures, the mean and median are the most widely used for summarizing data.\n\n\nThe arithmetic mean (\\(\\overline{{x}}\\)) is calculated by summing up of all data values, \\(x_{\\mathrm{i}}\\) and dividing the sum by the sample size \\(n\\):\n\\[\n{\\overline{{x}}}=\\sum_{i=1}^{n}{\\frac{x_{\\mathrm{i}}}{n}}\n\\]\n\n\n\nThe median is the middle value in a dataset when the data is ordered from smallest to largest. It’s a robust measure of central tendency that is not influenced by extreme values (outliers).\nFor an ordered dataset with ‘n’ values:\n\nIf ‘n’ is odd, the median is the middle value: \\[\n\\text{M} = x_{\\frac{n+1}{2}}\n\\]\nIf ‘n’ is even, the median is the average of the two middle values: \\[\n\\text{M} = \\frac{x_{\\frac{n}{2}} + x_{\\frac{n}{2}+1}}{2}\n\\]\n\n\n\n\nThe geometric mean (GM) is often used to compute summary statistic for positively skewed datasets (Machiwal and Jha 2012).\n\\[\n{\\mathrm{GM}}={\\mathrm{exp}}\\left[\\sum_{i=1}^{n}{\\frac{\\ln\\left(x_{\\mathrm{i}}\\right)}{n}}\\right]\n\\]\nFor the positively skewed data series, the GM is usually fairly close to the median of the series. In fact, the GM is an unbiased estimate of the median when the logarithms of the datasets are symmetric (Helsel et al. 2020)."
  },
  {
    "objectID": "dataprocess/statistic_basic.html#measures-of-spreaddispersion",
    "href": "dataprocess/statistic_basic.html#measures-of-spreaddispersion",
    "title": "Statistic Basic",
    "section": "",
    "text": "The ‘sample variance’ and ‘sample standard deviation’ (square root of sample variance) are classical measures of spread (dispersion), which are the most common measures of dispersion (Machiwal and Jha 2012).\n\\[\ns^{2}=\\sum_{i=1}^{n}\\frac{\\left(x_{\\mathrm{i}}-{\\overline{{x}}}\\,\\right)^{2}}{\\left(n-1\\right)}\n\\]\n\\[\ns={\\sqrt{\\sum_{i=1}^{n}{\\frac{\\left(x_{i}-{\\overline{{x}}}\\,\\right)^{2}}{\\left(n-1\\right)}}}}\n\\]\n\n\n\nRobust measures of spreading about the mean include ‘range’, ‘interquartile range’, ‘coefficient of variation’ and ‘median absolute deviation’ (Machiwal and Jha 2012).\n\n\nQuantiles are values that divide a dataset into equally sized subsets. Common quantiles include quartiles (dividing data into four parts), quintiles (dividing into five parts), deciles (dividing into ten parts), and percentiles (dividing into one hundred parts).\n\nSort the dataset in ascending order.\nCompute the index ‘i’ as\n\n\\[\ni = \\text{round}((n+1) \\cdot q)\n\\]\n\nIf ‘i’ is an integer, the quantile is\n\n\\[\n\\text{Q}(q) = x_i\n\\] - If ‘i’ is not an integer, the quantile is interpolated as\n\\[\n\\text{Q}(q) = x_{\\lfloor i \\rfloor} + (i - \\lfloor i \\rfloor) \\cdot (x_{\\lfloor i \\rfloor + 1} - x_{\\lfloor i \\rfloor})\n\\]\nQuantiles are used to understand the spread and distribution of data and are often used in box plots and histograms to visualize data distribution.\n\n\n\nThe coefficient of variation (CV) gives a normalized measure of spreading about the mean, and is estimated as (Machiwal and Jha 2012):\n\\[\n\\mathbf{C}\\mathbf{V}(\\vartheta_{0})={\\frac{s}{\\bar{x}}}\\times100\n\\]\nHydrologic variables with larger CV values are more variable than those with smaller values. Wilding (in (Nielsen and Bouma 1985)) suggested a classification scheme for identifying the extent of variability for soil properties based on their CV values, where CV values of 0-15, 16-35 and &gt;36 indicate little, moderate and high variability, respectively.\n\n\n\nQuartile coefficient (QC) of dispersion is another descriptive statistic which measures dispersion and is used to make comparison within and between datasets. The test-statistic is computed using the first (P25) and third (P75) quartiles for each data set. The quartile coefficient of dispersion (QC) is given as (Machiwal and Jha 2012):\n\\[\n\\text{QC}={\\frac{P_{75}-P_{25}}{P_{75}+P_{25}}}\n\\]"
  },
  {
    "objectID": "dataprocess/statistic_basic.html#measures-of-skewness",
    "href": "dataprocess/statistic_basic.html#measures-of-skewness",
    "title": "Statistic Basic",
    "section": "",
    "text": "Hydrologic time series data are usually skewed, which means that data in the time series are not symmetric around the mean or median, with extreme values extending out longer in one direction (Machiwal and Jha 2012).\n\n\nIt is defined as the adjusted third moment about the mean divided by the cube of the standard deviation (s), and is mathematically expressed as follows:\n\\[\ng={\\frac{n}{\\left(n-1\\right)\\,\\left(n-2\\right)}}\\sum_{i=1}^{n}{\\frac{\\left(x_{i}-{\\overline{{x}}}\\,\\right)^{3}}{s^{3}}}\n\\]\nA positively skewed distribution of hydrologic time series with right extended tail has a positive coefficient of skewness, whereas a time series with negative-skewed distribution with left extended tail has a negative coefficient of skewness (Machiwal and Jha 2012).\n\n\n\nA robust measure of skewness is the ‘quartile skew coefficient (QS)’, which is defined as the difference in distances of the upper and lower quartiles from the median, divided by the IQR (Kenney John F 1939). Mathematically, it is expressed as:\n\\[\n\\text{QS}=\\frac{\\left(P_{75}-P_{50}\\,\\right)-\\left(P_{50}-P_{25}\\,\\right)}{P_{75}-P_{25}}\n\\]"
  },
  {
    "objectID": "dataprocess/NetCDF.html",
    "href": "dataprocess/NetCDF.html",
    "title": "NetCDF",
    "section": "",
    "text": "NetCDF stands for “Network Common Data Form.” It is a file format that is designed to store large arrays of data, primarily used in scientific and engineering applications. NetCDF files are self-describing, meaning they contain metadata along with the data, which makes it easier to understand the contents. NetCDF is particularly well-suited for storing multi-dimensional data, such as time series, spatial data, and climate model outputs. It can handle data with complex structures like grids, which are common in environmental and geospatial datasets.\nIn simple terms, NetCDF is a file format for storing multi-dimensional arrays of data along with metadata.\nMore Details in unidata.\nNetCDF files have a hierarchical structure, consisting of dimensions, variables, and attributes. Dimensions define the size of arrays, variables hold the data, and attributes provide additional information about the data.\nWith these three components, you can efficiently handle the import, creation, and export of data in the NetCDF format."
  },
  {
    "objectID": "dataprocess/NetCDF.html#library",
    "href": "dataprocess/NetCDF.html#library",
    "title": "NetCDF",
    "section": "1 Library",
    "text": "1 Library\n\nRPython\n\n\nThe ncdf4 R package is a powerful tool for working with NetCDF data in R, allowing you to read, write, and manipulate datasets in this format with ease and efficiency.\n\nlibrary(ncdf4)\nlibrary(tidyverse)\n\n# Define the NetCDF file path\nfn_NetCDF &lt;- \"C:\\\\Lei\\\\HS_Web\\\\data_share\\\\minibeispiel_NetCDF.nc\"\n\n\n\nThe netCDF4 Python Library is a powerful tool for working with NetCDF data in R, allowing you to read, write, and manipulate datasets in this format with ease and efficiency.\n\nimport netCDF4 as nc\nimport numpy as np\n\n# Define the NetCDF file path\nfn_NetCDF = \"C:\\\\Lei\\\\HS_Web\\\\data_share\\\\minibeispiel_NetCDF.nc\"\n\n\n\n\nThe Test data minibeispiel_NetCDF.nc is avable from Github, but it can not be direcly read from Git hub so you need download to local."
  },
  {
    "objectID": "dataprocess/NetCDF.html#import",
    "href": "dataprocess/NetCDF.html#import",
    "title": "NetCDF",
    "section": "2 Import",
    "text": "2 Import\n\n2.1 Open\nThe first step in working with NetCDF files is to open the file using the nc_open() function. However, it’s important to note that opening the file doesn’t directly load its contents into the R environment. Instead, it establishes a connection between the file and the R session and effectively locks the file for reading or writing operations.\n\nRPython\n\n\n\n# Open the NetCDF file\nnc_Test &lt;- nc_open(fn_NetCDF)\n\n\n\n\n# Open the NetCDF file\nnc_Test = nc.Dataset(fn_NetCDF, \"r\")\n\n\n\n\n\n\n2.2 Basic Information\nAfter opening a NetCDF file in R, you can access the basic information about the dataset, which is contained in a list. This information typically includes details about three components: dimensions, variables, and attributes of the NetCDF file.\n\nRPython\n\n\n\n# Access the dimensions\n# nc_Test$dim\nnc_Test$dim |&gt; names()\n\n[1] \"latitude\"  \"longitude\" \"time\"     \n\n# Access the variables\n# nc_Test$var\nnc_Test$var |&gt; names()\n\n[1] \"T0\"  \"crs\"\n\nnc_Test$var$T0$size\n\n[1] 6 8 3\n\n# Access attributes\nncatt_get(nc_Test, 0)\n\n$title\n[1] \"Multidimensional data example\"\n\n$author\n[1] \"Kan, Lei, kan.lei@ruhr-uni-bochum.de\"\n\n\n\n\n\n# Access the dimensions\nprint(nc_Test.dimensions)\n\n{'latitude': &lt;class 'netCDF4._netCDF4.Dimension'&gt;: name = 'latitude', size = 6, 'longitude': &lt;class 'netCDF4._netCDF4.Dimension'&gt;: name = 'longitude', size = 8, 'time': &lt;class 'netCDF4._netCDF4.Dimension'&gt; (unlimited): name = 'time', size = 3}\n\n# Access the variables\nprint(nc_Test.variables)\n\n{'latitude': &lt;class 'netCDF4._netCDF4.Variable'&gt;\nfloat64 latitude(latitude)\n    units: degrees_north\n    long_name: latitude\nunlimited dimensions: \ncurrent shape = (6,)\nfilling on, default _FillValue of 9.969209968386869e+36 used, 'longitude': &lt;class 'netCDF4._netCDF4.Variable'&gt;\nfloat64 longitude(longitude)\n    units: degrees_east\n    long_name: longitude\nunlimited dimensions: \ncurrent shape = (8,)\nfilling on, default _FillValue of 9.969209968386869e+36 used, 'time': &lt;class 'netCDF4._netCDF4.Variable'&gt;\nint32 time(time)\n    units: day since 1961-01-01 00:00:00 +00\n    long_name: time\nunlimited dimensions: time\ncurrent shape = (3,)\nfilling on, default _FillValue of -2147483647 used, 'T0': &lt;class 'netCDF4._netCDF4.Variable'&gt;\nfloat32 T0(time, longitude, latitude)\n    units: cel\n    _FillValue: -9999.0\nunlimited dimensions: time\ncurrent shape = (3, 8, 6)\nfilling on, 'crs': &lt;class 'netCDF4._netCDF4.Variable'&gt;\nfloat32 crs()\n    long_name: coordinate reference system\n    EPSG: EPSG:4236\nunlimited dimensions: \ncurrent shape = ()\nfilling on, default _FillValue of 9.969209968386869e+36 used}\n\n# Get the size of the \"T0\" variable\nprint(nc_Test.variables[\"T0\"].size)\n\n144\n\n# Access attributes associated with the NetCDF file\nprint(nc_Test.__dict__)\n\n{'title': 'Multidimensional data example', 'author': 'Kan, Lei, kan.lei@ruhr-uni-bochum.de'}\n\n\n\n\n\n\n\n2.3 Values and Attributes\nWith the basic information about variables obtained, you can access the values and attributes of each variable as needed. You can also obtain specific subsets of variables using start points and counts for each dimension.\nAdditionally, dimensions are treated as variables in the NetCDF structure, making it easier to work with them.\n\nRPython\n\n\n\nncvar_get()\n\nstart: The starting point of every dimension to load variable values\ncount: The length of every dimension to read\n\nncatt_get()\n\n\n# Retrieve the variable \"T0\" WHOLE\nncvar_get(nc_Test, \"T0\")\n\n, , 1\n\n          [,1]      [,2]      [,3]      [,4]      [,5]      [,6]       [,7]\n[1,] 0.4243155 0.9355373 0.6876228 0.8002278 0.4408700 0.4708699 0.59079015\n[2,] 0.4031014 0.7100455 0.4772366 0.3784053 0.6279713 0.9022897 0.89029056\n[3,] 0.6261856 0.2216192 0.7574885 0.3998128 0.1462598 0.2891743 0.34810999\n[4,] 0.2051169 0.7212336 0.2052466 0.2371419 0.8615803 0.9635419 0.06449199\n[5,] 0.4202865 0.3519120 0.8772820 0.4323429 0.8802429 0.7829933 0.02408920\n[6,] 0.5832691 0.2425931 0.8981051 0.8781416 0.9701738 0.7729005 0.74079919\n           [,8]\n[1,] 0.96064091\n[2,] 0.02331837\n[3,] 0.59957153\n[4,] 0.49539265\n[5,] 0.30605116\n[6,] 0.88251084\n\n, , 2\n\n          [,1]      [,2]       [,3]      [,4]       [,5]      [,6]       [,7]\n[1,] 0.2956963 0.5266238 0.80266231 0.9940577 0.26920769 0.9580371 0.27879658\n[2,] 0.9417820 0.5029727 0.03080061 0.6261121 0.31751490 0.0802093 0.50476611\n[3,] 0.9786192 0.5408758 0.46791443 0.4680299 0.05485917 0.1339196 0.93703490\n[4,] 0.6288502 0.7536872 0.46902880 0.1146667 0.57469219 0.9119628 0.41187954\n[5,] 0.5857720 0.4706038 0.57941103 0.6392034 0.34206086 0.8067344 0.05105839\n[6,] 0.6905151 0.6474653 0.67272288 0.1335317 0.64459616 0.8997121 0.04749461\n          [,8]\n[1,] 0.7533994\n[2,] 0.7098062\n[3,] 0.2107040\n[4,] 0.3593461\n[5,] 0.8707433\n[6,] 0.5308710\n\n, , 3\n\n          [,1]      [,2]       [,3]       [,4]       [,5]       [,6]      [,7]\n[1,] 0.2964470 0.3185527 0.02501507 0.78078473 0.88581914 0.35747567 0.2205382\n[2,] 0.5802779 0.5473396 0.72289288 0.42013261 0.06668617 0.14891115 0.9619625\n[3,] 0.4290859 0.6458802 0.32526892 0.07880253 0.67707014 0.78467715 0.5030632\n[4,] 0.1994348 0.2251340 0.16004220 0.44191572 0.28654459 0.14325349 0.4434912\n[5,] 0.5352656 0.8188938 0.03500481 0.88045228 0.45287898 0.06745657 0.7280724\n[6,] 0.9411278 0.2118689 0.13505027 0.91586888 0.94076896 0.79496217 0.8474919\n          [,8]\n[1,] 0.1471476\n[2,] 0.6209607\n[3,] 0.3606857\n[4,] 0.9326360\n[5,] 0.9319638\n[6,] 0.1878635\n\n# Retrieve a subset of the variable \"T0\"\n# This subset starts at position (1, 1, 1) and has a count of (2, 3, 1) along each dimension\nncvar_get(nc_Test, \"T0\", start = c(1, 1, 1), count = c(2, 3, 1))\n\n          [,1]      [,2]      [,3]\n[1,] 0.4243155 0.9355373 0.6876228\n[2,] 0.4031014 0.7100455 0.4772366\n\n# Retrieve attributes associated with the variable \"T0\"\nncatt_get(nc_Test, \"T0\")\n\n$units\n[1] \"cel\"\n\n$`_FillValue`\n[1] -9999\n\n\n\n\n\nnc.variables[\"var_Name\"]\nnc.variables[\"var_Name\"].__dict__\n\n\n# Retrieve the entire \"T0\" variable\nt0_variable = nc_Test.variables[\"T0\"][:]\nprint(\"T0 variable (whole):\", t0_variable)\n\nT0 variable (whole): [[[0.42431554 0.40310138 0.6261856  0.20511685 0.42028654 0.5832691 ]\n  [0.9355373  0.71004546 0.22161925 0.7212336  0.351912   0.24259308]\n  [0.68762285 0.47723657 0.7574885  0.20524664 0.87728196 0.8981051 ]\n  [0.80022776 0.37840527 0.39981276 0.23714186 0.43234295 0.8781416 ]\n  [0.44087005 0.6279713  0.14625978 0.86158025 0.8802429  0.97017384]\n  [0.47086993 0.9022897  0.28917426 0.96354187 0.7829933  0.7729005 ]\n  [0.59079015 0.89029056 0.34811    0.06449199 0.0240892  0.7407992 ]\n  [0.9606409  0.02331837 0.5995715  0.49539265 0.30605116 0.88251084]]\n\n [[0.29569626 0.941782   0.97861916 0.6288502  0.585772   0.6905151 ]\n  [0.5266238  0.5029727  0.5408758  0.7536872  0.4706038  0.6474653 ]\n  [0.8026623  0.03080061 0.46791443 0.4690288  0.57941103 0.6727229 ]\n  [0.9940577  0.6261121  0.46802992 0.11466672 0.6392034  0.13353172]\n  [0.2692077  0.3175149  0.05485917 0.5746922  0.34206086 0.64459616]\n  [0.9580371  0.0802093  0.13391963 0.9119628  0.80673444 0.8997121 ]\n  [0.27879658 0.5047661  0.9370349  0.41187954 0.05105839 0.04749461]\n  [0.75339943 0.7098062  0.21070404 0.3593461  0.87074333 0.53087103]]\n\n [[0.296447   0.5802779  0.42908588 0.19943485 0.53526556 0.94112784]\n  [0.31855267 0.5473396  0.64588016 0.22513399 0.81889385 0.21186887]\n  [0.02501507 0.7228929  0.32526892 0.1600422  0.03500481 0.13505027]\n  [0.7807847  0.4201326  0.07880253 0.44191572 0.8804523  0.9158689 ]\n  [0.88581914 0.06668617 0.67707014 0.2865446  0.45287898 0.94076896]\n  [0.35747567 0.14891115 0.78467715 0.14325349 0.06745657 0.79496217]\n  [0.22053824 0.96196246 0.5030632  0.44349116 0.7280724  0.8474919 ]\n  [0.14714763 0.62096065 0.3606857  0.932636   0.9319638  0.18786351]]]\n\n# Retrieve a subset of the \"T0\" variable\n# This subset starts at position (0, 0, 0) and has a count of (2, 3, 1) along each dimension\nprint(t0_variable[0:2, 0:3, 0:1])\n\n[[[0.42431554]\n  [0.9355373 ]\n  [0.68762285]]\n\n [[0.29569626]\n  [0.5266238 ]\n  [0.8026623 ]]]\n\n# Access attributes associated with the \"T0\" variable\nprint(nc_Test.variables[\"T0\"].__dict__)\n\n{'units': 'cel', '_FillValue': -9999.0}\n\n\n\n\n\n\n\n2.4 Close\nWhen working with NetCDF files in R using the ncdf4 package, it’s crucial to remember that opening a file establishes a connection. This prevents data corruption and conflicts. To finish, always close the file using nc_close() once you’ve completed your operations.\n\nRPython\n\n\n\n# Close the NetCDF file\nnc_close(nc_Test)\n\n\n\n\n# Close the NetCDF file\nnc_Test.close()"
  },
  {
    "objectID": "dataprocess/NetCDF.html#create-and-export",
    "href": "dataprocess/NetCDF.html#create-and-export",
    "title": "NetCDF",
    "section": "3 Create and Export",
    "text": "3 Create and Export\nIn this section, we will walk you through the steps to create a NetCDF file with your data. By following these steps, you’ll be able to prepare your data and save it in the NetCDF format for further analysis or sharing.\n\n3.1 Create new empty NetCDF file (Python)\n\nRPython\n\n\nIn R, you need after defining the dimensions and variables to create the file in the system. See Section 3.4.\n\n\nIn Python, you first need to create (connect) a new empty file in the system and an object in Python.\n\n# Create a NetCDF file\nnc_Create = nc.Dataset(\"C:\\\\Lei\\\\HS_Web\\\\data_share\\\\minibeispiel_NetCDF_Py.nc\", \"w\")\n\n\n\n\n\n\n3.2 Define the Dimensions\nThe initial step in creating a NetCDF dataset is dimension definition:\n\nRPython\n\n\n\nncdim_def()\n\n\n# Define dimension metadata\nnum_Dim_Lon &lt;- seq(11.72, 11.79, 0.01)\nnum_Dim_Lat &lt;- seq(50.08, 50.13, 0.01)\nnum_Dim_Time &lt;- 1:3\n\ndim_lon &lt;- ncdim_def(\"longitude\", \"degrees_east\",\n                     num_Dim_Lon,\n                     longname = \"longitude\")\ndim_lat &lt;- ncdim_def(\"latitude\", \"degrees_north\",\n                     num_Dim_Lat,\n                     longname = \"latitude\")\ndim_time &lt;- ncdim_def(\"time\", \"day since 1961-01-01 00:00:00 +00\",\n                      num_Dim_Time, unlim=TRUE,\n                      longname = \"time\")\n\n\n\n\nnc.createDimension()\n\n\n# Define dimension metadata\nnum_Dim_Lon = np.arange(11.72, 11.8, 0.01)\nnum_Dim_Lat = np.arange(50.08, 50.14, 0.01)\nnum_Dim_Time = np.arange(1, 4)\n\n\n# Define dimensions\nnc_Create.createDimension(\"longitude\", len(num_Dim_Lon))\n\n&lt;class 'netCDF4._netCDF4.Dimension'&gt;: name = 'longitude', size = 9\n\nnc_Create.createDimension(\"latitude\", len(num_Dim_Lat))\n\n&lt;class 'netCDF4._netCDF4.Dimension'&gt;: name = 'latitude', size = 7\n\nnc_Create.createDimension(\"time\", len(num_Dim_Time))  # Use None for unlimited dimension\n\n&lt;class 'netCDF4._netCDF4.Dimension'&gt;: name = 'time', size = 3\n\n\ndim_lon = nc_Create.createVariable(\"longitude\", \"f4\", \"longitude\")\ndim_lat = nc_Create.createVariable(\"latitude\", \"f4\", \"latitude\")\ndim_time = nc_Create.createVariable(\"time\", \"i\", \"time\") \n\ndim_lon[:] = num_Dim_Lon\ndim_lat[:] = num_Dim_Lat\ndim_time[:] = num_Dim_Time\n\nCompared to R, in Python, you need to create a variable with the same name to store the values of the dimension. In Python, a pure dimension will only consider the dimension’s size and name.\n\n\n\nIn this example, we will create a 3D array with latitude, longitude, and time dimensions.\n\n\n3.3 Define the Variales\nThe next step is to define a variable, but you don’t need to assign values to it at this stage. There are three common attributes (name, units and dimensions) that are essential for every variable and should always be defined. Other user-defined attributes can be added later as needed.\n\nRPython\n\n\n\nncvar_def()\n\nname\nunits\ndim\n\n\nYou also have the option to create a dimension with no data values, effectively making it a null dimension. However, you can still set attributes for this dimension to store non-array information.\nAfter defining all the variables, it’s necessary to gather them into a list.\n\n# Define a variable named \"T0\" with the units \"cel\" and dimensions dim_lat, dim_lon, and dim_time.\n# The missing value for this variable is set to -9999.\nvar_T0 &lt;- ncvar_def(\"T0\", \"cel\", list(dim_lat, dim_lon, dim_time), -9999)\n\n# Define a variable named \"crs\" with no units and no dimensions (empty list).\n# This variable is defined as NULL initially.\nvar_crs &lt;- ncvar_def(\"crs\", \"\", list(), NULL)\n\n# Combine variables into a list\nvars &lt;- list(var_T0, var_crs)\n\n\n\n\nnc.createVariable()\n\nname\nunits\ndim\n\n\n\n# Define variables\nvar_T0 = nc_Create.createVariable(\"T0\", \"f4\", (\"latitude\", \"longitude\", \"time\"))\nvar_T0.units = \"cel\"\nvar_T0.missing_value = -9999\n\nvar_crs = nc_Create.createVariable(\"crs\", \"S1\")  # Create an empty variable\n\n\n\n\n\n\n3.4 Create new empty NetCDF file (R)\n\nR\n\n\nYou can now create a NetCDF file with the (list of) variables you have:\n\nnc_create(filename, vars)\n\n\nnc_Create &lt;- nc_create(\"C:\\\\Lei\\\\HS_Web\\\\data_share\\\\minibeispiel_NetCDF.nc\", vars)\n\n\n\n\n\n\n3.5 Put the Data\nAfter creating the NetCDF file, it will be an empty file in your local folder. The next step is to populate the file with data for each of the variables. This involves specifying the values for each variable and writing them to the file.\n\nRPython\n\n\n\nncvar_put()\n\n\nncvar_put(nc_Create, var_T0, runif(length(num_Dim_Lat) * length(num_Dim_Lon) * length(num_Dim_Time)))\n\n\n\n\n# Add data to the \"T0\" variable (random data)\nvar_T0[:] = np.random.rand(len(num_Dim_Lat), len(num_Dim_Lon), len(num_Dim_Time))\n\n\n\n\n\n\n3.6 Put Attributes\nWhen populating a NetCDF file, it’s essential to not only specify the variable data values but also the attributes associated with those variables. Attributes provide crucial metadata that describes the data, such as units, long names, and other relevant information.\n\nRPython\n\n\n\nncatt_put()\n\nAbsolutely, you can set attributes not only for individual variables.\n\n# Add the \"long_name\" and \"EPSG\" attributes to the variable \"var_crs\"\nncatt_put(nc_Create, var_crs, \"long_name\", \"coordinate reference system\")\nncatt_put(nc_Create, var_crs, \"EPSG\", \"EPSG:4236\")\n\n\n\n\nvar_crs.long_name = \"coordinate reference system\"\nvar_crs.EPSG = \"EPSG:4236\"\n\n\n\n\nBut also for the entire NetCDF file as global attributes. Global attributes provide overarching information about the dataset, such as its title, source, creation date, and any other relevant details.\n\nRPython\n\n\n\n# Add the \"title\" and \"author\" global attributes to the NetCDF file\nncatt_put(nc_Create, 0, \"title\", \"Multidimensional data example\")\nncatt_put(nc_Create, 0, \"author\", \"Kan, Lei, kan.lei@ruhr-uni-bochum.de\")\n\n\n\n\n# Add global attributes\nnc_Create.title = \"Multidimensional data example\"\nnc_Create.author = \"Kan, Lei, kan.lei@ruhr-uni-bochum.de\"\n\n\n\n\n\n\n3.7 Close\nAt the end, make sure to close the connections to your NetCDF files.\n\nRPython\n\n\n\nnc_close(nc_Create)\n\n\n\n\n# Close the NetCDF file\nnc_Create.close()\n\n\n\n\nOnce you’ve gone through these steps, you’ll have a well-maintained NetCDF file that can be easily used for any further processing, transformations, or visualization."
  },
  {
    "objectID": "dataprocess/extract_spatial.html",
    "href": "dataprocess/extract_spatial.html",
    "title": "Values Extract",
    "section": "",
    "text": "Raster Data is actually a kind of Sample data of a area, the area will be divided in regular grids (equally sized rectangles). The typical raster data like elevation is already the most important data for many spatial-based research fields. The raster form is also the important form for meteorological data, in order the area-value (e.g. Temperature and Presentation) to represent. But for the application e.g. in the Hydrology need we some statistical values for specific research regions, especially the Average.\nTherefore we need the operate EXTRACT.\n\n\nThe basic Data are like:\n\n\n\nThe example Data\n\n\n\ntwo Raster data\ntwo Regions shape files\n\nFor the operate EXTRACT, we need basically the raster data and the regions (shape files). Before this we must confirm that, the both data are in the same CRS (coordinate reference system). In this Blog we discuss only the theories and ideas about EXTRACT. There will show total four methods:\n\n\nThe first method we just use the original Raster with original resolution. But when the resolution not so fine, it will occur to that the selected grids have the big difference than the region. This is one very typical Problem in meteorological data, they have not so gut space resolution because the time resolution is always finer than the common geological data. In order to balance the data size, we must reduce the space resolution.\n\n\n\nIllustration of Mask with “Touch” oder “Centerpoint”\n\n\nFor the SELECT, there are two familiar methods Touch and Center-point:\n\nTouch: all the grids, who touched by the region, will be selected\nCenter-point: only the grids, who’s Center-point is with in the region, will be selected\n\nFor the both SELECT methods there some implausible cases:\n\nwhen we use Touch method, it will select some grids, who has only a little area within the region, like Cell 4\nCell 5: only an eighth of the area within the region, but it counts as a “whole cell” just because its center is in the region\nCell 18: with three quarters of the area in the region, but is not selected, just because the center is not in the region\n\nSummary we can say: the original resolution can be used, only when the deviation between the grids and region is not so big and\n\nTouch includes all grid cells that are touched, so can be used for some extreme statistical value (e.g. Max or Min)\nCenter-point can be used for the average value and actually the deviation maybe reduced, due to the surplus of selected grids and deficit of not selected grids in the boundary.\n\n\n\n\nThe second method is one simplest method, we need only refine our data in higher resolution, like resolution in 10 times finer and the grids will in 100 times more.\nEssentially there is no difference as 1. method, but the problem will be solved. This method is pointed, just because I must use Matlab processing the data, but there is no spatial Analyse Toolbox in Matlab. Therefore this is fast the only Answer, just because the Refine needs no supply from spatial Analyse Toolbox, we can easily repeat the data 10 more in row and 10 more in column.\n\n\n\nIllustration of Mask with “Touch” oder “Centerpoint” after Disaggregation\n\n\nLike the figure shows: the accuracy is gut improvement, the deviation should lay under the 1%.\n\n\n\nThe weighted mean is always exacter than the numerical average. The important Point for the weighted mean is the weights, in the spatial analyse it’s the portion of the area. So, the main task in third method is calculate the area of every value, that within one region.\nIn order to calculate the area, we need actually convert the raster grids into shape, for the convert we have also two methods:\n\nthe same value as one Polygon (this method should more convenient for the categories data with only several value)\nevery grid as a rectangle polygon, then calculate the portion of the area, where is located within the region (this method is use in R::terra package, but there is also a small deviation, when the CRS lie in lon-lat. The portion of one grid will be not equal to the portion of the region, because the grid area one to one is already different.)\n\n\n\n\nIllustration von Extract mit Wert-Polygon\n\n\nIn the Illustration is every value as the same polygon converted.\n\n\n\nThis method is designed only for the meteorological data, those have the big mange on time scalar. It’s also the most effective method in the practice.\nThe theory and the formal is just like:\n\\[\n\\vec{\\Omega}_{[time,region]} = \\vec{A}_{[time,grid]} \\cdot \\vec{W}_{[grid,region]}\n\\]\n\\(\\vec{\\Omega}_{[time,region]}\\) = Region-value of every region\n\\(\\vec{A}_{[time,grid]}\\) = all Value in the matrix [time, grid]\n\\(\\vec{W}_{[grid,region]}\\) = Weights of every grid to every region in the matrix [grid,region]\n\n\nFor the Weight-Matrix calculate we just the portion of the grid area only that within the region to the whole region area (but not the whole grid), then divide the area of the region.\nOne example weight_grid:\n            [R1]      [R2]\n [G1]      0.000      0.00\n [G2] 134364.119 189431.77\n [G3] 212464.416      0.00\n [G4]   2747.413      0.00\n [G5] 150176.618      0.00\n [G6]      0.000  45011.22\nG for Grid and R for Region\n\n\n\nOne example mat_value:\n     [G1] [G2] [G3] [G4] [G5] [G6] \n[T1]    2    1    3    4    1    1  \n[T2]    3    1    2    4    1    1  \nT for Time\nThe end."
  },
  {
    "objectID": "dataprocess/extract_spatial.html#extract-from-raster",
    "href": "dataprocess/extract_spatial.html#extract-from-raster",
    "title": "Values Extract",
    "section": "",
    "text": "The basic Data are like:\n\n\n\nThe example Data\n\n\n\ntwo Raster data\ntwo Regions shape files\n\nFor the operate EXTRACT, we need basically the raster data and the regions (shape files). Before this we must confirm that, the both data are in the same CRS (coordinate reference system). In this Blog we discuss only the theories and ideas about EXTRACT. There will show total four methods:\n\n\nThe first method we just use the original Raster with original resolution. But when the resolution not so fine, it will occur to that the selected grids have the big difference than the region. This is one very typical Problem in meteorological data, they have not so gut space resolution because the time resolution is always finer than the common geological data. In order to balance the data size, we must reduce the space resolution.\n\n\n\nIllustration of Mask with “Touch” oder “Centerpoint”\n\n\nFor the SELECT, there are two familiar methods Touch and Center-point:\n\nTouch: all the grids, who touched by the region, will be selected\nCenter-point: only the grids, who’s Center-point is with in the region, will be selected\n\nFor the both SELECT methods there some implausible cases:\n\nwhen we use Touch method, it will select some grids, who has only a little area within the region, like Cell 4\nCell 5: only an eighth of the area within the region, but it counts as a “whole cell” just because its center is in the region\nCell 18: with three quarters of the area in the region, but is not selected, just because the center is not in the region\n\nSummary we can say: the original resolution can be used, only when the deviation between the grids and region is not so big and\n\nTouch includes all grid cells that are touched, so can be used for some extreme statistical value (e.g. Max or Min)\nCenter-point can be used for the average value and actually the deviation maybe reduced, due to the surplus of selected grids and deficit of not selected grids in the boundary.\n\n\n\n\nThe second method is one simplest method, we need only refine our data in higher resolution, like resolution in 10 times finer and the grids will in 100 times more.\nEssentially there is no difference as 1. method, but the problem will be solved. This method is pointed, just because I must use Matlab processing the data, but there is no spatial Analyse Toolbox in Matlab. Therefore this is fast the only Answer, just because the Refine needs no supply from spatial Analyse Toolbox, we can easily repeat the data 10 more in row and 10 more in column.\n\n\n\nIllustration of Mask with “Touch” oder “Centerpoint” after Disaggregation\n\n\nLike the figure shows: the accuracy is gut improvement, the deviation should lay under the 1%.\n\n\n\nThe weighted mean is always exacter than the numerical average. The important Point for the weighted mean is the weights, in the spatial analyse it’s the portion of the area. So, the main task in third method is calculate the area of every value, that within one region.\nIn order to calculate the area, we need actually convert the raster grids into shape, for the convert we have also two methods:\n\nthe same value as one Polygon (this method should more convenient for the categories data with only several value)\nevery grid as a rectangle polygon, then calculate the portion of the area, where is located within the region (this method is use in R::terra package, but there is also a small deviation, when the CRS lie in lon-lat. The portion of one grid will be not equal to the portion of the region, because the grid area one to one is already different.)\n\n\n\n\nIllustration von Extract mit Wert-Polygon\n\n\nIn the Illustration is every value as the same polygon converted.\n\n\n\nThis method is designed only for the meteorological data, those have the big mange on time scalar. It’s also the most effective method in the practice.\nThe theory and the formal is just like:\n\\[\n\\vec{\\Omega}_{[time,region]} = \\vec{A}_{[time,grid]} \\cdot \\vec{W}_{[grid,region]}\n\\]\n\\(\\vec{\\Omega}_{[time,region]}\\) = Region-value of every region\n\\(\\vec{A}_{[time,grid]}\\) = all Value in the matrix [time, grid]\n\\(\\vec{W}_{[grid,region]}\\) = Weights of every grid to every region in the matrix [grid,region]\n\n\nFor the Weight-Matrix calculate we just the portion of the grid area only that within the region to the whole region area (but not the whole grid), then divide the area of the region.\nOne example weight_grid:\n            [R1]      [R2]\n [G1]      0.000      0.00\n [G2] 134364.119 189431.77\n [G3] 212464.416      0.00\n [G4]   2747.413      0.00\n [G5] 150176.618      0.00\n [G6]      0.000  45011.22\nG for Grid and R for Region\n\n\n\nOne example mat_value:\n     [G1] [G2] [G3] [G4] [G5] [G6] \n[T1]    2    1    3    4    1    1  \n[T2]    3    1    2    4    1    1  \nT for Time\nThe end."
  },
  {
    "objectID": "dataprocess/extract_spatial.html#extract-from-polygons",
    "href": "dataprocess/extract_spatial.html#extract-from-polygons",
    "title": "Values Extract",
    "section": "2.1 Extract from Polygons",
    "text": "2.1 Extract from Polygons\nWhen we just EXTRACT one value (one attribute) we can straight use the function intersect(), to intersect the data-polygons and regions-polygons then calculate the statistic values for every regions.\n\n\n\nIllustration of Extract with Polygon-value\n\n\nBut when we need to extract more values (attributes), again the idea from last Blog Exact with scale product will be used:\n\\[\n\\vec{\\Omega}_{[attribute,region]} = \\vec{A}_{[attribute,polygon]} \\cdot \\vec{W}_{[polygon,region]}\n\\]\n\\(\\vec{\\Omega}_{[attribute,region]}\\) = Region-value of every region\n\\(\\vec{A}_{[attribute,polygon]}\\) = attributes list in the matrix [attribute,polygon]\n\\(\\vec{W}_{[polygon,region]}\\) = Weights of every polygon to every region in the matrix [polygon,region]\n\nWeight-Matrix create: just use intersect() then statistic the portion of the value-area to the region-area in matrix [value-polygon, regions]\nValue-Matrix create: connect the attribute list\nscale product with both Value- and Weight-Matrix"
  },
  {
    "objectID": "dataprocess/extract_spatial.html#extract-from-points",
    "href": "dataprocess/extract_spatial.html#extract-from-points",
    "title": "Values Extract",
    "section": "2.2 Extract from Points",
    "text": "2.2 Extract from Points\n\n2.2.1 Numerical Mean\nThe moooost simple and direct method is the Numerical Mean of points in the region:\n\n\n\nIllustration of Extract with Point-Value: Numerical Mean of points in the region\n\n\n\nInterset with regions, then select points which in the region\nCalculate mean value of points\n\nThe weakness are also obviously, many points, who lay just near the boundary of region, will be ignored. It’s also familiar that in some regions there are no points laying in.\nSo, we need maybe convert the point-data to polygon- or raster-data\n\n\n2.2.2 Tiessen (Dirichlet) Polygon\nActually the convert to the polygon is the most popular and typical method specially with the Tiessen (Dirichlet) Polygon in meteorological fields.\n\n\n\nIllustration of Extract mit Punkt-Wert\n\n\n\nConvert point data to Tiessen polygon data\nuse the method of polygon like above\n\n\n\n2.2.3 Interpolate as Raster\nThe second convert idea is convert to the raster: Interpolation\n\n\n\nIllustration der Interpolation mit Punkt-Werten\n\n\nThe Interpolation is also one important issue, and it will be discussed in th near future. Here will just show the three most impotent methods: Nearest neighbor, IDW (Inverse distance weighted) and Kringing\nThe three methods are also very easy processed in R::terra, that will be showed in the next Blog.\nThe end."
  },
  {
    "objectID": "dataprocess/basic_r_python.html",
    "href": "dataprocess/basic_r_python.html",
    "title": "R & Python Basic",
    "section": "",
    "text": "More Details of R in R for Data Science (2e) and Advanced R\nMore Details of Python in Automate the Boring Stuff with Python and W3 School Python\nThis article serves as a brief introduction to the fundamental coding aspects of both R and Python. It provides a first impression of these scripting languages. For a more comprehensive understanding and in-depth techniques related to both languages, you are encouraged to explore the website mentioned above. The content here is primarily a condensed compilation of information from the provided links, aimed at facilitating a comparison between R and Python.\nData and Functions are the two essential components of every programming language, especially in the context of data science and data processing. They can be likened to nouns and verbs in natural languages. Data describes information, while Functions define actions for manipulating that data.\nThis article is divided into two main sections: Data (Section 1) and Coding (Section 2).\nIn the Data section, we will explore:\nIn the Coding section, we will delve into three key aspects:\nThe above five elements can be considered as the most fundamental elements of every scripting language. Additionally, we will explore object creation and naming in a section called ‘New Objects’ (Section 3). Objects can encompass functions and variables, further enriching our understanding of scripting.\nThis article will provide a solid introduction to the core concepts in programming, laying the groundwork for further exploration in both R and Python.\nOverview:"
  },
  {
    "objectID": "dataprocess/basic_r_python.html#datatypes-structure",
    "href": "dataprocess/basic_r_python.html#datatypes-structure",
    "title": "R & Python Basic",
    "section": "1.1 Datatypes & Structure",
    "text": "1.1 Datatypes & Structure\nIn programming, the concept of datatypes is fundamental. It forms the basis for how we handle and manipulate information in software. The most basic data types, such as integers, numerics, booleans, characters, and bytes, are supported by almost all programming languages. Additionally, there are more complex data types built upon these basics, like strings, which are sequences of characters, and dates, which can be represented as variables of integers and more.\nData structures are equally important, as they determine the organization of data, whether it involves the same data types in multiple dimensions or combinations of different types. Data types and structures are intertwined, serving as the cornerstone for our programming endeavors.\nVariables play a pivotal role in storing data of different types. The choice of data type and structure is critical, as different types and structures enable various operations and functionalities. Therefore, understanding data types and structures is paramount before embarking on data manipulation tasks.\n\n1.1.1 Datatypes\nA data type of a variable specifies the type of data that is stored inside that variable. In this context, we will just discuss Atomic Variables, which represent fundamental data types. There are six basic atomic data types:\n\nLogical (boolean data type)\n\ncan only have two values: TRUE and FALSE\n\nNumeric (double, float, lang)\n\nrepresents all real numbers with or without decimal values.\n\nInteger\n\nspecifies real values without decimal points.\n\nComplex\n\nis used to specify purely imaginary values\n\nCharacter (string)\n\ndata type is used to specify character or string values in a variable\n\nRaw (bytes)\n\nspecifies values as raw bytes\n\n\n\nRPython\n\n\nIn R, variables do not require explicit declaration with a particular data type. Instead, R is dynamically typed, allowing variables to adapt to the data they contain. You can use the following techniques to work with data types in R:\n\nChecking Data Types: To determine the data type of a variable, you can use the class() function.\nType Conversion: When needed, you can change the data type of a variable using R’s conversion functions, typically prefixed with as..\n\nR’s flexibility in data type handling simplifies programming tasks and allows for efficient data manipulation without the need for explicit type declarations.\n\n# Numeric\nx &lt;- 10.5\nclass(x)\n\n[1] \"numeric\"\n\n# Integer\nx &lt;- 1000L\nclass(x)\n\n[1] \"integer\"\n\n# Complex\nx &lt;- 9i + 3\nclass(x)\n\n[1] \"complex\"\n\n# Character/String\nx &lt;- \"R is exciting\"\nclass(x)\n\n[1] \"character\"\n\n# Logical/Boolean\nx &lt;- TRUE\nclass(x)\n\n[1] \"logical\"\n\n# Convert\ny &lt;- as.numeric(x)\nclass(y)\n\n[1] \"numeric\"\n\n# Raw (bytes)\nx &lt;- charToRaw(\"A\")\nx\n\n[1] 41\n\nclass(x)\n\n[1] \"raw\"\n\n\n\n\nIn Python, variables also do not require explicit declaration with a particular data type. Python is dynamically typed, allowing variables to adapt to the data they contain. You can use the following techniques to work with data types in Python:\n\nChecking Data Types: To determine the data type of a variable, you can use the type() function. It allows you to inspect the current data type of a variable.\nType Conversion: When needed, you can change the data type of a variable in Python using various conversion functions, like float().\n\nPython’s flexibility in data type handling simplifies programming tasks and allows for efficient data manipulation without the need for explicit type declarations.\n\n# Numeric\nx = 10.5\nprint(type(x))\n\n&lt;class 'float'&gt;\n\n# Integer\nx = 1000\nprint(type(x))\n\n&lt;class 'int'&gt;\n\n# Complex\nx = 9j + 3\nprint(type(x))\n\n&lt;class 'complex'&gt;\n\n# Character/String\nx = \"Python is exciting\"\nprint(type(x))\n\n&lt;class 'str'&gt;\n\n# Logical/Boolean\nx = True\nprint(type(x))\n\n&lt;class 'bool'&gt;\n\n# Convert to Numeric\ny = float(x)\nprint(type(y))\n\n&lt;class 'float'&gt;\n\n# Raw (bytes)\nx = b'A'\nprint(x)\n\nb'A'\n\nprint(type(x))\n\n&lt;class 'bytes'&gt;\n\n\n\n\n\n\n\n1.1.2 Data Structure\nComparatively, data structures between R and Python tend to exhibit more differences than their data types. However, by incorporating additional libraries like NumPy and pandas, we can access shared data structures which play a vital role in the field of data science.\n\nVector: A set of multiple values (items)\n\nContains items of the same data type or structure\nIndexed: Allows you to get and change items using indices\nAllows duplicates\nChangeable: You can modify, add, and remove items after creation\n\nArray: A multi-dimensional extension of a vector\n\nMatrix: two dimensions\n\nList: A set of multiple values (items)\n\nContains items of different data types or structures\nIndexed: Allows you to get and change items using indices\nAllows duplicates\nChangeable: You can modify, add, and remove items after creation\n\nTable (Data Frame): Tabular data structure\n\nTwo-dimensional objects with rows and columns\nContains elements of several types\nEach column has the same data type\n\n\n\nRPython\n\n\nThe structure of R variable can be checked with str()ucture:\n\n# Create a vector\nvct_Test &lt;- c(1,5,7)\n# View the structure\nstr(vct_Test)\n\n num [1:3] 1 5 7\n\n# Create a array\nary_Test &lt;- array(1:24, c(2,3,4))\n# View the structure\nstr(ary_Test)\n\n int [1:2, 1:3, 1:4] 1 2 3 4 5 6 7 8 9 10 ...\n\n# Create a matrix\nmat_Test &lt;- matrix(1:24, 6, 4)\nmat_Test\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    7   13   19\n[2,]    2    8   14   20\n[3,]    3    9   15   21\n[4,]    4   10   16   22\n[5,]    5   11   17   23\n[6,]    6   12   18   24\n\n# View the structure\nstr(mat_Test)\n\n int [1:6, 1:4] 1 2 3 4 5 6 7 8 9 10 ...\n\n# Create a list\nlst_Test &lt;- list(c(1,3,5), \"abc\", FALSE)\n# View the structure\nstr(lst_Test)\n\nList of 3\n $ : num [1:3] 1 3 5\n $ : chr \"abc\"\n $ : logi FALSE\n\n# Create a table (data frame)\ndf_Test &lt;- data.frame(name = c(\"Bob\", \"Tom\"), age = c(12, 13))\ndf_Test\n\n  name age\n1  Bob  12\n2  Tom  13\n\n# View the structure\nstr(df_Test)\n\n'data.frame':   2 obs. of  2 variables:\n $ name: chr  \"Bob\" \"Tom\"\n $ age : num  12 13\n\n\n\n\nIn Python, the structure of a variable is treated as the data type, and you can confirm it using the type() function.\nIt’s important to note that some of the most commonly used data structures, such as arrays and data frames (tables), are not part of the core Python language itself. Instead, they are provided by two popular libraries: numpy and pandas.\n\nimport numpy as np\nimport pandas as pd\n\n# Create a vector (list in Python)\nvct_Test = [1, 5, 7]\n# View the structure\nprint(type(vct_Test))\n\n&lt;class 'list'&gt;\n\n# Create a 3D array (NumPy ndarray)\nary_Test = np.arange(1, 25).reshape((2, 3, 4))\n# View the structure\nprint(type(ary_Test))\n\n&lt;class 'numpy.ndarray'&gt;\n\n# Create a matrix (NumPy ndarray)\nmat_Test = np.arange(1, 25).reshape((6, 4))\nprint(type(mat_Test))\n\n&lt;class 'numpy.ndarray'&gt;\n\n# Create a list\nlst_Test = [[1, 3, 5], \"abc\", False]\n# View the structure\nprint(type(lst_Test))\n\n&lt;class 'list'&gt;\n\n# Create a table (pandas DataFrame)\ndf_Test = pd.DataFrame({\"name\": [\"Bob\", \"Tom\"], \"age\": [12, 13]})\nprint(type(df_Test))\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n\nprint(df_Test)\n\n  name  age\n0  Bob   12\n1  Tom   13\n\n\nPython offers several original data structures, including:\n\nTuples: Tuples are ordered collections of elements, similar to lists, but unlike lists, they are immutable, meaning their elements cannot be changed after creation. Tuples are often used to represent fixed collections of items.\nSets: Sets are unordered collections of unique elements. They are valuable for operations that require uniqueness, such as finding unique values in a dataset or performing set-based operations like unions and intersections.\nDictionaries: Dictionaries, also known as dicts, are collections of key-value pairs. They are used to store data in a structured and efficient manner, allowing quick access to values using their associated keys.\n\nWhile these data structures may not be as commonly used in data manipulation and calculations as arrays and data frames, they have unique features and use cases that can be valuable in various programming scenarios."
  },
  {
    "objectID": "dataprocess/basic_r_python.html#index-subset",
    "href": "dataprocess/basic_r_python.html#index-subset",
    "title": "R & Python Basic",
    "section": "1.2 Index & subset",
    "text": "1.2 Index & subset\nAdditionally, subsetting plays a crucial role in data manipulation. Subsetting allows you to extract specific subsets of data based on conditions, criteria, or filters.\n\nRPython\n\n\nMore Details in Advanced R: 4 Subsetting.\nR’s subsetting operators are fast and powerful. Mastering them allows you to succinctly perform complex operations in a way that few other languages can match. Subsetting in R is easy to learn but hard to master because you need to internalise a number of interrelated concepts:\n\nThere are six ways to subset atomic vectors.\nThere are three subsetting operators, [[, [, and $.\nSubsetting operators interact differently with different vector types (e.g., atomic vectors, lists, factors, matrices, and data frames).\n\nSubsetting is a natural complement to str(). While str() shows you all the pieces of any object (its structure).\n\n\n\n\n\n\n\n\nTip\n\n\n\nIn Python, indexing starts from 0, not 1.\n\n\n\n\n\n\n1.2.1 Vector\n\nRPython\n\n\n\nPositive integers return elements at the specified positions:\n\n\nx &lt;- c(2.1, 4.2, 3.3, 5.4)\n\n# One value\nx[1]\n\n[1] 2.1\n\n# More values\nx[c(1:2, 4)]\n\n[1] 2.1 4.2 5.4\n\n# Duplicate indices will duplicate values\nx[c(1, 1)]\n\n[1] 2.1 2.1\n\n# Real numbers are silently truncated to integers\nx[c(2.1, 2.9)]\n\n[1] 4.2 4.2\n\n\n\nNegative integers exclude elements at the specified positions:\n\n\n# Exclude elements\nx[-c(3, 1)]\n\n[1] 4.2 5.4\n\n\n\n\n\n\n\n\nNOTE\n\n\n\nNote that you can’t mix positive and negative integers in a single subset:\n\n\n\nx[c(-1, 2)]\n\nError in x[c(-1, 2)]: nur Nullen dürfen mit negativen Indizes gemischt werden\n\n\n\n\n\nPositive integers return elements at the specified positions:\n\n\nimport numpy as np\nimport pandas as pd\n\n# Create a NumPy array\nx = np.array([2.1, 4.2, 3.3, 5.4])\n\n# One value\nprint(x[0])\n\n2.1\n\n# More values\nprint(x[np.array([0, 1, 3])])\n\n[2.1 4.2 5.4]\n\n# Duplicate indices will duplicate values\nprint(x[np.array([0, 0])])\n\n[2.1 2.1]\n\n\n\negative indexing to access an array from the end:\n\n\n# One value\nprint(x[-1])\n\n5.4\n\n# More values\nprint(x[-np.array([1, 3])])\n\n[5.4 4.2]\n\n\n\n\n\n\n\n1.2.2 Matrices and arrays\n\nRPython\n\n\nThe most common way of subsetting matrices (2D) and arrays (&gt;2D) is a simple generalisation of 1D subsetting: supply a 1D index for each dimension, separated by a comma. Blank subsetting is now useful because it lets you keep all rows or all columns.\n\n# Create a matrix\na2 &lt;- matrix(1:9, nrow = 3)\n# Rename the columns (equivalent to colnames in R)\ncolnames(a2) &lt;- c(\"A\", \"B\", \"C\")\n# Access a specific element using column name\na2[1, \"A\"]\n\nA \n1 \n\n# Select specific rows with all columns\na2[1:2, ]\n\n     A B C\n[1,] 1 4 7\n[2,] 2 5 8\n\n# columns which are excluded \na2[0, -2]\n\n     A C\n\n# Create a 3D array\na3 &lt;- array(1:24, c(2,3,4))\n# Access a specific element(s), in different dimensions\na3[1,2,2]\n\n[1] 9\n\na3[1,2,]\n\n[1]  3  9 15 21\n\na3[1,,]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    7   13   19\n[2,]    3    9   15   21\n[3,]    5   11   17   23\n\n\n\n\nIn Python, the : symbol is used to indicate all elements of a particular dimension or slice. It allows you to select or reference all items along that dimension in a sequence, array, or data structure.\n\nimport numpy as np\n\n# Create a NumPy matrix\na2 = np.array([[1, 2, 3],\n               [4, 5, 6],\n               [7, 8, 9]])\n\n# Rename the columns (equivalent to colnames in R)\ncolnames = [\"A\", \"B\", \"C\"]\n\n# Access a specific element using column name\nprint(a2[0, colnames.index(\"A\")])\n\n1\n\n# Select the first two rows\nprint(a2[0:2, :])\n\n[[1 2 3]\n [4 5 6]]\n\n# Create a NumPy 3D array\na3 = np.arange(1, 25).reshape((2, 3, 4))\n\n# Access a specific element in the 3D array\nprint(a3[0, 1, 1])\n\n6\n\nprint(a3[0, 1, :])\n\n[5 6 7 8]\n\nprint(a3[0, :, :])\n\n[[ 1  2  3  4]\n [ 5  6  7  8]\n [ 9 10 11 12]]\n\n\n\n\n\n\n\n1.2.3 Data frames\n\nRPython\n\n\nData frames have the characteristics of both lists and matrices:\n\nWhen subsetting with a single index, they behave like lists and index the columns, so df[1:2] selects the first two columns.\nWhen subsetting with two indices, they behave like matrices, so df[1:3, ] selects the first three rows (and all the columns)[^python-dims].\n\n\n# Create a DataFrame\ndf &lt;- data.frame(x = 1:3, y = 3:1, z = letters[1:3])\n\n# Select rows\ndf[df$x == 2, ]\n\n  x y z\n2 2 2 b\n\ndf[c(1, 3), ]\n\n  x y z\n1 1 3 a\n3 3 1 c\n\n# There are two ways to select columns from a data frame\n# Like a list\ndf[c(\"x\", \"z\")]\n\n  x z\n1 1 a\n2 2 b\n3 3 c\n\n# Like a matrix\ndf[, c(\"x\", \"z\")]\n\n  x z\n1 1 a\n2 2 b\n3 3 c\n\n# There's an important difference if you select a single \n# column: matrix subsetting simplifies by default, list \n# subsetting does not.\nstr(df[\"x\"])\n\n'data.frame':   3 obs. of  1 variable:\n $ x: int  1 2 3\n\nstr(df[, \"x\"])\n\n int [1:3] 1 2 3\n\n\n\n\nMore detail about Function pandas.Seies.iloc() and pandas.Seies.loc() in pandas document\n\nloc gets rows (and/or columns) with particular labels.\niloc gets rows (and/or columns) at integer locations.\n\n\nimport pandas as pd\n\n# Create a DataFrame\ndf = pd.DataFrame({'x': range(1, 4), 'y': range(3, 0, -1), 'z': list('abc')})\n\n# Select rows\nprint(df[df['x'] == 2])\n\n   x  y  z\n1  2  2  b\n\nprint(df.iloc[[0, 2]])\n\n   x  y  z\n0  1  3  a\n2  3  1  c\n\n# Select columns\nprint(df[['x', 'z']])\n\n   x  z\n0  1  a\n1  2  b\n2  3  c\n\n# Select columns like a DataFrame\nprint(df.loc[:, ['x', 'z']])\n\n   x  z\n0  1  a\n1  2  b\n2  3  c\n\n# Select a single column as a Series (simplifies by default)\nprint(df['x'])\n\n0    1\n1    2\n2    3\nName: x, dtype: int64\n\n# Select a single column as a DataFrame (does not simplify)\nprint(df[['x']])\n\n   x\n0  1\n1  2\n2  3\n\n\n\n\n\n\n\n1.2.4 List\n\nRPython\n\n\nThere are two other subsetting operators: [[ and $. [[ is used for extracting single items, while x$y is a useful shorthand for x[[\"y\"]].\n[[ is most important when working with lists because subsetting a list with [ always returns a smaller list. To help make this easier to understand we can use a metaphor:\n[[ can return only a single item, you must use it with either a single positive integer or a single string.\n\nx &lt;- list(a = 1:3, b = \"a\", d = 4:6)\n\n# Get the subset \nx[1]\n\n$a\n[1] 1 2 3\n\nstr(x[1])\n\nList of 1\n $ a: int [1:3] 1 2 3\n\nx[1:2]\n\n$a\n[1] 1 2 3\n\n$b\n[1] \"a\"\n\n# Get the element\nx[[1]]\n\n[1] 1 2 3\n\nstr(x[1])\n\nList of 1\n $ a: int [1:3] 1 2 3\n\n# with Label\nx$a\n\n[1] 1 2 3\n\nx[[\"a\"]]\n\n[1] 1 2 3\n\n\n\n\nIn Python there are no effectiv ways to create a items named list. It can always get the element of the list but not a subset of the list.\nIn Python, there are no effective ways to create items with named elements in a list. While you can access individual elements by their positions, there isn’t a straightforward method to create a subset of the list with named elements.\n\n# Create a Python list with nested lists\nx = [list(range(1, 4)), \"a\", list(range(4, 7))]\n\n# Get the subset (Python list slice)\nprint([x[0]])\n\n[[1, 2, 3]]\n\n# Get the element using list indexing\nprint(x[0])\n\n[1, 2, 3]\n\nprint(type(x[0]))\n\n&lt;class 'list'&gt;\n\n\nHowever, dictionaries in Python excel in this regard, as they allow you to assign and access elements using user-defined keys, providing a more efficient way to work with named elements and subsets of data.\n\n# Create a dictionary with labels\nx = {\"a\": list(range(1, 4)), \"b\": \"a\", \"d\": list(range(4, 7))}\n\n\n# Get the element using dictionary indexing\nprint(x[\"a\"])\n\n[1, 2, 3]\n\n# Access an element with a label\nprint(x[\"a\"])\n\n[1, 2, 3]\n\nprint(x.get(\"a\"))\n\n[1, 2, 3]\n\nprint(type(x[\"a\"]))\n\n&lt;class 'list'&gt;"
  },
  {
    "objectID": "dataprocess/basic_r_python.html#data-crud",
    "href": "dataprocess/basic_r_python.html#data-crud",
    "title": "R & Python Basic",
    "section": "1.3 Data CRUD",
    "text": "1.3 Data CRUD\nData manipulation is the art and science of transforming raw data into a more structured and useful format for analysis, interpretation, and decision-making. It’s a fundamental process in data science, analytics, and database management.\nOperations for creating and managing persistent data elements can be summarized as CRUD:\n\nCreate (Add): The creation of new data elements or records.\nRead: The retrieval and access of existing data elements for analysis or presentation.\nUpdate: The modification or editing of data elements to reflect changes or corrections.\nDelete: The removal or elimination of data elements that are no longer needed or relevant.\n\nCombining CRUD operations with subsetting provides a powerful toolkit for working with data, ensuring its accuracy, relevance, and utility in various applications, from database management to data analysis.\n\n1.3.1 Create & Add\nMost of the original data we work with is often loaded from external data sources or files. This process will be discussed in detail in the article titled Data Load.\nIn this section, we will focus on the fundamental aspects of creating and adding data, which may have already been mentioned several times in the preceding text.\n\nRPython\n\n\nCreating new objects in R is commonly done using the assignment operator &lt;-.\nWhen it comes to vectors or list, there are two primary methods to append new elements:\n\nc(): allows you to combine the original vector with a new vector or element, effectively extending the vector.\nappend(): enables you to append a new vector or element at a specific location within the original vector.\n\n\n# Automic value\na &lt;- 1 / 200 * 30\n\n# vector\nx_v &lt;- c(2.1, 4.2, 3.3, 5.4)\n# List\nx_l &lt;- list(a = 1:3, b = \"a\", d = 4:6)\n# add new elements\nc(x_v, c(-1,-5.6))\n\n[1]  2.1  4.2  3.3  5.4 -1.0 -5.6\n\nc(x_l, list(e = c(TRUE, FALSE)))\n\n$a\n[1] 1 2 3\n\n$b\n[1] \"a\"\n\n$d\n[1] 4 5 6\n\n$e\n[1]  TRUE FALSE\n\n# append after 2. Element\nappend(x_v, c(-1,-5.6), 2)\n\n[1]  2.1  4.2 -1.0 -5.6  3.3  5.4\n\nappend(x_l, list(e = c(TRUE, FALSE)), 2)\n\n$a\n[1] 1 2 3\n\n$b\n[1] \"a\"\n\n$e\n[1]  TRUE FALSE\n\n$d\n[1] 4 5 6\n\n\nWhen working with 2D matrices or data frames in R, you can use the following functions to add new elements in the row or column dimensions:\n\ncbind(): to combine data frames or matrices by adding new columns.\nrbind(): to combine data frames or matrices by adding new rows.\n\n\n# Create a matrix\nx_m &lt;- matrix(1:9, nrow = 3)\n# data frame\ndf &lt;- data.frame(x = 1:3, y = 3:1, z = letters[1:3])\n# append in colum dimension\ncbind(x_m, -1:-3)\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   -1\n[2,]    2    5    8   -2\n[3,]    3    6    9   -3\n\ncbind(df, k = -1:-3)\n\n  x y z  k\n1 1 3 a -1\n2 2 2 b -2\n3 3 1 c -3\n\n# append in row dimension\nrbind(x_m, -1:-3)\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n[4,]   -1   -2   -3\n\nrbind(df, list(-1, -2, \"z\")) # try with rbind(df, c(-1, -2, \"z\"))\n\n   x  y z\n1  1  3 a\n2  2  2 b\n3  3  1 c\n4 -1 -2 z\n\n\nAdditionally, for both lists and data frames in R, you can use the $ &lt;- operator to add new elements:\n\n# Data frame\ndf &lt;- data.frame(x = 1:3, y = 3:1, z = letters[1:3])\ncbind(df, k = -1:-3)\n\n  x y z  k\n1 1 3 a -1\n2 2 2 b -2\n3 3 1 c -3\n\ndf$k &lt;- -1:-3 # same to df[['k']] &lt;- -1:-3\ndf\n\n  x y z  k\n1 1 3 a -1\n2 2 2 b -2\n3 3 1 c -3\n\n# List\nx_l &lt;- list(a = 1:3, b = \"a\", d = 4:6)\nc(x_l, list(e = c(TRUE, FALSE)))\n\n$a\n[1] 1 2 3\n\n$b\n[1] \"a\"\n\n$d\n[1] 4 5 6\n\n$e\n[1]  TRUE FALSE\n\nx_l$e &lt;- c(TRUE, FALSE) # same to x_l[['e']] &lt;- c(TRUE, FALSE)\nx_l\n\n$a\n[1] 1 2 3\n\n$b\n[1] \"a\"\n\n$d\n[1] 4 5 6\n\n$e\n[1]  TRUE FALSE\n\n\n\n\nCreating new objects in Python is often accomplished using the assignment operator =. When it comes to adding elements to list, there are three primary functions to consider:\n\nappend(): add a single element to the end of a list.\ninsert(): add an element at a specific position within a list.\nextend() same as +: append elements from an iterable (e.g., another list) to the end of an existing list, allowing for the expansion of the list with multiple elements.\n\n\n# Atomic element\na = 1 / 200 * 30\nb = a + 1\nprint(a)\n\n0.15\n\nprint(b)\n\n1.15\n\n# List\nx = [2.1, 4.2, 3.3, 5.4]\n\n# Append on element\nx.append(-1)\nprint(x)\n\n[2.1, 4.2, 3.3, 5.4, -1]\n\n# Insert on eelement\nx.insert(3, -5.6)\nprint(x)\n\n[2.1, 4.2, 3.3, -5.6, 5.4, -1]\n\n# Extend with new list\nx.extend([6.7, 7.9])\nprint(x)\n\n[2.1, 4.2, 3.3, -5.6, 5.4, -1, 6.7, 7.9]\n\n\nWhen working with numpy.array in Python, you can add elements in two primary ways:\n\nappend(): add element or a new numpy array to the end.\ninsert(): insert element or a new numpy array at specific locations within the original numpy array.\n\n\nimport numpy as np\n\n# Create a NumPy array\nx_a = np.array([2.1, 4.2, 3.3, 5.4])\n\nprint(np.append(x_a, -1))\n\n[ 2.1  4.2  3.3  5.4 -1. ]\n\nprint(np.append(x_a, np.array([6.7, 7.9])))\n\n[2.1 4.2 3.3 5.4 6.7 7.9]\n\nprint(np.insert(x_a, 3, -5.6))\n\n[ 2.1  4.2  3.3 -5.6  5.4]\n\nprint(np.insert(x_a, 3, np.array([6.7, 7.9])))\n\n[2.1 4.2 3.3 6.7 7.9 5.4]\n\n\n\n\n\n\n\n1.3.2 Read\nThe read process is essentially a form of subsetting, where you access specific elements or subsets of data using their indexes. The crucial aspect of this operation is how to obtain and utilize these indexes effectively.\n\nRPython\n\n\n\n# Create a DataFrame\ndf &lt;- data.frame(x = 1:3, y = 3:1, z = letters[1:3])\n\n# Access using integer index \ndf[1,2]\n\n[1] 3\n\n# Access using names index\ndf[,\"z\"]\n\n[1] \"a\" \"b\" \"c\"\n\ndf$z\n\n[1] \"a\" \"b\" \"c\"\n\n# Access with a value condition\nidx &lt;- which(df$x &gt; 1)\ndf[idx,]\n\n  x y z\n2 2 2 b\n3 3 1 c\n\ndf[idx, \"z\"]\n\n[1] \"b\" \"c\"\n\nidx &lt;- which(df$z == \"a\")\ndf[idx,]\n\n  x y z\n1 1 3 a\n\ndf[idx, 1:2]\n\n  x y\n1 1 3\n\n\n\n\n\nimport pandas as pd\n\n# Create a pandas DataFrame\ndf = pd.DataFrame({'x': range(1, 4), 'y': range(3, 0, -1), 'z': list('abc')})\n\n# Access using integer index (iloc)\nprint(df.iloc[0, 1])\n\n3\n\n# Access using column label\nprint(df['z'])\n\n0    a\n1    b\n2    c\nName: z, dtype: object\n\nprint(df.z)\n\n0    a\n1    b\n2    c\nName: z, dtype: object\n\n# Access with a value condition\nidx = df['x'] &gt; 1\nprint(df[idx])\n\n   x  y  z\n1  2  2  b\n2  3  1  c\n\nprint(df[df['z'] == 'a'])\n\n   x  y  z\n0  1  3  a\n\nprint(df[df['z'] == 'a'][['x', 'y']])\n\n   x  y\n0  1  3\n\n\n\n\n\n\n\n1.3.3 Update\nThe update operation builds upon the principles of reading. It involves replacing an existing value with a new one, but with certain constraints. The new value must have the same data type, size, and structure as the original value. This ensures data consistency and integrity when modifying data elements. About “data type” it is not so strength, somtimes it is chanable if you replace the whol e.g. colums in data frame.\nIt’s important to note that the concept of ‘data type’ isn’t always rigid. There are cases where data types can change, particularly when replacing entire columns in a data frame, for instance. While data types typically define the expected format and behavior of data, specific operations and transformations may lead to changes in data types to accommodate new values or structures.\n\nRPython\n\n\n\n# Create a DataFrame\ndf &lt;- data.frame(x = 1:3, y = 3:1, z = letters[1:3])\ndf\n\n  x y z\n1 1 3 a\n2 2 2 b\n3 3 1 c\n\n# Update using integer index \ndf[1,2] &lt;- 0\ndf\n\n  x y z\n1 1 0 a\n2 2 2 b\n3 3 1 c\n\n# Update using names index\ndf[2,\"z\"] &lt;- \"lk\"\ndf\n\n  x y  z\n1 1 0  a\n2 2 2 lk\n3 3 1  c\n\n# Update with a value condition\nidx &lt;- which(df$x &gt; 1)\ndf[idx, \"z\"] &lt;- \"bg1\"\ndf\n\n  x y   z\n1 1 0   a\n2 2 2 bg1\n3 3 1 bg1\n\nidx &lt;- which(df$z == \"a\")\ndf[idx,] &lt;- c(-1, -5, \"new_a\")\ndf\n\n   x  y     z\n1 -1 -5 new_a\n2  2  2   bg1\n3  3  1   bg1\n\n\n\n\n\nimport pandas as pd\n\n# Create a pandas DataFrame\ndf = pd.DataFrame({'x': range(1, 4), 'y': range(3, 0, -1), 'z': list('abc')})\nprint(df)\n\n   x  y  z\n0  1  3  a\n1  2  2  b\n2  3  1  c\n\n# Update using integer index\ndf.iat[0, 1] = 0\nprint(df)\n\n   x  y  z\n0  1  0  a\n1  2  2  b\n2  3  1  c\n\n# Update using column label and row index\ndf.at[1, 'z'] = \"lk\"\nprint(df)\n\n   x  y   z\n0  1  0   a\n1  2  2  lk\n2  3  1   c\n\n# Update with a value condition\nidx_x_gt_1 = df['x'] &gt; 1\ndf.loc[idx_x_gt_1, 'z'] = \"bg1\"\nprint(df)\n\n   x  y    z\n0  1  0    a\n1  2  2  bg1\n2  3  1  bg1\n\nidx_z_eq_a = df['z'] == 'a'\ndf.loc[idx_z_eq_a] = [-1, -5, \"new_a\"]\nprint(df)\n\n   x  y      z\n0 -1 -5  new_a\n1  2  2    bg1\n2  3  1    bg1\n\n\n\n\n\n\n\n1.3.4 Delete\n\nRPython\n\n\nDeletion in R can be accomplished relatively easily using methods like specifying negative integer indices or setting elements to NULL within a list. However, it’s essential to recognize that there are limitations to deletion operations. For instance, when dealing with multi-dimensional arrays, you cannot delete a single element in the same straightforward manner; instead, you can only delete entire sub-dimensions.\n\n# Create a DataFrame\ndf &lt;- data.frame(x = 1:3, y = 3:1, z = letters[1:3])\ndf\n\n  x y z\n1 1 3 a\n2 2 2 b\n3 3 1 c\n\n# Delete using negative integer index \ndf[,-2]\n\n  x z\n1 1 a\n2 2 b\n3 3 c\n\ndf[-2,]\n\n  x y z\n1 1 3 a\n3 3 1 c\n\n# Setting elements to `NULL`\ndf$y &lt;- NULL\ndf\n\n  x z\n1 1 a\n2 2 b\n3 3 c\n\n\n\n\nIn Python is to use the .drop() command to delete the elemnts in datatframe. More details in pandas document\n\ndf = pd.DataFrame({'x': range(1, 4), 'y': range(3, 0, -1), 'z': list('abc')})\nprint(df)\n\n   x  y  z\n0  1  3  a\n1  2  2  b\n2  3  1  c\n\n# Drop columns\nprint(df.drop(['x', 'z'], axis=1))\n\n   y\n0  3\n1  2\n2  1\n\nprint(df.drop(columns=['x', 'y']))\n\n   z\n0  a\n1  b\n2  c\n\n# Drop a row by index\nprint(df.drop([0, 1]))\n\n   x  y  z\n2  3  1  c"
  },
  {
    "objectID": "dataprocess/basic_r_python.html#math",
    "href": "dataprocess/basic_r_python.html#math",
    "title": "R & Python Basic",
    "section": "2.1 Math",
    "text": "2.1 Math\n\n‘+’ ‘-’ ’*’ ‘/’\nExponent, Logarithm\nTrigonometric functions\nLinear algebra, Matrix multiplication\n\n\nRPython\n\n\n\n1 / 200 * 30\n\n[1] 0.15\n\n(59 + 73 - 2) / 3\n\n[1] 43.33333\n\n3^2\n\n[1] 9\n\nsin(pi / 2) # pi as Const number in R\n\n[1] 1\n\n\n\n\n\nprint(1 / 200 * 30)\n\n0.15\n\nprint((59 + 73 - 2) / 3)\n\n43.333333333333336\n\nprint(3**2)\n\n9\n\nimport math\nprint(math.sin(math.pi/2))\n\n1.0"
  },
  {
    "objectID": "dataprocess/basic_r_python.html#control-flow",
    "href": "dataprocess/basic_r_python.html#control-flow",
    "title": "R & Python Basic",
    "section": "2.2 Control flow",
    "text": "2.2 Control flow\nThere are two primary tools of control flow: choices and loops.\n\nChoices, like if statements calls, allow you to run different code depending on the input.\nLoops, like for and while, allow you to repeatedly run code, typically with changing options.\n\n\n2.2.1 choices\n\n2.2.1.1 Basic If-Else\n\nRPython\n\n\nThe basic form of an if statement in R is as follows:\n\nif (condition) {\n  true_action\n}\nif (condition) {\n  true_action\n} else {\n  false_action\n}\n\nIf condition is TRUE, true_action is evaluated; if condition is FALSE, the optional false_action is evaluated.\nTypically the actions are compound statements contained within {:\nif returns a value so that you can assign the results:\n\na &lt;- 6\nb &lt;- 8\n\nif (b &gt; a) {\n  cat(\"b is greater than a\\n\")\n} else if (a == b) {\n  cat(\"a and b are equal\\n\")\n} else {\n  cat(\"a is greater than b\\n\")\n}\n\nb is greater than a\n\n\n\n\n\n# if statements\nif condition: \n  true_action\n  \n# if-else\nif condition: \n  true_action \nelse: \n  false_action\n\n\n# if-ifel-else\nif condition1: \n  true_action1 \nelif condition2: \n  true_action2 \nelse: \n  false_action\n\n\na = 6\nb = 8\nif b &gt; a:\n  print(\"b is greater than a\")\nelif a == b:\n  print(\"a and b are equal\")\nelse:\n  print(\"a is greater than b\")\n\nb is greater than a\n\n\n\n\n\n\n\n2.2.1.2 switch\n\nRPython\n\n\nClosely related to if is the switch()-statement. It’s a compact, special purpose equivalent that lets you replace code like:\n\nx_option &lt;- function(x) {\n  if (x == \"a\") {\n    \"option 1\"\n  } else if (x == \"b\") {\n    \"option 2\" \n  } else if (x == \"c\") {\n    \"option 3\"\n  } else {\n    stop(\"Invalid `x` value\")\n  }\n}\n\nwith the more succinct:\n\nx_option &lt;- function(x) {\n  switch(x,\n    a = \"option 1\",\n    b = \"option 2\",\n    c = \"option 3\",\n    stop(\"Invalid `x` value\")\n  )\n}\nx_option(\"b\")\n\n[1] \"option 2\"\n\n\nThe last component of a switch() should always throw an error, otherwise unmatched inputs will invisibly return NULL:\n\n\n\nmatch subject:\n    case &lt;pattern_1&gt;:\n        &lt;action_1&gt;\n    case &lt;pattern_2&gt;:\n        &lt;action_2&gt;\n    case &lt;pattern_3&gt;:\n        &lt;action_3&gt;\n    case _:\n        &lt;action_wildcard&gt;\n\n\ndef x_option(x):\n    options = {\n        \"a\": \"option 1\",\n        \"b\": \"option 2\",\n        \"c\": \"option 3\"\n    }\n    return options.get(x, \"Invalid `x` value\")\n\nprint(x_option(\"b\"))\n\noption 2\n\n\n\n\n\n\n\n2.2.1.3 Vectorised if\n\nRPython\n\n\nGiven that if only works with a single TRUE or FALSE, you might wonder what to do if you have a vector of logical values. Handling vectors of values is the job of ifelse(): a vectorised function with test, yes, and no vectors (that will be recycled to the same length):\n\nx &lt;- 1:10\nifelse(x %% 5 == 0, \"XXX\", as.character(x))\n\n [1] \"1\"   \"2\"   \"3\"   \"4\"   \"XXX\" \"6\"   \"7\"   \"8\"   \"9\"   \"XXX\"\n\nifelse(x %% 2 == 0, \"even\", \"odd\")\n\n [1] \"odd\"  \"even\" \"odd\"  \"even\" \"odd\"  \"even\" \"odd\"  \"even\" \"odd\"  \"even\"\n\n\nNote that missing values will be propagated into the output.\nI recommend using ifelse() only when the yes and no vectors are the same type as it is otherwise hard to predict the output type. See https://vctrs.r-lib.org/articles/stability.html#ifelse for additional discussion.\n\n\n\n\n\n\n\n\n\n2.2.2 Loops\n\n2.2.2.1 for-Loops\nA for loop is used for iterating over a sequence (that is either a list, a tuple, a dictionary, a set, or a string). For each item in vector, perform_action is called once; updating the value of item each time.\n\nRPython\n\n\nIn R, for loops are used to iterate over items in a vector. They have the following basic form:\n\nfor (item in vector) perform_action\n\n\nfor (i in 1:3) {\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n\n\n\n\n\nfor item in vector \n  perform_action\n\n\nfor i in range(1, 3):\n  print(i)\n\n1\n2\n\n\n\n\n\n\n\n2.2.2.2 while-Loops\nWith the while loop we can execute a set of statements as long as a condition is TRUE:\n\nRPython\n\n\n\ni &lt;- 1\nwhile (i &lt; 6) {\n  print(i)\n  i &lt;- i + 1\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\n\n\n\ni = 1\nwhile i &lt; 6:\n  print(i)\n  i += 1\n\n1\n2\n3\n4\n5\n\n\n\n\n\n\n\n2.2.2.3 terminate\n\nRPython\n\n\nThere are two ways to terminate a for loop early:\n\nnext exits the current iteration.\nbreak exits the entire for loop.\n\n\nfor (i in 1:10) {\n  if (i &lt; 3) \n    next\n\n  print(i)\n  \n  if (i &gt;= 5)\n    break\n}\n\n[1] 3\n[1] 4\n[1] 5\n\n\n\n\n\nfor i in range(1, 10):\n    if i &lt; 3:\n        continue\n    \n    print(i)\n    \n    if i &gt;= 5:\n        break\n\n3\n4\n5"
  },
  {
    "objectID": "dataprocess/basic_r_python.html#function",
    "href": "dataprocess/basic_r_python.html#function",
    "title": "R & Python Basic",
    "section": "2.3 Function",
    "text": "2.3 Function\nMore details of in Advanced R Chapter 6\nA function is a block of code which only runs when it is called. It can be broken down into three components:\n\nThe formals(), the list of arguments that control how you call the function.\nThe body(), the code inside the function.\nThe environment(), the data structure that determines how the function finds the values associated with the names.\n\nWhile the formals and body are specified explicitly when you create a function, the environment is specified implicitly, based on where you defined the function. This location could be within another package or within the workspace (global environment).\n\nRPython\n\n\nThe function environment always exists, but it is only printed when the function isn’t defined in the global environment.\n\nfct_add &lt;- function(x, y) {\n  # A comment\n  x + y\n}\n\n# Get the formal arguments\nformals(fct_add)\n\n$x\n\n\n$y\n\n# Get the function's source code (body)\nbody(fct_add)\n\n{\n    x + y\n}\n\n# Get the function's global environment (module-level namespace)\nenvironment(fct_add)\n\n&lt;environment: R_GlobalEnv&gt;\n\n\n\n\n\ndef fct_add(x, y):\n    # A comment\n    return x + y\n\n# Get the formal arguments\nprint(fct_add.__code__.co_varnames)\n\n('x', 'y')\n\n# Get the function's source code (body)\nprint(fct_add.__code__.co_code)\n\nb'\\x97\\x00|\\x00|\\x01z\\x00\\x00\\x00S\\x00'\n\n# Get the function's global environment (module-level namespace)\nprint(fct_add.__globals__)\n\n{'__name__': '__main__', '__doc__': None, '__package__': None, '__loader__': &lt;class '_frozen_importlib.BuiltinImporter'&gt;, '__spec__': None, '__annotations__': {}, '__builtins__': &lt;module 'builtins' (built-in)&gt;, 'r': &lt;__main__.R object at 0x000002577B44E4D0&gt;, 'x': [2.1, 4.2, 3.3, -5.6, 5.4, -1, 6.7, 7.9], 'y': 1.0, 'np': &lt;module 'numpy' from 'C:\\\\Users\\\\lei\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\PYTHON~1\\\\Lib\\\\site-packages\\\\numpy\\\\__init__.py'&gt;, 'pd': &lt;module 'pandas' from 'C:\\\\Users\\\\lei\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\PYTHON~1\\\\Lib\\\\site-packages\\\\pandas\\\\__init__.py'&gt;, 'vct_Test': [1, 5, 7], 'ary_Test': array([[[ 1,  2,  3,  4],\n        [ 5,  6,  7,  8],\n        [ 9, 10, 11, 12]],\n\n       [[13, 14, 15, 16],\n        [17, 18, 19, 20],\n        [21, 22, 23, 24]]]), 'mat_Test': array([[ 1,  2,  3,  4],\n       [ 5,  6,  7,  8],\n       [ 9, 10, 11, 12],\n       [13, 14, 15, 16],\n       [17, 18, 19, 20],\n       [21, 22, 23, 24]]), 'lst_Test': [[1, 3, 5], 'abc', False], 'df_Test':   name  age\n0  Bob   12\n1  Tom   13, 'a2': array([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]]), 'colnames': ['A', 'B', 'C'], 'a3': array([[[ 1,  2,  3,  4],\n        [ 5,  6,  7,  8],\n        [ 9, 10, 11, 12]],\n\n       [[13, 14, 15, 16],\n        [17, 18, 19, 20],\n        [21, 22, 23, 24]]]), 'df':    x  y  z\n0  1  3  a\n1  2  2  b\n2  3  1  c, 'a': 6, 'b': 8, 'x_a': array([2.1, 4.2, 3.3, 5.4]), 'idx': 0    False\n1     True\n2     True\nName: x, dtype: bool, 'idx_x_gt_1': 0    False\n1     True\n2     True\nName: x, dtype: bool, 'idx_z_eq_a': 0     True\n1    False\n2    False\nName: z, dtype: bool, 'math': &lt;module 'math' (built-in)&gt;, 'x_option': &lt;function x_option at 0x000002577C79CCC0&gt;, 'i': 5, 'fct_add': &lt;function fct_add at 0x000002577C7A5260&gt;}\n\n\n\n\n\n\n2.3.1 Call\n\nRPython\n\n\nCalling Syntax:\n\nfunction_name(argument1 = value1, argument2 = value2, ...)\n\nTry using seq(), which makes regular sequences of numbers:\n\nseq(from = 1, to = 10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nWe often omit the names of the first several arguments in function calls, so we can rewrite this as follows:\n\nseq(1, 10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nWe can also check the arguments and other information with:\n?seq\nThe “help” windows shows as:\n\n\n\nCalling Syntax:\n\nfunction_name(argument1 = value1, argument2 = value2)\n\n\nsequence = list(range(1, 11))\nprint(sequence)\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n\n\n\n\n\n\n2.3.2 Define\n\nRPython\n\n\nUse the function() keyword:\n\nmy_add1 &lt;- function(x) {\n  x + 1\n}\n\ncalling the function my_add1:\n\nmy_add1(2)\n\n[1] 3\n\n\n\n\n\n\n\n\nTip\n\n\n\nIn R, the return statement is not essential for a function to yield a value as its result. By default, R will return the result of the last command within the function as its output.\n\n\n\n\nIn Python a function is defined using the def keyword:\n\ndef my_add(x):\n  return x + 1\n\ncalling the function my_add1:\n\nprint(my_add(2))\n\n3\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe return statement is essential for a function to yield a value as its result."
  },
  {
    "objectID": "dataprocess/basic_r_python.html#naming-rules",
    "href": "dataprocess/basic_r_python.html#naming-rules",
    "title": "R & Python Basic",
    "section": "3.1 Naming rules",
    "text": "3.1 Naming rules\n\nRPython\n\n\n\nmust start with a letter\ncan only contain letters, numbers, underscores _, and dot .\ncase-sensitive (age, Age and AGE are three different variables)\ncannot be any of the Reserved Words\n\nTRUE FALSE\nNULL Inf NaN NA NA_real NA_complex_ NA_character_\nif else\nfor while repeat\nnext break\nfunction\nin\n\n\n\n\n\n\n\n\n\n\nLegal\n\n\n\ni_use_snake_case\notherPeopleUseCamelCase\nsome.people.use.periods\naFew.People_RENOUNCEconvention6\n\n\n\n\n\n\n\n\n\n\n\nIllegal\n\n\n\n_start_with_underscores\n1_start_with_number\nif\ncontain sapce\ncontain-other+charater\n\n\n\n\nmore Reserved Words in:\nhelp(\"reserved\")\n\n\n\nmust start with a letter or the underscore character _\ncan only contain letters, numbers, and underscores _\ncase-sensitive (age, Age and AGE are three different variables)\ncannot be any of the Python keywords (35 keywors in Python 3.8)\n\nTrue False\nNone\nif else elif\nfor while repeat\ntry break continue finally\ndef\nin and or not\nreturn\n\n\n\n\n\n\n\n\n\n\nLegal\n\n\n\ni_use_snake_case\n_start_with_underscores\notherPeopleUseCamelCase\naFew_People_RENOUNCEconvention6\n\n\n\n\n\n\n\n\n\n\n\nIllegal\n\n\n\nwant.contain.dot\n1_start_with_number\nif\ncontain sapce\ncontain-other+charater\n\n\n\n\nMore Keywords in:\n\nhelp(\"keywords\")"
  },
  {
    "objectID": "dataprocess/basic_r_python.html#naming-conventions",
    "href": "dataprocess/basic_r_python.html#naming-conventions",
    "title": "R & Python Basic",
    "section": "3.2 Naming Conventions",
    "text": "3.2 Naming Conventions\n\nCamel Case\n\nEach word, except the first, starts with a capital letter:\nmyVariableName\n\nPascal Case\n\nEach word starts with a capital letter:\nMyVariableName\n\nSnake Case\n\nEach word is separated by an underscore character:\nmy_variable_name"
  },
  {
    "objectID": "dataprocess/basic_format.html",
    "href": "dataprocess/basic_format.html",
    "title": "Basic Data & File Format",
    "section": "",
    "text": "In the realm of hydrological modeling, various data structures and file formats are employed to effectively manage, analyze, and simulate hydrological processes. These structures and formats facilitate the representation of hydrological data, making it accessible for researchers, modelers, and decision-makers. Below are some of the common data structures and file formats used in hydrological modeling, along with their key features.\nOverview:"
  },
  {
    "objectID": "dataprocess/basic_format.html#array",
    "href": "dataprocess/basic_format.html#array",
    "title": "Basic Data & File Format",
    "section": "Array",
    "text": "Array\nArrays are collections of elements, typically of the SAME data type, organized in a linear or multi-dimensional fashion. They provide efficient data storage and manipulation, making them essential for numerical computations."
  },
  {
    "objectID": "dataprocess/basic_format.html#table-dataframe",
    "href": "dataprocess/basic_format.html#table-dataframe",
    "title": "Basic Data & File Format",
    "section": "Table (Dataframe)",
    "text": "Table (Dataframe)\nTabular data structures, often referred to as tables, are a fundamental way of organizing and representing data in a structured format. They consist of rows and columns, where each row typically represents a single observation or record, and each column represents a specific attribute or variable associated with those observations. Tabular structures are highly versatile and are widely used for storing and analyzing various types of data, ranging from simple lists to complex datasets. This characteristic of tables enables them to represent and manage a wide range of information efficiently.\nCompare to array, in a table, columns are allowed to have different data types, but all values within a specific column must share the same data type."
  },
  {
    "objectID": "dataprocess/basic_format.html#sec-spatialData",
    "href": "dataprocess/basic_format.html#sec-spatialData",
    "title": "Basic Data & File Format",
    "section": "Spatial Data",
    "text": "Spatial Data\nMore Details in Spatial Data Science\nSpatial data refers to data that has a geographic or spatial component, representing the locations and shapes of physical objects on the Earth’s surface. This type of data is essential in various fields, including geography, environmental science, urban planning, and more. One of the key elements in spatial data is its association with coordinate systems, which allow precise location referencing.\n\nSpatial Vector\nSpatial vector data structures represent geometric shapes like points, lines, and polygons in space. They are widely used in geographic information systems (GIS) for mapping and analyzing spatial data, such as landuse boundaries or river networks.\n\nThe term “Vector” is used because spatial vector data is essentially stored as a vector of points, lines, or polygons (which are composed of lines). The data structure for geographic shapes is divided into two key components:\n\nGeometry: Geometry represents the spatial shape or location of the geographic feature. It defines the boundaries, points, lines, or polygons that make up the feature. These geometric elements are used to precisely describe the geometric feature.\nAttributes: Attributes are associated with the geographic feature and provide additional information about it. These attributes can include data such as the feature’s name, population, temperature, or any other relevant details. Attributes are typically organized and stored in a tabular format, making it easy to perform data analysis and visualization.\n\nThe data structure of points in geospatial data is relatively simple. The geometry of one point is described by its coordinates, typically represented as X (or longitude) and Y (or latitude) values.\n\nOn the other hand, lines and polygons are more complex geometric shapes. The geometry of a line or polygon is defined by a sequence of multiple points. These points are connected in a specific order to form the shape of the line or polygon. In other words, the geometry of every line (or polygon) is composed of a series of coordinates of points.\n\n\n\nSpatial Raster\nSpatial raster data structures are grid-based representations of spatial data, where each cell holds a value. They are commonly used for storing continuous data, like satellite imagery or elevation models.\nThe datastructure of raster data is quite simple. In a raster, each row shares the same X value, and each column shares the same Y value. Additionally, in most situations, the resolution in each dimension remains constant. This means that specifying the starting point and the resolutions is usually sufficient to describe the coordinates of every grid cell. A single raster layer indeed resembles a 2D matrix.\n\n\n\nCoordinate Reference System (CRS)\nIn addition to Geometry (of Vector) and Koordinate (of aster), another essential component of spatial data is the Coordinate Reference System (CRS). The CRS plays a crucial role in geospatial data by providing a framework for translating the Earth’s 3D surface into a 2D coordinate system.\nKey points about the Coordinate Reference System (CRS) include:\n\nAngular coordinates: The earth has an irregular spheroid-like shape. The natural coordinate reference system for geographic data is longitude/latitude.\nProjection: The CRS defines how the Earth’s curved surface is projected onto a 2D plane, enabling the representation of geographic features on maps and in geographic information systems (GIS). Different projection methods exist, each with its own strengths and weaknesses depending on the region and purpose of the map.\nUnits: CRS specifies the units of measurement for coordinates. Common units include degrees (for latitude and longitude), meters, and feet, among others.\nReference Point: It establishes a reference point (usually the origin) and orientation for the coordinate system.\nEPSG Code: Many CRS are identified by an EPSG (European Petroleum Survey Group) code, which is a unique numeric identifier that facilitates data sharing and standardization across GIS systems.\n\nThe CRS is fundamental for correctly interpreting and analyzing spatial data, as it ensures that geographic features are accurately represented in maps and GIS applications. Different CRSs are used for different regions and applications to minimize distortion and provide precise geospatial information.\nThe use of EPSG (European Petroleum Survey Group) codes is highly recommended for defining Coordinate Reference Systems (CRS) in spatial data. These codes consist of a string of numbers that uniquely identify a specific CRS. By using EPSG codes, you can easily access comprehensive definitions of different CRSs, which include details about their coordinate systems, datums, projections, and other parameters. Many software applications and libraries support EPSG codes, making it a standardized and convenient way to specify CRS information in spatial data.\nYou can obtain information about EPSG codes from the EPSG website. This website serves as a valuable resource for accessing detailed information associated with EPSG codes, including coordinate reference system (CRS) definitions and specifications."
  },
  {
    "objectID": "dataprocess/basic_format.html#time-series",
    "href": "dataprocess/basic_format.html#time-series",
    "title": "Basic Data & File Format",
    "section": "Time Series",
    "text": "Time Series\nTime series data structures are specifically designed to capture and represent information recorded over a period of time. They play a crucial role in analyzing trends, patterns, and dependencies within sequences of data. Time series data, by definition, have a temporal dimension, making time an essential component of these structures.\nIn comparison to spatial information, time information is relatively straightforward. When the time dimension progresses in uniform steps, it can be efficiently described using the start time and step intervals. However, when the time intervals are irregular or non-uniform, additional time-related details are necessary. This can include specifying the year, month, and day for date-based time data or the hour, minute, and second for time-based information.\nIt’s worth noting that while most time series data adheres to the standard calendar system, some datasets may use alternative calendar systems such as the Julian calendar. Additionally, time zone information is crucial when working with time data, as it ensures accurate temporal references across different geographical regions."
  },
  {
    "objectID": "dataprocess/basic_format.html#plain-text-ascii",
    "href": "dataprocess/basic_format.html#plain-text-ascii",
    "title": "Basic Data & File Format",
    "section": "Plain text (ASCII)",
    "text": "Plain text (ASCII)\nASCII (American Standard Code for Information Interchange) is a plain text format, making it human-readable.\n\nAdvantages\n\nHuman-Readable: Users can easily view, understand, and edit the data directly in a text editor.\nWidespread Support, Ease of Import/Export: ASCII is universally supported. Most programming languages, data analysis tools, and software applications can read and write ASCII files, ensuring high compatibility.\nLightweight: ASCII files are typically lightweight and do not consume excessive storage space, making them suitable for large datasets.\nSimple Structure: ASCII files have a straightforward structure, often using lines of text with fields separated by delimiters. This simplicity aids in data extraction and manipulation.\n\n\n\nDisadvantages\n\nLimited Data Types: ASCII primarily handles text-based data and is not suitable for complex data types such as images, multimedia, or hierarchical data.\nNo Inherent Data Validation: ASCII files lack built-in mechanisms for data validation or integrity checks, requiring users to ensure data conformity.\nLack of Compression: ASCII files do not inherently support data compression, potentially resulting in larger file sizes compared to binary formats.\nSlower Reading/Writing: Reading and writing data in ASCII format may be slower, especially for large datasets, due to additional parsing required to interpret text-based data.\n\n\n\nFile format for ASCII data\nWhen it comes to plain text formats, there is no universal standard, and it’s highly adaptable to specific needs. The initial step in loading a plain text table is to analyze the structure of the file.\nTypically, a text table can store 2D data, comprising columns and rows or a matrix. However, above the data body, there’s often metadata that describes the data. Metadata can vary widely between data body.\nDividing rows is usually straightforward and can be achieved by identifying row-end characters. However, dividing columns within each row presents multiple possibilities, such as spaces, tabs, commas, or semicolons.\n\n.txt: This is the most generic and widely used file extension for plain text files. It doesn’t imply any specific format or structure; it’s just a simple text file.\n.csv (Comma-Separated Values): While CSV files contain data separated by commas, they are still considered ASCII files because they use plain text characters to represent data values. Each line in a CSV file typically represents a record, with values separated by commas.\n\nIn .txt files, any of these separators can be used, but in .csv files, commas or semicolons are commonly employed as separator characters."
  },
  {
    "objectID": "dataprocess/basic_format.html#excel-files",
    "href": "dataprocess/basic_format.html#excel-files",
    "title": "Basic Data & File Format",
    "section": "Excel Files",
    "text": "Excel Files\nExcel files, often denoted with the extensions .xls or .xlsx, are a common file format used for storing structured data in tabular form. These files are not to be confused with the Microsoft Excel software itself but are the data containers created and manipulated using spreadsheet software like Excel.\nExcel files are widely used in various applications, including data storage, analysis, reporting, and sharing. They consist of rows and columns, where each cell can contain text, numbers, formulas, or dates. These files are versatile and can hold different types of data, making them a popular choice for managing information.\n\nAdvantages:\n\nUser-Friendly Interface: Excel’s user-friendly interface makes it accessible to users with varying levels of expertise. Its familiar grid layout simplifies data input and manipulation.\nVersatility: Excel can handle various types of data, from simple lists to complex calculations.\nFormulas and Functions: Excel provides an extensive library of built-in formulas and functions, allowing users to automate calculations and streamline data processing.\nData Visualization: Creating charts and graphs in Excel is straightforward. It helps in visualizing data trends and patterns, making complex information more accessible.\nData Validation: Excel allows you to set rules and validation criteria for data entry, reducing errors and ensuring data accuracy.\n\n\n\nDisadvantages:\n\nLimited Data Handling: Excel has limitations in handling very large datasets. Performance may degrade, and it’s not suitable for big data analytics.\nLack of Version Control: Excel lacks robust version control features, making it challenging to track changes and manage document versions in collaborative environments.\n\nIn conclusion, Excel is a valuable tool for various data-related tasks but comes with limitations in terms of scalability, data integrity, and security. Careful consideration of its strengths and weaknesses is essential when deciding whether it’s the right choice for your data management needs."
  },
  {
    "objectID": "dataprocess/basic_format.html#binary",
    "href": "dataprocess/basic_format.html#binary",
    "title": "Basic Data & File Format",
    "section": "Binary",
    "text": "Binary\nUnlike text-based files, binary files store data in a way that is optimized for computer processing and can represent a wide range of data types, from simple numbers to complex structures. These files are used in various applications, including programming, scientific research, and data storage, due to their efficiency in handling data.\n\nAdvantages of Binary Formats\n\nEfficiency: Binary formats are highly efficient for data storage and transmission because they represent data in a compact binary form. This can significantly reduce storage space and data transfer times, making them ideal for large datasets.\nData Integrity: Binary formats often include built-in mechanisms for data integrity and error checking. This helps ensure that data remains intact and accurate during storage and transmission.\nComplex Data: Binary formats can represent complex data structures, which makes them suitable for a wide range of data types.\nFaster I/O: Reading and writing data in binary format is generally faster than text-based formats like ASCII. This efficiency is particularly important for applications that require high-speed data processing.\nSecurity: Binary formats can provide a level of data security because they are not easily human-readable. This can be advantageous when dealing with sensitive information.\n\n\n\nDisadvantages of Binary Formats\n\nLack of Human-Readability: Binary formats are not human-readable, making it difficult to view or edit the data directly. This can be a disadvantage when data inspection or manual editing is required.\nCompatibility: Binary formats may not be universally compatible across different software platforms and programming languages. This can lead to issues when sharing or accessing data in various environments.\nLimited Metadata: Binary formats may not include comprehensive metadata structures, making it challenging to document and describe the data effectively.\nVersion Compatibility: Changes in the binary format’s structure or encoding can lead to compatibility issues when working with data created using different versions of software or hardware.\nPlatform Dependence: Binary formats can be platform-dependent, meaning they may not be easily transferable between different operating systems or hardware architectures.\n\nBinary formats are a valuable choice for certain applications, particularly when efficiency, data integrity, and complex data types are crucial. However, they may not be suitable for all scenarios, especially when human readability, compatibility, or ease of data inspection is essential."
  },
  {
    "objectID": "dataprocess/basic_format.html#nectcdf",
    "href": "dataprocess/basic_format.html#nectcdf",
    "title": "Basic Data & File Format",
    "section": "NectCDF",
    "text": "NectCDF\n\nNetCDF (Network Common Data Form) is a versatile data format widely used in scientific and environmental applications. It is primarily a binary data format, but it includes structured elements for efficient data storage and management. Here are some key characteristics of NetCDF:\n\nBinary Representation: NetCDF data files are primarily stored in binary format, which enables efficient storage and handling of numerical data, particularly floating-point numbers.\nSelf-Describing: NetCDF files are self-describing, meaning they include metadata alongside the data. This metadata provides essential information about the data’s structure, dimensions, units, and other attributes.\nHierarchical Structure: NetCDF supports a hierarchical structure capable of representing complex data types, including multi-dimensional arrays and groups of data variables.\nData Compression: NetCDF allows for data compression, which can reduce the storage space required for large datasets while maintaining data integrity.\nLanguage Support: NetCDF libraries and tools are available for multiple programming languages, making it accessible to a wide range of scientific and data analysis applications.\n\nNetCDF’s combination of binary efficiency and structured metadata makes it an invaluable choice for storing and sharing scientific data, particularly in fields such as meteorology, oceanography, and environmental science."
  },
  {
    "objectID": "dataprocess/basic_format.html#database-systems",
    "href": "dataprocess/basic_format.html#database-systems",
    "title": "Basic Data & File Format",
    "section": "Database Systems",
    "text": "Database Systems\nDatabase Systems, such as SQL and NoSQL databases, are crucial for efficiently managing and querying large, structured datasets. They provide structured data storage, ensuring data integrity and consistency. SQL databases like MySQL and PostgreSQL are well-suited for relational data, while NoSQL databases like MongoDB excel in handling semi-structured or unstructured data. These systems are commonly used for storing long-term observational data, model outputs, and sensor data in scientific research and various enterprise applications.\n\nAdvantages\n\nEfficient Data Retrieval: Databases are optimized for querying and retrieving data, making it quick and efficient to access information.\nData Integrity: Databases enforce data integrity rules, ensuring that data remains consistent and reliable over time.\nStructured Storage: They provide a structured way to store data, making it easier to organize and manage large datasets.\nConcurrent Access: Multiple users or applications can access the database simultaneously, enabling collaboration and scalability.\nSecurity: Database systems offer security features like user authentication and authorization to protect sensitive data.\nBackup and Recovery: They often include mechanisms for automated data backup and recovery, reducing the risk of data loss.\n\n\n\nDisadvantages\n\nComplexity: Setting up and maintaining a database can be complex and requires specialized knowledge.\nCost: Licensing, hardware, and maintenance costs can be significant, especially for enterprise-grade database systems.\nScalability Challenges: Some database systems may face scalability limitations as data volume grows.\nLearning Curve: Users and administrators need to learn query languages (e.g., SQL) and database management tools.\nOverhead: Databases can introduce overhead due to indexing, data normalization, and transaction management.\nVendor Lock-In: Depending on the chosen database system, there may be vendor lock-in, making it challenging to switch to another system.\nResource Intensive: Databases consume computing resources, such as CPU and RAM, which can affect system performance.\n\nThe choice of using a database system depends on specific requirements, such as data volume, complexity, security, and scalability needs. It’s essential to carefully evaluate the advantages and disadvantages in the context of your project."
  },
  {
    "objectID": "dataprocess/basic_format.html#spatial-data-file-formats",
    "href": "dataprocess/basic_format.html#spatial-data-file-formats",
    "title": "Basic Data & File Format",
    "section": "Spatial Data File Formats",
    "text": "Spatial Data File Formats\nSpatial data files are a specialized type of data format designed for storing geographic or location-based information. Unlike standard data files that store text, numbers, or other types of data, spatial data files are tailored for representing the geographical features of our world.\nSpatial data comes in various file formats, each tailored for specific types of geographic data and applications. Here are some commonly used formats and their key differences:\n\nRaster Data Formats\nMore Raster file-formats in GDAL\n\nTIFF (Tagged Image File Format):\n\nA widely used raster format for storing high-quality images and raster datasets.\nSupports georeferencing and metadata, making it suitable for spatial applications.\n\nASC (Arc/Info ASCII Grid):\n\nA plain text format used to represent raster data in a grid format.\nContains elevation or other continuous data with rows and columns of values.\n\nJPEG (Joint Photographic Experts Group), PNG (Portable Network Graphics):\n\nCommonly used for photographs and images, but not ideal for spatial analysis due to lossy compression.\n\n\n\n\nVector Data Formats\nMore Vector file-formats in GDAL\n\nShapefile (SHP):\n\nOne of the most common vector formats used in GIS applications.\nConsists of multiple files (.shp, .shx, .dbf, etc.) to store point, line, or polygon geometries and associated attributes.\n\n\n\n\n\nFile extension\nContent\n\n\n\n\n.dbf\nAttribute information\n\n\n.shp\nFeature geometry\n\n\n.shx\nFeature geometry index\n\n\n.aih\nAttribute index\n\n\n.ain\nAttribute index\n\n\n.prj\nCoordinate system information\n\n\n.sbn\nSpatial index file\n\n\n.sbx\nSpatial index file\n\n\n\n\nGeoPackage (GPKG):\n\nAn open, standards-based platform-independent format for spatial data.\nCan store multiple layers, attributes, and geometries in a single file.\n\nKML (Keyhole Markup Language):\n\nXML-based format used for geographic visualization in Earth browsers like Google Earth.\nSuitable for storing points, lines, polygons, and related attributes.\n\nGeoJSON:\n\nA lightweight format for encoding geographic data structures using JSON (JavaScript Object Notation).\nIdeal for web applications due to its simplicity and ease of use."
  },
  {
    "objectID": "dataprocess/data_load.html",
    "href": "dataprocess/data_load.html",
    "title": "Data Loading",
    "section": "",
    "text": "This Aritcl will show the process to load data from other files. I t will divide into four paties: plain text (read able ASCII), Excel, NetCDF and spatial data.\nOverview:"
  },
  {
    "objectID": "dataprocess/data_load.html#example-file",
    "href": "dataprocess/data_load.html#example-file",
    "title": "Data Loading",
    "section": "1.1 Example File",
    "text": "1.1 Example File\nLet’s start with an example CSV file named Bachum_2763190000100.csv. This file contains pegel discharge data and is sourced from open data available at ELWAS-WEB NRW. You can also access it directly from the internet via Github, just like you would access a local file.\nTake a look:"
  },
  {
    "objectID": "dataprocess/data_load.html#library-and-functions",
    "href": "dataprocess/data_load.html#library-and-functions",
    "title": "Data Loading",
    "section": "1.2 Library and functions",
    "text": "1.2 Library and functions\n\nRPython\n\n\nFirst, we need to load the necessary library tidyverse. This library collection includes readr for reading files and dplyr for data manipulation, among others.\nAnd, we set the URL address as the file path (including the file name).\n\n# load the library\nlibrary(tidyverse)\nfn_Bachum &lt;- \"https://raw.githubusercontent.com/HydroSimul/Web/main/data_share/Bachum_2763190000100.csv\"\nfn_Datatype &lt;- \"https://raw.githubusercontent.com/HydroSimul/Web/main/data_share/load_Datatype.txt\"\n\nThe documentation for the readr library is available online and can be accessed at https://readr.tidyverse.org.\nOf particular interest are the following functions:\n\nreadr::read_csv()\nreadr::read_table()\n\nWe can observe that the CSV file is divided by semicolons. Therefore, it’s more appropriate to use read_csv2() rather than read_csv().\nThe difference between read_*() functions in the readr package is determined by the delimiter character used in the files:\n\n\n\nCHEAT SHEET from Rstudio\n\n\n\n\n\n# load the library\nimport pandas as pd\nfn_Bachum = \"https://raw.githubusercontent.com/HydroSimul/Web/main/data_share/Bachum_2763190000100.csv\"\nfn_Datatype = \"https://raw.githubusercontent.com/HydroSimul/Web/main/data_share/load_Datatype.txt\"\n\nThe documentation for the pandas library is available online and can be accessed at https://pandas.pydata.org/docs/index.html.\nOf particular interest are the following functions:\n\npandas.read_csv()\npandas.read_table()"
  },
  {
    "objectID": "dataprocess/data_load.html#metadata-handel",
    "href": "dataprocess/data_load.html#metadata-handel",
    "title": "Data Loading",
    "section": "1.3 Metadata Handel",
    "text": "1.3 Metadata Handel\nMetadata can vary widely between datasets, so it’s handled separately from the data body.\nThere are three ways to deal with metadata:\n\nDirectly Ignore: This approach involves ignoring metadata when it’s redundant or readily available from other data sources, such as file names or external references.\nExtract from Text: When metadata is crucial but not in table form, you can extract information from text strings. For more information, refer to the section on string manipulation Section 4.\nRead as a Second Table: If metadata is well-organized in a tabular format, it can be read as a separate table to facilitate its use.\n\nIn the Bachum_2763190000100.csv file, you will find that there are 10 lines of metadata, which are well-organized in a tabular format. However, it’s important to note that the consistency in values column varies.\n\n1.3.1 Directly Ignore use grguments skip\n\n# skip = 10\nread_csv2(fn_Bachum, skip = 10, n_max = 10, col_names = FALSE)\n\n# A tibble: 10 × 2\n   X1            X2\n   &lt;chr&gt;      &lt;dbl&gt;\n 1 01.01.1990  20.6\n 2 02.01.1990  19.0\n 3 03.01.1990  17.9\n 4 04.01.1990  16.8\n 5 05.01.1990  16.0\n 6 06.01.1990  14.8\n 7 07.01.1990  14.3\n 8 08.01.1990  14.0\n 9 09.01.1990  14.4\n10 10.01.1990  14.5\n\n\n\n\n1.3.2 Read metadata as table\nWhen directly reading all metadata into one table, you may encounter mixed data types. In the metadata, there are three data types:\n\nNumeric: Examples include Pegelnullpunkt and Einzugsgebiet.\nString: This category covers fields like Name, Pegelnummer, and others.\nDate: Date values are present in columns like Datum von and Datum bis.\n\nIn a data frame (tibble), columns must have the same data type. Consequently, R will automatically convert them to a single data type, which is typically string.\nTo address this situation, you should specify the data type you want to read. For example, to read the date values in lines 4 and 5, you can use the following settings: 1. skip = 3 to skip the first three lines of metadata. 2. n_max = 2 to read the next two lines (lines 4 and 5) as date values.\n\nRPython\n\n\n\n# skip = 3\nread_csv2(fn_Bachum, skip = 3, n_max = 2, col_names = FALSE)\n\n# A tibble: 2 × 2\n  X1        X2        \n  &lt;chr&gt;     &lt;chr&gt;     \n1 Datum von 01.01.1990\n2 Datum bis 31.12.2022\n\n\n\n\n\ndf_bach = pd.read_csv(fn_Bachum, skiprows=3, nrows=2, header=None, delimiter=';', encoding='latin-1')\nprint(df_bach)\n\n           0           1\n0  Datum von  01.01.1990\n1  Datum bis  31.12.2022\n\n\n\n\n\nUnfortunately, R may not always recognize date values correctly, so you may need to perform additional steps for conversion:\n\nAfter Reading: This involves transforming the data from its initial format to the desired date format within your R environment.\nSet the Data Type by Reading: Another approach is to set the data type while reading the data.\n\nMore details in the next section:"
  },
  {
    "objectID": "dataprocess/data_load.html#load-tabular-data",
    "href": "dataprocess/data_load.html#load-tabular-data",
    "title": "Data Loading",
    "section": "1.4 Load tabular data",
    "text": "1.4 Load tabular data\n\nRPython\n\n\nTo read the first 10 lines of metadata, you can use the n_max setting with a value of n_max = 10 in the read_csv2() function.\n\nread_csv2(fn_Bachum, n_max = 10, col_names = FALSE)\n\n# A tibble: 10 × 2\n   X1                          X2             \n   &lt;chr&gt;                       &lt;chr&gt;          \n 1 \"Name\"                      \"Bachum\"       \n 2 \"Pegelnummer\"               \"2763190000100\"\n 3 \"Gew\\xe4sser\"               \"Ruhr\"         \n 4 \"Datum von\"                 \"01.01.1990\"   \n 5 \"Datum bis\"                 \"31.12.2022\"   \n 6 \"Parameter\"                 \"Abfluss\"      \n 7 \"Q Einheit\"                 \"m\\xb3/s\"      \n 8 \"Tagesmittelwerte\"           &lt;NA&gt;          \n 9 \"Pegelnullpunkt [m\\xfcNHN]\" \"146,83\"       \n10 \"Einzugsgebiet [km\\xb2]\"    \"1.532,02\"     \n\n\nAfter dealing with the metadata, we can proceed to load the data body using the readr::read_*() function cluster. Plain text files typically store data in a tabular or matrix format, both of which have at most two dimensions. When using the readr::read_() function, it automatically returns a tibble. If your data in the text file is in matrix format, you can use conversion functions like as.matrix() to transform it into other data structures.\n\n# 1. load\ntb_Read &lt;- read_csv2(fn_Bachum, skip = 10, n_max = 10, col_names = FALSE)\ntb_Read\n\n# A tibble: 10 × 2\n   X1            X2\n   &lt;chr&gt;      &lt;dbl&gt;\n 1 01.01.1990  20.6\n 2 02.01.1990  19.0\n 3 03.01.1990  17.9\n 4 04.01.1990  16.8\n 5 05.01.1990  16.0\n 6 06.01.1990  14.8\n 7 07.01.1990  14.3\n 8 08.01.1990  14.0\n 9 09.01.1990  14.4\n10 10.01.1990  14.5\n\n# 2. convert\ndf_Read &lt;- as.data.frame(tb_Read)\nmat_Read &lt;- as.matrix(tb_Read)\n\ndf_Read\n\n           X1     X2\n1  01.01.1990 20.640\n2  02.01.1990 18.994\n3  03.01.1990 17.949\n4  04.01.1990 16.779\n5  05.01.1990 16.019\n6  06.01.1990 14.817\n7  07.01.1990 14.296\n8  08.01.1990 13.952\n9  09.01.1990 14.403\n10 10.01.1990 14.500\n\nmat_Read\n\n      X1           X2      \n [1,] \"01.01.1990\" \"20.640\"\n [2,] \"02.01.1990\" \"18.994\"\n [3,] \"03.01.1990\" \"17.949\"\n [4,] \"04.01.1990\" \"16.779\"\n [5,] \"05.01.1990\" \"16.019\"\n [6,] \"06.01.1990\" \"14.817\"\n [7,] \"07.01.1990\" \"14.296\"\n [8,] \"08.01.1990\" \"13.952\"\n [9,] \"09.01.1990\" \"14.403\"\n[10,] \"10.01.1990\" \"14.500\"\n\n\n\n\n\ntb_Read = pd.read_csv(fn_Bachum, skiprows=10, nrows=10, header=None, delimiter=';', decimal=',', encoding='latin-1')\nprint(tb_Read)\n\n            0       1\n0  01.01.1990  20.640\n1  02.01.1990  18.994\n2  03.01.1990  17.949\n3  04.01.1990  16.779\n4  05.01.1990  16.019\n5  06.01.1990  14.817\n6  07.01.1990  14.296\n7  08.01.1990  13.952\n8  09.01.1990  14.403\n9  10.01.1990  14.500"
  },
  {
    "objectID": "dataprocess/data_load.html#sec-datatype",
    "href": "dataprocess/data_load.html#sec-datatype",
    "title": "Data Loading",
    "section": "1.5 Data type",
    "text": "1.5 Data type\nIn this section, we will work with a custom-made text file that contains various data types and formats. The file consists of three rows, with one of them serving as the header containing column names, and six columns in total.\nLet’s take a look:\n\nActually the function will always guse the dattype for each column, when the data really normally format the function will return the right datatype for the data:\n\nRPython\n\n\n\nread_table(fn_Datatype)\n\n# A tibble: 2 × 6\n    int float_en float_de date_en    date_de    str  \n  &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;date&gt;     &lt;chr&gt;      &lt;chr&gt;\n1     1      0.1 0,1      2023-09-15 15.09.2023 en   \n2     9      9.6 9,6      2023-09-16 16.09.2023 de   \n\n\n\n\n\ndf = pd.read_table(fn_Datatype)\nprint(df)\n\n   int  float_en float_de     date_en     date_de str\n0    1       0.1      0,1  2023-09-15  15.09.2023  en\n1    9       9.6      9,6  2023-09-16  16.09.2023  de\n\nprint(df.dtypes)\n\nint           int64\nfloat_en    float64\nfloat_de     object\ndate_en      object\ndate_de      object\nstr          object\ndtype: object\n\n\n\n\n\nBy default, functions like readr::read_table() in R and pandas.read_table() in Python will attempt to guess data types automatically when reading data. Here’s how this guessing typically works:\n\nIf a column contains only numbers and decimal dots (periods), it will be recognized as numeric (double in R and int or float in Python).\nIf a date is formatted in “Y-M-D” (e.g., “2023-08-27”) or “h:m:s” (e.g., “15:30:00”) formats, it may be recognized as a date or time type. Nur in R\nIf the data type cannot be confidently determined, it is often treated as a string (str in R and object in Python).\n\nThis automatic guessing is convenient, but it’s essential to verify the inferred data types, especially when working with diverse datasets.\n\n1.5.1 Set the Data Type by Reading\nExplicitly setting data types using the col_types (in R) or dtype (in Python) argument can help ensure correct data handling.\n\nRPython\n\n\nTo address the issue of date recognition, you can set the col_types argument, you can use a compact string representation where each character represents one column:\n\nc: Character\ni: Integer\nn: Number\nd: Double\nl: Logical\nf: Factor\nD: Date\nT: Date Time\nt: Time\n?: Guess\n_ or -: Skip\n\nto \"cD\" when reading the data. This informs the function that the first column contains characters (c) and the second column contains Dates (D).\n\nread_table(fn_Datatype, col_types = \"iddDDc\")\n\n# A tibble: 2 × 6\n    int float_en float_de date_en    date_de str  \n  &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;date&gt;     &lt;date&gt;  &lt;chr&gt;\n1     1      0.1       NA 2023-09-15 NA      en   \n2     9      9.6       NA 2023-09-16 NA      de   \n\n\n\nread_table(fn_Datatype, col_types = \"idd?Dc\")\n\n# A tibble: 2 × 6\n    int float_en float_de date_en    date_de str  \n  &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;date&gt;     &lt;date&gt;  &lt;chr&gt;\n1     1      0.1       NA 2023-09-15 NA      en   \n2     9      9.6       NA 2023-09-16 NA      de   \n\n\n\n\nTo set data types when reading data using functions pandas.read_*, you have three main choices by using the dtype parameter:\n\nstr: Specify the data type as a string.\nint: Specify the data type as an integer.\nfloat: Specify the data type as a floating-point number.\n\nHowever, you can also use the dtype parameter with a callable function to perform more advanced type conversions. Some commonly used functions include:\n\npd.to_datetime: Converts a column to datetime format.\npd.to_numeric: Converts a column to numeric (integer or float) format.\npd.to_timedelta: Converts a column to timedelta format.\n\n\n# Define column names and types as a dictionary\ncol_types = {\"X1\": str, \"X2\": pd.to_datetime}\n# Read the CSV file, skip 3 rows, read 2 rows, and specify column names and types\ndf = pd.read_csv(fn_Bachum, skiprows=3, nrows=2, header=None, delimiter=';', names=[\"X1\", \"X2\"], dtype=col_types, encoding='latin-1')\n\n# Display the loaded data\nprint(df)\n\n\nDON’T RUN Error, because data doesn’t match the default format of ‘Y-m-d’.\n\n\n\n\n\nUnfortunately, the default date format in R and Python may not work for German-style dates like “d.m.Y” as R and Python primarily recognizes the “Y-m-d” format.\n\n\n\n1.5.2 After Reading\nTo address this issue, you can perform date conversions after reading the data:\n\nRPython\n\n\nUsing function as.Date() and specify the date format using the format argument, such as format = \"%d.%m.%Y\".\n\ndf_Date &lt;- read_csv2(fn_Bachum, skip = 3, n_max = 2, col_names = FALSE)\ndf_Date$X2 &lt;- df_Date$X2 |&gt; as.Date(format = \"%d.%m.%Y\")\ndf_Date\n\n# A tibble: 2 × 2\n  X1        X2        \n  &lt;chr&gt;     &lt;date&gt;    \n1 Datum von 1990-01-01\n2 Datum bis 2022-12-31\n\n\n\n\n\ndf_Date = pd.read_csv(fn_Bachum, skiprows=3, nrows=2, header=None, delimiter=';', encoding='latin-1')\n\n# Display the loaded data\nprint(df_Date)\n\n           0           1\n0  Datum von  01.01.1990\n1  Datum bis  31.12.2022\n\n# 2. Convert the second column (X2) to a date format\ndf_Date[1] = pd.to_datetime(df_Date[1], format='%d.%m.%Y')\n\n# Display the DataFrame with the second column converted to date format\nprint(df_Date)\n\n           0          1\n0  Datum von 1990-01-01\n1  Datum bis 2022-12-31"
  },
  {
    "objectID": "dataprocess/data_load.html#example-file-1",
    "href": "dataprocess/data_load.html#example-file-1",
    "title": "Data Loading",
    "section": "2.1 Example File",
    "text": "2.1 Example File\nLet’s begin with an example Excel file named Pegeln_NRW.xlsx. This file contains information about measurement stations in NRW (Nordrhein-Westfalen, Germany) and is sourced from open data available at ELWAS-WEB NRW. You can also access it directly from Github.\nTake a look:"
  },
  {
    "objectID": "dataprocess/data_load.html#r-library-and-functions",
    "href": "dataprocess/data_load.html#r-library-and-functions",
    "title": "Data Loading",
    "section": "2.2 R library and functions",
    "text": "2.2 R library and functions\nTo load the necessary library, readxl, and access its help documentation, you can visit this link. The readxl::read_excel() function is versatile, as it can read both .xls and .xlsx files and automatically detects the format based on the file extension. Additionally, you have the options of using read_xls() for .xls files and read_xlsx() for .xlsx files. More details in the Page.\n\n# load the library\nlibrary(readxl)\n# The Excel file cannot be read directly from GitHub. You will need to download it to your local machine first\nfn_Pegeln &lt;- \"C:\\\\Lei\\\\HS_Web\\\\data_share/Pegeln_NRW.xlsx\""
  },
  {
    "objectID": "dataprocess/data_load.html#load-tabular-data-1",
    "href": "dataprocess/data_load.html#load-tabular-data-1",
    "title": "Data Loading",
    "section": "2.3 Load tabular data",
    "text": "2.3 Load tabular data\nSimilar to plain text files, metadata is often provided before the data body in Excel files. In Excel, each cell can be assigned a specific data type, while in R tables (data.frame or tibble), every column must have the same data type. This necessitates separate handling of metadata and data body to ensure that the correct data types are maintained.\nUnlike plain text files where we can only select lines to load, Excel allows us to define coordinates to access a specific celles-box wherever they are located.\n\n2.3.1 First try without any setting\n\n# try without setting\ntb_Pegeln &lt;- read_excel(fn_Pegeln)\ntb_Pegeln\n\n# A tibble: 277 × 16\n   Suchergebnisse Pegel.…¹ ...2  ...3  ...4  ...5  ...6  ...7  ...8  ...9  ...10\n   &lt;chr&gt;                   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n 1 \"Suchkriterien:\\n -- \\… &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; \n 2  &lt;NA&gt;                   &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; \n 3  &lt;NA&gt;                   &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; \n 4 \"Name\"                  Pege… Gewä… Betr… Pege… Einz… Q von Q bis NQ    MNQ  \n 5 \"Ahlen\"                 3211… Werse LANU… 73,47 46,62 1975  2013  0     0,07 \n 6 \"Ahmsen\"                4639… Werre LANU… 64,28 593   1963  2022  1,21  2,22 \n 7 \"Ahrhütte-Neuhof\"       2718… Ahr   LANU… 340,… 124   1986  2011  0,22  0,36 \n 8 \"Albersloh\"             3259… Werse LANU… 48,68 321,… 1973  2020  0,12  0,24 \n 9 \"Altena\"                2766… Lenne LANU… 154,… 1.190 1950  2021  1,36  6,48 \n10 \"Altena_Rahmedestraße\"  2766… Rahm… LANU… 157,… 29,6  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; \n# ℹ 267 more rows\n# ℹ abbreviated name: ¹​`Suchergebnisse Pegel.xlsx 14.09.2023 10:01`\n# ℹ 6 more variables: ...11 &lt;chr&gt;, ...12 &lt;chr&gt;, ...13 &lt;chr&gt;, ...14 &lt;chr&gt;,\n#   ...15 &lt;chr&gt;, ...16 &lt;chr&gt;\n\n\nWhen we provide only the file name to the function, we will always retrieve all the content from the first sheet. However, due to the limitations in R tables, every column will be recognized as the same data type, typically character.\n\n\n2.3.2 Give a range\n\n# using the range argument\ntb_Pegeln_Range &lt;- read_excel(fn_Pegeln, range = \"Suchergebnisse Pegel!A5:P10\")\ntb_Pegeln_Range\n\n# A tibble: 5 × 16\n  Name            Pegelnummer   Gewässername Betreiber  `Pegelnullpunkt [müNHN]`\n  &lt;chr&gt;           &lt;chr&gt;         &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;                   \n1 Ahlen           3211000000300 Werse        LANUV, NRW 73,47                   \n2 Ahmsen          4639000000100 Werre        LANUV, NRW 64,28                   \n3 Ahrhütte-Neuhof 2718193000100 Ahr          LANUV, NRW 340,58                  \n4 Albersloh       3259000000100 Werse        LANUV, NRW 48,68                   \n5 Altena          2766930000100 Lenne        LANUV, NRW 154,22                  \n# ℹ 11 more variables: `Einzugsgebiet [km²]` &lt;chr&gt;, `Q von` &lt;chr&gt;,\n#   `Q bis` &lt;chr&gt;, NQ &lt;chr&gt;, MNQ &lt;chr&gt;, MQ &lt;chr&gt;, MHQ &lt;chr&gt;, HQ &lt;chr&gt;,\n#   `Q Einheit` &lt;chr&gt;, `Ostwert in UTM` &lt;chr&gt;, `Nordwert in UTM` &lt;chr&gt;\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe data type of “Pegelnullpunkt [müNHN]” appears to be incorrect due to improper settings in Excel."
  },
  {
    "objectID": "dataprocess/data_load.html#data-type",
    "href": "dataprocess/data_load.html#data-type",
    "title": "Data Loading",
    "section": "2.4 Data type",
    "text": "2.4 Data type\nCompared to plain text files, Excel data already contains data type information for each cell. Therefore, the data type will be directly determined by the data type specified in Excel.\nHowever, there are instances where the data type in Excel is not correctly set, so manual data type conversion may be necessary. For more details, refer to Section 1.5."
  },
  {
    "objectID": "dataprocess/index.html",
    "href": "dataprocess/index.html",
    "title": "Data Processing",
    "section": "",
    "text": "Data processing is a fundamental step in transforming raw data into valuable insights. In the domain of hydrology, data often involves spatial and temporal aspects, making it complex and voluminous. Effective processing tools and methods are essential to successfully analyze and utilize this data. This page provides valuable technological skills for (pre)processing raw data."
  },
  {
    "objectID": "dataprocess/spatial_data.html",
    "href": "dataprocess/spatial_data.html",
    "title": "Basic Manipulation",
    "section": "",
    "text": "RPython\n\n\nThe terra package in R is a powerful and versatile package for working with geospatial data, including vector and raster data. It provides a wide range of functionality for reading, processing, analyzing, and visualizing spatial data.\nFor more in-depth information and resources on the terra package and spatial data science in R, you can explore the original website Spatial Data Science.\nFirstly load the library to the R space:\n\n# load the library\nlibrary(terra)\nlibrary(tidyverse)\n\n\n\nThe libraries for spatial data in Python are divided into several libraries, unlike the comprehensive ‘terra’ library in R. For vector data, you can use the ‘geopandas’ library, and for raster data, ‘rasterio’ is a good choice, among others.\n\nimport os\nimport pandas as pd\nimport numpy as np\n# Vector\nimport geopandas as gpd\nfrom shapely.geometry import Point, LineString, Polygon, shape\nimport fiona\n\n# Rster\nimport rasterio\nfrom rasterio.plot import show as rast_plot\nfrom rasterio.crs import CRS\nfrom rasterio.warp import calculate_default_transform, reproject, Resampling\nimport rasterio.features\nfrom rasterio.enums import Resampling\n\n# Plot\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "dataprocess/spatial_data.html#vector",
    "href": "dataprocess/spatial_data.html#vector",
    "title": "Basic Manipulation",
    "section": "2.1 Vector",
    "text": "2.1 Vector\nAs introduced in the section, spatial vector data typically consists of three main components:\n\nGeometry: Describes the spatial location and shape of features.\nAttributes: Non-spatial properties associated with features.\nCRS (Coordinate Reference System): Defines the spatial reference framework.\n\n\nRPython\n\n\n\n# Define the coordinate reference system (CRS) with EPSG codes\ncrs_31468 &lt;- \"EPSG:31468\"\n\n# Define coordinates for the first polygon\nx_polygon_1 &lt;- c(4484566, 4483922, 4483002, 4481929, 4481222, 4482500, 4483000, 4484666, 4484233)\ny_polygon_1 &lt;- c(5554566, 5554001, 5553233, 5554933, 5550666, 5551555, 5550100, 5551711, 5552767)\ngeometry_polygon_1 &lt;- cbind(id=1, part=1, x_polygon_1, y_polygon_1)\n# Define coordinates for the second polygon\nx_polygon_2 &lt;- c(4481929, 4481222, 4480500)\ny_polygon_2 &lt;- c(5554933, 5550666, 5552555)\ngeometry_polygon_2 &lt;- cbind(id=2, part=1, x_polygon_2, y_polygon_2)\n# Combine the two polygons into one data frame\ngeometry_polygon &lt;- rbind(geometry_polygon_1, geometry_polygon_2)\n\n# Create a vector layer for the polygons, specifying their type, attributes, CRS, and additional attributes\nvect_Test &lt;- vect(geometry_polygon, type=\"polygons\", \n                  atts = data.frame(ID_region = 1:2, Name = c(\"a\", \"b\")), \n                  crs = crs_31468)\nvect_Test$region_area &lt;- expanse(vect_Test)\n\n# Visualize the created polygons\nplot(vect_Test)\n\n\n\n\n\n\n\n# Define the coordinate reference system (CRS) with EPSG codes\ncrs_31468 = \"EPSG:31468\"\n\n# Define coordinates for the first polygon\nx_polygon_1 = [4484566, 4483922, 4483002, 4481929, 4481222, 4482500, 4483000, 4484666, 4484233]\ny_polygon_1 = [5554566, 5554001, 5553233, 5554933, 5550666, 5551555, 5550100, 5551711, 5552767]\n# Create a list of coordinate pairs for the first polygon\ngeometry_polygon_1 = Polygon([(x, y) for x, y in zip(x_polygon_1, y_polygon_1)])\n\n# Define coordinates for the second polygon\nx_polygon_2 = [4481929, 4481222, 4480500]\ny_polygon_2 = [5554933, 5550666, 5552555]\n# Create a list of coordinate pairs for the second polygon\ngeometry_polygon_2 = Polygon([(x, y) for x, y in zip(x_polygon_2, y_polygon_2)])\n\n# Construct Shapely polygons using the lists of coordinates\ngeometry_polygon = [geometry_polygon_1, geometry_polygon_2]\n\n\n# Create a GeoDataFrame with the polygons, specifying their attributes, CRS, and additional attributes\nvect_Test = gpd.GeoDataFrame({\n    'ID_region': [1, 2],\n    'Name': ['a', 'b'],\n    'geometry': geometry_polygon,\n}, crs=crs_31468)\n\n# Calculate the region area and add it as a new column\nvect_Test['region_area'] = vect_Test.area\n\n# Visualize the created polygons\nvect_Test.plot()\nplt.show()\n\n\n\nplt.close()"
  },
  {
    "objectID": "dataprocess/spatial_data.html#raster",
    "href": "dataprocess/spatial_data.html#raster",
    "title": "Basic Manipulation",
    "section": "2.2 Raster",
    "text": "2.2 Raster\nFor raster data, the geometry is relatively simple and can be defined by the following components:\n\nCoordinate of Original Point (X0, Y0) plus Resolutions (X and Y)\nBoundaries (Xmin, Xmax, Ymin, Ymax) plus Number of Rows and Columns\n\nOne of the most critical aspects of raster data is the values stored within its cells. You can set or modify these values using the values()&lt;- function in R.\n\nRPython\n\n\n\nrast_Test &lt;- rast(ncol=10, nrow=10, xmin=-150, xmax=-80, ymin=20, ymax=60)\nvalues(rast_Test) &lt;- runif(ncell(rast_Test))\n\nplot(rast_Test)\n\n\n\n\n\n\n\nfn_Rster_Test = \"C:\\\\Lei\\\\HS_Web\\\\data_share/raster_Py.tif\"\n\n# Create a new raster with the specified dimensions and extent\nncol, nrow = 10, 10\nxmin, xmax, ymin, ymax = -150, -80, 20, 60\n\n# Create the empty raster with random values\nwith rasterio.open(\n    fn_Rster_Test,\n    \"w\",\n    driver=\"GTiff\",\n    dtype=np.float32,\n    count=1,\n    width=ncol,\n    height=nrow,\n    transform=rasterio.transform.from_origin(xmin, ymax, (xmax - xmin) / ncol, (ymax - ymin) / nrow),\n    crs=\"EPSG:4326\"\n) as dst:\n    # Generate random values and assign them to the raster\n    random_values = np.random.rand(nrow, ncol).astype(np.float32)\n    dst.write(random_values, 1)  # Write the values to band 1\n\n# Now you have an empty raster with random values, and you can read and manipulate it as needed\nwith rasterio.open(fn_Rster_Test) as src:\n    rast_Test = src.read(1)\n\n\nrast_plot(rast_Test)\n\n\n\n\n\n\n\nCertainly, you can directly create a data file like an ASC (ASCII) file for raster data."
  },
  {
    "objectID": "dataprocess/spatial_data.html#assigning-a-crs",
    "href": "dataprocess/spatial_data.html#assigning-a-crs",
    "title": "Basic Manipulation",
    "section": "4.1 Assigning a CRS",
    "text": "4.1 Assigning a CRS\nIn cases where the Coordinate Reference System (CRS) information is not included in the data file’s content, you can assign it manually using the crs() function. This situation often occurs when working with raster data in formats like ASC (Arc/Info ASCII Grid) or other file formats that may not store CRS information.\n\nRPython\n\n\n\ncrs(rast_Test) &lt;- \"EPSG:31468\"\nrast_Test\n\nclass       : SpatRaster \ndimensions  : 5, 5, 1  (nrow, ncol, nlyr)\nresolution  : 1000, 1000  (x, y)\nextent      : 4480000, 4485000, 5550000, 5555000  (xmin, xmax, ymin, ymax)\ncoord. ref. : DHDN / 3-degree Gauss-Kruger zone 4 (EPSG:31468) \nsource      : minibeispiel_raster.asc \nname        : minibeispiel_raster \n\n\n\n\n\nrast_Test = rasterio.open(\"https://raw.githubusercontent.com/HydroSimul/Web/main/data_share/minibeispiel_raster.asc\", 'r+')\nrast_Test.crs = CRS.from_epsg(31468)\nprint(rast_Test.crs)\n\nEPSG:31468\n\n\n\n\n\nAs the results showed, the CRS information has been filled with the necessary details in line coord. ref..\nThe use of EPSG (European Petroleum Survey Group) codes is highly recommended for defining Coordinate Reference Systems (CRS) in spatial data. You can obtain information about EPSG codes from the EPSG website.\n\n\n\n\n\n\nNOTE\n\n\n\nYou should not use this approach to change the CRS of a data set from what it is to what you want it to be. Assigning a CRS is like labeling something."
  },
  {
    "objectID": "dataprocess/spatial_data.html#transforming-vector-data",
    "href": "dataprocess/spatial_data.html#transforming-vector-data",
    "title": "Basic Manipulation",
    "section": "4.2 Transforming vector data",
    "text": "4.2 Transforming vector data\nThe transformation of vector data is relatively simple, as it involves applying a mathematical formula to the coordinates of each point to obtain their new coordinates. This transformation can be considered as without loss of precision.\n\nRPython\n\n\nThe project() function can be utilized to reproject both vector and raster data.\n\n# New CRS\ncrs_New &lt;- \"EPSG:4326\"\n# Reproject\nvect_Test_New &lt;- project(vect_Test, crs_New)\n\n# Info of vector layer\nvect_Test_New\n\n class       : SpatVector \n geometry    : polygons \n dimensions  : 2, 3  (geometries, attributes)\n extent      : 11.72592, 11.78419, 50.08692, 50.13034  (xmin, xmax, ymin, ymax)\n coord. ref. : lon/lat WGS 84 (EPSG:4326) \n names       : ID_region  Name region_area\n type        :     &lt;int&gt; &lt;chr&gt;       &lt;num&gt;\n values      :         1     a   8.853e+06\n                       2     b   2.208e+06\n\n\n\n\n\ncrs_New = \"EPSG:4326\"\n\n# Reproject the vector layer to the new CRS\nvect_Test_New = vect_Test.to_crs(crs=crs_New)\n\n# Info of vector layer\nprint(vect_Test_New)\n\n   ID_region  ...                                           geometry\n0          1  ...  MULTIPOLYGON (((11.78268 50.12711, 11.77370 50...\n1          2  ...  MULTIPOLYGON (((11.74578 50.13033, 11.73611 50...\n\n[2 rows x 4 columns]"
  },
  {
    "objectID": "dataprocess/spatial_data.html#transforming-raster-data",
    "href": "dataprocess/spatial_data.html#transforming-raster-data",
    "title": "Basic Manipulation",
    "section": "4.3 Transforming raster data",
    "text": "4.3 Transforming raster data\nVector data can be transformed from lon/lat coordinates to planar and back without loss of precision. This is not the case with raster data. A raster consists of rectangular cells of the same size (in terms of the units of the CRS; their actual size may vary). It is not possible to transform cell by cell. For each new cell, values need to be estimated based on the values in the overlapping old cells. If the values are categorical data, the “nearest neighbor” method is commonly used. Otherwise some sort of interpolation is employed (e.g. “bilinear”). (From Spatial Data Science)\n\n\n\n\n\n\nNote\n\n\n\nBecause projection of rasters affects the cell values, in most cases you will want to avoid projecting raster data and rather project vector data.\n\n\n\n4.3.1 With CRS\nThe simplest approach is to provide a new CRS:\n\nRPython\n\n\n\n# New CRS\ncrs_New &lt;- \"EPSG:4326\"\n# Reproject\nrast_Test_New &lt;- project(rast_Test, crs_New, method = 'near')\n\n# Info and Plot of vector layer\nrast_Test_New\n\nclass       : SpatRaster \ndimensions  : 4, 6, 1  (nrow, ncol, nlyr)\nresolution  : 0.01176853, 0.01176853  (x, y)\nextent      : 11.7188, 11.78941, 50.08395, 50.13102  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource(s)   : memory\nname        : minibeispiel_raster \nmin value   :                   1 \nmax value   :                   3 \n\n\n\nplot(rast_Test)\nplot(rast_Test_New)\n\n\n\n\n\n\nOriginal\n\n\n\n\n\n\n\nNew\n\n\n\n\n\n\n\n\n\nfn_Rast_New = 'C:\\\\Lei\\\\HS_Web\\\\data_share/minibeispiel_raster.tif'\n# Define the new CRS\nnew_crs = {'init': 'EPSG:4326'}\ntransform, width, height = calculate_default_transform(\n        rast_Test.crs, new_crs, rast_Test.width, rast_Test.height, *rast_Test.bounds)\nkwargs = rast_Test.meta.copy()\nkwargs.update({\n        'crs': new_crs,\n        'transform': transform,\n        'width': width,\n        'height': height\n    })        \nrast_Test_New = rasterio.open(fn_Rast_New, 'w', **kwargs)        \nreproject(\n    source=rasterio.band(rast_Test, 1),\n    destination=rasterio.band(rast_Test_New, 1),\n    #src_transform=rast_Test.transform,\n    src_crs=rast_Test.crs,\n    #dst_transform=transform,\n    dst_crs=new_crs,\n    resampling=Resampling.nearest)\n\n(Band(ds=&lt;open BufferedDatasetWriter name='C:/Lei/HS_Web/data_share/minibeispiel_raster.tif' mode='w'&gt;, bidx=1, dtype='float32', shape=(4, 6)), None)\n\nrast_Test_New.close()        \n\nrast_Test = rasterio.open(\"https://raw.githubusercontent.com/HydroSimul/Web/main/data_share/minibeispiel_raster.asc\")\nrast_plot(rast_Test)\n\n\n\nrast_Test_New = rasterio.open(fn_Rast_New)\nrast_plot(rast_Test_New)\n\n\n\n\n\n\n\n\n\n4.3.2 With Mask Raster\nA second way is provide an existing SpatRaster with the geometry you desire, with special boundary and resolution, this is a better way.\n\n# New CRS\nrast_Mask &lt;- rast(ncol=10, nrow=10, xmin=265000, xmax=270000, ymin=5553000, ymax=5558000)\ncrs(rast_Mask) &lt;- \"EPSG:25833\"\nvalues(rast_Mask) &lt;- 1\n# Reproject\nrast_Test_New &lt;- project(rast_Test, rast_Mask)\n\n# Info and Plot of vector layer\nrast_Test_New\n\nclass       : SpatRaster \ndimensions  : 10, 10, 1  (nrow, ncol, nlyr)\nresolution  : 500, 500  (x, y)\nextent      : 265000, 270000, 5553000, 5558000  (xmin, xmax, ymin, ymax)\ncoord. ref. : ETRS89 / UTM zone 33N (EPSG:25833) \nsource(s)   : memory\nname        : minibeispiel_raster \nmin value   :                   1 \nmax value   :                   3 \n\n\n\nplot(rast_Test)\nplot(rast_Test_New)\n\n\n\n\n\n\nOriginal\n\n\n\n\n\n\n\nNew"
  },
  {
    "objectID": "dataprocess/spatial_data.html#attributes-manipulation",
    "href": "dataprocess/spatial_data.html#attributes-manipulation",
    "title": "Basic Manipulation",
    "section": "5.1 Attributes manipulation",
    "text": "5.1 Attributes manipulation\n\n5.1.1 Extract all Attributes\n\nas.data.frame()\n\n\ndf_Attr &lt;- as.data.frame(vect_Test)\ndf_Attr\n\n  ID_region Name region_area\n1         1    a     8853404\n2         2    b     2208109\n\n\n\n\n5.1.2 Extract one with attribute name\n\n$name\n[, \"name\"]\n\n\nvect_Test$ID_region\n\n[1] 1 2\n\nvect_Test[,\"ID_region\"]\n\n class       : SpatVector \n geometry    : polygons \n dimensions  : 2, 1  (geometries, attributes)\n extent      : 4480500, 4484666, 5550100, 5554933  (xmin, xmax, ymin, ymax)\n source      : minibeispiel_polygon.geojson\n coord. ref. : DHDN / 3-degree Gauss-Kruger zone 4 (EPSG:31468) \n names       : ID_region\n type        :     &lt;int&gt;\n values      :         1\n                       2\n\n\n\n\n5.1.3 Add a new attribute\n\n$name &lt;-\n[, \"name\"] &lt;-\n\n\nvect_Test$New_Attr &lt;- c(\"n1\", \"n2\")\nvect_Test[,\"New_Attr\"] &lt;- c(\"n1\", \"n2\")\n\n\n\n5.1.4 Merge several attributes\n\nsame order\n\ncbind()\n\ncommon (key-)attributes\n\nmerge()\n\n\n\ndf_New_Attr &lt;- data.frame(Name = c(\"a\", \"b\"), new_Attr2 = c(9, 6))\n\ncbind(vect_Test, df_New_Attr)\n\n class       : SpatVector \n geometry    : polygons \n dimensions  : 2, 6  (geometries, attributes)\n extent      : 4480500, 4484666, 5550100, 5554933  (xmin, xmax, ymin, ymax)\n source      : minibeispiel_polygon.geojson\n coord. ref. : DHDN / 3-degree Gauss-Kruger zone 4 (EPSG:31468) \n names       : ID_region  Name region_area New_Attr  Name new_Attr2\n type        :     &lt;int&gt; &lt;chr&gt;       &lt;num&gt;    &lt;chr&gt; &lt;chr&gt;     &lt;num&gt;\n values      :         1     a   8.853e+06       n1     a         9\n                       2     b   2.208e+06       n2     b         6\n\nmerge(vect_Test, df_New_Attr, by = \"Name\")\n\n class       : SpatVector \n geometry    : polygons \n dimensions  : 2, 5  (geometries, attributes)\n extent      : 4480500, 4484666, 5550100, 5554933  (xmin, xmax, ymin, ymax)\n coord. ref. : DHDN / 3-degree Gauss-Kruger zone 4 (EPSG:31468) \n names       :  Name ID_region region_area New_Attr new_Attr2\n type        : &lt;chr&gt;     &lt;int&gt;       &lt;num&gt;    &lt;chr&gt;     &lt;num&gt;\n values      :     a         1   8.853e+06       n1         9\n                   b         2   2.208e+06       n2         6\n\n\n\n\n5.1.5 Delete a attribute\n\n$name &lt;- NULL\n\n\nvect_Test$New_Attr &lt;- c(\"n1\", \"n2\")\nvect_Test[,\"New_Attr\"] &lt;- c(\"n1\", \"n2\")"
  },
  {
    "objectID": "dataprocess/spatial_data.html#object-append-and-aggregate",
    "href": "dataprocess/spatial_data.html#object-append-and-aggregate",
    "title": "Basic Manipulation",
    "section": "5.2 Object Append and aggregate",
    "text": "5.2 Object Append and aggregate\n\n5.2.1 Append new Objects\n\nRPython\n\n\n\nrbind()\n\n\n# New Vect\n# Define the coordinate reference system (CRS) with EPSG codes\ncrs_31468 &lt;- \"EPSG:31468\"\n\n# Define coordinates for the first polygon\nx_polygon_3 &lt;- c(4480400, 4481222, 4480500)\ny_polygon_3 &lt;- c(5551000, 5550666, 5552555)\ngeometry_polygon_3 &lt;- cbind(id=3, part=1, x_polygon_3, y_polygon_3)\n\n# Create a vector layer for the polygons, specifying their type, attributes, CRS, and additional attributes\nvect_New &lt;- vect(geometry_polygon_3, type=\"polygons\", atts = data.frame(ID_region = 3, Name = c(\"b\")), crs = crs_31468)\nvect_New$region_area &lt;- expanse(vect_New)\n\n# Append the objects\nvect_Append &lt;- rbind(vect_Test, vect_New)\nvect_Append\n\n class       : SpatVector \n geometry    : polygons \n dimensions  : 3, 4  (geometries, attributes)\n extent      : 4480400, 4484666, 5550100, 5554933  (xmin, xmax, ymin, ymax)\n source      : minibeispiel_polygon.geojson\n coord. ref. : DHDN / 3-degree Gauss-Kruger zone 4 (EPSG:31468) \n names       : ID_region  Name region_area New_Attr\n type        :     &lt;int&gt; &lt;chr&gt;       &lt;num&gt;    &lt;chr&gt;\n values      :         1     a   8.853e+06       n1\n                       2     b   2.208e+06       n2\n                       3     b   6.558e+05       NA\n\n\n\n\n\npandas.concat()\n\n\n# Define the coordinate reference system (CRS) with EPSG code\ncrs_31468 = \"EPSG:31468\"\n\n# Define coordinates for the new polygon\nx_polygon_3 = [4480400, 4481222, 4480500]\ny_polygon_3 = [5551000, 5550666, 5552555]\n\n# Create a Polygon geometry\ngeometry_polygon_3 = Polygon(zip(x_polygon_3, y_polygon_3))\n\n# Create a GeoDataFrame for the new polygon\nvect_New = gpd.GeoDataFrame({'ID_region': [3], 'Name': ['b'], 'geometry': [geometry_polygon_3]}, crs=crs_31468)\n\n# Calculate the region area\nvect_New['region_area'] = vect_New['geometry'].area\n\n# Append the new GeoDataFrame to the existing one\nvect_Append = gpd.GeoDataFrame(pd.concat([vect_Test, vect_New], ignore_index=True), crs=crs_31468)\n\n# Now, vect_Append contains the combined data\nprint(vect_Append)\n\n   ID_region  ...                                           geometry\n0          1  ...  MULTIPOLYGON (((4484566.000 5554566.000, 44839...\n1          2  ...  MULTIPOLYGON (((4481929.000 5554933.000, 44812...\n2          3  ...  POLYGON ((4480400.000 5551000.000, 4481222.000...\n\n[3 rows x 4 columns]\n\n\n\n\n\n\n\n5.2.2 Aggregate / Dissolve\nIt is common to aggregate (“dissolve”) polygons that have the same value for an attribute of interest.\n\nRPython\n\n\n\naggregate()\n\n\n# Aggregate by the \"Name\"\nvect_Aggregated &lt;- terra::aggregate(vect_Append, by = \"Name\")\nvect_Aggregated\n\n class       : SpatVector \n geometry    : polygons \n dimensions  : 2, 5  (geometries, attributes)\n extent      : 4480400, 4484666, 5550100, 5554933  (xmin, xmax, ymin, ymax)\n coord. ref. : DHDN / 3-degree Gauss-Kruger zone 4 (EPSG:31468) \n names       :  Name mean_ID_region mean_region_area New_Attr agg_n\n type        : &lt;chr&gt;          &lt;num&gt;            &lt;num&gt;    &lt;chr&gt; &lt;int&gt;\n values      :     a              1        8.853e+06       n1     1\n                   b            2.5        1.432e+06       NA     2\n\n\n\nplot(vect_Append, \"ID_region\")\nplot(vect_Aggregated, \"Name\")\n\n\n\n\n\n\nOriginal\n\n\n\n\n\n\n\nAggregated\n\n\n\n\n\n\n\n\n\n# Aggregate by the \"Name\"\nvect_Aggregated = vect_Append.dissolve(by=\"Name\", aggfunc=\"first\")\n\nprint(vect_Aggregated)\n\n                                               geometry  ...   region_area\nName                                                     ...              \na     POLYGON ((4484566.000 5554566.000, 4483922.000...  ...  8.853404e+06\nb     POLYGON ((4480400.000 5551000.000, 4480500.000...  ...  2.208109e+06\n\n[2 rows x 3 columns]\n\n\n\nvect_Test.plot()\nplt.show()\n\n\n\nplt.close()\nvect_Aggregated.plot()\nplt.show()\n\n\n\nplt.close()"
  },
  {
    "objectID": "dataprocess/spatial_data.html#overlap",
    "href": "dataprocess/spatial_data.html#overlap",
    "title": "Basic Manipulation",
    "section": "5.3 Overlap",
    "text": "5.3 Overlap\nTo perform operations that involve overlap between two vector datasets, we will create a new vector dataset:\n\nRPython\n\n\n\nvect_Overlap &lt;- as.polygons(rast_Test)[1,]\nnames(vect_Overlap) &lt;- \"ID_Rast\"\n\nplot(vect_Overlap, \"ID_Rast\")\n\n\n\n\n\n\n\n# Read the raster and get the shapes\nrast_Test = rasterio.open(\"https://raw.githubusercontent.com/HydroSimul/Web/main/data_share/minibeispiel_raster.asc\", 'r+')\nrast_Test.crs = CRS.from_epsg(31468)\ntransform = rast_Test.transform\nshapes = rasterio.features.shapes(rast_Test.read(1), transform=transform)\n\n# Convert the shapes to a GeoDataFrame\ngeometries = [shape(s) for s, v in shapes if v == 1]\nvect_Overlap = gpd.GeoDataFrame({'geometry': geometries})\n\n# Add an \"ID_Rast\" column to the GeoDataFrame\nvect_Overlap['ID_Rast'] = range(1, len(geometries) + 1)\nvect_Overlap.crs =\"EPSG:31468\"\n\n# Plot the polygons with \"ID_Rast\" as the attribute\nvect_Overlap.plot(column='ID_Rast')\nplt.show()\n\n\n\nplt.close()\n\n\n\n\n\n5.3.1 Erase\n\nRPython\n\n\n\nerase()\n\n\nvect_Erase &lt;- erase(vect_Test, vect_Overlap)\nplot(vect_Erase, \"ID_region\")\n\n\n\n\n\n\n\nvect_Erase = gpd.overlay(vect_Test, vect_Overlap, how='difference')\nvect_Erase.plot(column='ID_region', cmap='jet')\nplt.show()\n\n\n\nplt.close()\n\n\n\n\n\n\n5.3.2 Intersect\n\nRPython\n\n\n\nintersect()\n\n\nvect_Intersect &lt;- terra::intersect(vect_Test, vect_Overlap)\nplot(vect_Intersect, \"ID_region\")\n\n\n\n\n\n\n\nvect_Intersect = gpd.overlay(vect_Test, vect_Overlap, how='intersection')\nvect_Intersect.plot(column='ID_region', cmap='jet')\nplt.show()\n\n\n\nplt.close()\n\n\n\n\n\n\n5.3.3 Union\nAppends the geometries and attributes of the input.\n\nRPython\n\n\n\nunion()\n\n\nvect_Union &lt;- terra::union(vect_Test, vect_Overlap)\nplot(vect_Union, \"ID_region\")\n\n\n\n\n\n\n\nvect_Union = gpd.overlay(vect_Test, vect_Overlap, how='union')\nvect_Union.plot(column='ID_region', cmap='jet')\nplt.show()\n\n\n\nplt.close()\n\n\n\n\n\n\n5.3.4 Cover\ncover() is a combination of intersect() and union(). intersect returns new (intersected) geometries with the attributes of both input datasets. union appends the geometries and attributes of the input. cover returns the intersection and appends the other geometries and attributes of both datasets.\n\nRPython\n\n\n\ncover()\n\n\nvect_Cover &lt;- terra::cover(vect_Test, vect_Overlap)\nplot(vect_Cover, \"ID_region\")\n\n\n\n\n\n\n\nvect_Cover = gpd.overlay(vect_Test, vect_Overlap, how='identity')\nvect_Cover.plot(column='ID_region', cmap='jet')\nplt.show()\n\n\n\nplt.close()\n\n\n\n\n\n\n5.3.5 Difference\n\nRPython\n\n\n\nsymdif()\n\n\nvect_Difference &lt;- terra::symdif(vect_Test, vect_Overlap)\nplot(vect_Difference, \"ID_region\")\n\n\n\n\n\n\n\nvect_Difference = gpd.overlay(vect_Test, vect_Overlap, how='symmetric_difference')\nvect_Difference.plot(column='ID_region', cmap='jet')\nplt.show()\n\n\n\nplt.close()"
  },
  {
    "objectID": "dataprocess/spatial_data.html#raster-algebra",
    "href": "dataprocess/spatial_data.html#raster-algebra",
    "title": "Basic Manipulation",
    "section": "6.1 Raster algebra",
    "text": "6.1 Raster algebra\nMany generic functions that allow for simple and elegant raster algebra have been implemented for Raster objects, including the normal algebraic operators such as +, -, *, /, logical operators such as &gt;, &gt;=, &lt;, ==, !, and functions like abs, round, ceiling, floor, trunc, sqrt, log, log10, exp, cos, sin, atan, tan, max, min, range, prod, sum, any, all. In these functions, you can mix raster objects with numbers, as long as the first argument is a raster object. (Spatial Data Science)\n\nRPython\n\n\n\nrast_Add &lt;- rast_Test + 10\nplot(rast_Add)\n\n\n\n\n\n\n\nrast_Add = rast_Test_data + 10\nrast_plot(rast_Add)"
  },
  {
    "objectID": "dataprocess/spatial_data.html#replace-with-condition",
    "href": "dataprocess/spatial_data.html#replace-with-condition",
    "title": "Basic Manipulation",
    "section": "6.2 Replace with Condition",
    "text": "6.2 Replace with Condition\n\nRPython\n\n\n\nrast[condition] &lt;-\n\n\n# Copy to a new raster\nrast_Replace &lt;- rast_Test\n\n# Replace\nrast_Replace[rast_Replace &gt; 1] &lt;- 10\nplot(rast_Replace)\n\n\n\n\n\n\n\nrast_Replace = rast_Test_data\n\n# Replace values greater than 1 with 10\nrast_Replace[rast_Replace &gt; 1] = 10\nrast_plot(rast_Replace)"
  },
  {
    "objectID": "dataprocess/spatial_data.html#summary-of-multi-layers",
    "href": "dataprocess/spatial_data.html#summary-of-multi-layers",
    "title": "Basic Manipulation",
    "section": "6.3 Summary of multi-layers",
    "text": "6.3 Summary of multi-layers\n\nRPython\n\n\n\nrast_Mean &lt;- mean(rast_Test, rast_Replace)\nplot(rast_Mean)\n\n\n\n\n\n\n\nrast_Mean = (rast_Test_data + rast_Replace) / 2\nrast_plot(rast_Mean)"
  },
  {
    "objectID": "dataprocess/spatial_data.html#aggregate-and-disaggregate",
    "href": "dataprocess/spatial_data.html#aggregate-and-disaggregate",
    "title": "Basic Manipulation",
    "section": "6.4 Aggregate and disaggregate",
    "text": "6.4 Aggregate and disaggregate\n\nRPython\n\n\n\naggregate()\ndisagg()\n\n\n# Aggregate by factor 2\nrast_Aggregate &lt;- aggregate(rast_Test, 2)\nplot(rast_Aggregate)\n\n\n\n# Disaggregate by factor 2\nrast_Disagg &lt;- disagg(rast_Test, 2)\nrast_Disagg\n\nclass       : SpatRaster \ndimensions  : 10, 10, 1  (nrow, ncol, nlyr)\nresolution  : 500, 500  (x, y)\nextent      : 4480000, 4485000, 5550000, 5555000  (xmin, xmax, ymin, ymax)\ncoord. ref. : DHDN / 3-degree Gauss-Kruger zone 4 (EPSG:31468) \nsource(s)   : memory\nname        : minibeispiel_raster \nmin value   :                   1 \nmax value   :                   3 \n\nplot(rast_Disagg)\n\n\n\n\n\n\n\n# Aggregate by factor 2\nrast_Aggregate = rast_Test_data\nrast_Aggregate = rast_Aggregate[::2, ::2]\nrast_plot(rast_Aggregate)\n\n\n\n# Disaggregate by factor 2\nrast_Disagg = rast_Test_data\nrast_Disagg = np.repeat(np.repeat(rast_Disagg, 2, axis=0), 2, axis=1)\nrast_plot(rast_Disagg)"
  },
  {
    "objectID": "dataprocess/spatial_data.html#crop",
    "href": "dataprocess/spatial_data.html#crop",
    "title": "Basic Manipulation",
    "section": "6.5 Crop",
    "text": "6.5 Crop\nThe crop function lets you take a geographic subset of a larger raster object with an extent. But you can also use other spatial object, in them an extent can be extracted.\n\ncrop()\n\nwith extention\nwith rster\nwith vector\n\n\n\nrast_Crop &lt;- crop(rast_Test, vect_Test[1,])\nplot(rast_Crop)"
  },
  {
    "objectID": "dataprocess/spatial_data.html#trim",
    "href": "dataprocess/spatial_data.html#trim",
    "title": "Basic Manipulation",
    "section": "6.6 Trim",
    "text": "6.6 Trim\n\ntrim()\n\nTrim (shrink) a SpatRaster by removing outer rows and columns that are NA or another value.\n\nrast_Trim0 &lt;- rast_Test\nrast_Trim0[21:25] &lt;- NA\nrast_Trim &lt;- trim(rast_Trim0)\n\n\nplot(rast_Trim0)\nplot(rast_Trim)\n\n\n\n\n\n\nwith NA\n\n\n\n\n\n\n\nTrimed"
  },
  {
    "objectID": "dataprocess/spatial_data.html#mask",
    "href": "dataprocess/spatial_data.html#mask",
    "title": "Basic Manipulation",
    "section": "6.7 Mask",
    "text": "6.7 Mask\n\nRPython\n\n\n\nmask()\ncrop(mask = TRUE) = mask() + trim()\n\nWhen you use mask manipulation in spatial data analysis, it involves setting the cells that are not covered by a mask to NA (Not Available) values. If you apply the crop(mask = TRUE) operation, it means that not only will the cells outside of the mask be set to NA, but the resulting raster will also be cropped to match the extent of the mask.\n\nrast_Mask &lt;- mask(rast_Disagg, vect_Test[1,])\nrast_CropMask &lt;- crop(rast_Disagg, vect_Test[1,], mask = TRUE)\n\n\nplot(rast_Mask)\nplot(rast_CropMask)\n\n\n\n\n\n\nMask\n\n\n\n\n\n\n\nMask + Crop (Trim)\n\n\n\n\n\n\n\n\n\nvect_Mask = vect_Test.iloc[0:1].geometry.values[0]\n\n# Create a mask for the vect_Mask on the raster\nrast_Mask = rasterio.features.geometry_mask([vect_Mask], out_shape=rast_Test.shape, transform=rast_Test.transform, invert=True)\n# Apply the mask to the raster\nrast_Crop = rast_Test_data.copy()\nrast_Crop[~rast_Mask] = rast_Test.nodata  # Set values outside the geometry to nodata\n\nrast_plot(rast_Crop)"
  },
  {
    "objectID": "dataprocess/statistic_graphic.html",
    "href": "dataprocess/statistic_graphic.html",
    "title": "Graphical Statistic",
    "section": "",
    "text": "Graphical statistic is a branch of statistics that involves using visual representations to analyze and communicate data. It provides a powerful way to convey complex information in a more understandable and intuitive form.\n\n1 Example Data\nThe example files provided consist of three discharge time series for the Ruhr River in the Rhein basin, Germany. These data sets are sourced from open data available at ELWAS-WEB NRW. You can also access it directly from the internet via Github.\n\n# Library\nlibrary(xts)\nlibrary(tidyverse)\ntheme_set(theme_bw())\nlibrary(plotly)\n\n# File name\nfn_Bachum &lt;- \"https://raw.githubusercontent.com/HydroSimul/Web/main/data_share/Bachum_2763190000100.csv\"\nfn_Oeventrop &lt;- \"https://raw.githubusercontent.com/HydroSimul/Web/main/data_share/Oeventrop_2761759000100.csv\"\nfn_Villigst &lt;- \"https://raw.githubusercontent.com/HydroSimul/Web/main/data_share/Villigst_2765590000100.csv\"\n\n# Load Data\ndf_Bachum &lt;- read_csv2(fn_Bachum, skip = 10, col_names = FALSE)\ndf_Oeventrop &lt;- read_csv2(fn_Oeventrop, skip = 10, col_names = FALSE)\ndf_Villigst &lt;- read_csv2(fn_Villigst, skip = 10, col_names = FALSE)\n\n# Convert Date column to a Date type\ndf_Bachum$X1 &lt;- as_date(df_Bachum$X1, format = \"%d.%m.%Y\")\ndf_Oeventrop$X1 &lt;- as_date(df_Oeventrop$X1, format = \"%d.%m.%Y\")\ndf_Villigst$X1 &lt;- as_date(df_Villigst$X1, format = \"%d.%m.%Y\")\n\n# Create an xts object\nxts_Bachum &lt;- as.xts(df_Bachum)\nxts_Oeventrop &lt;- as.xts(df_Oeventrop)\nxts_Villigst &lt;- as.xts(df_Villigst)\n\n# Merge into one data frame\nxts_Rhur &lt;- merge(xts_Bachum, xts_Oeventrop, xts_Villigst)\nnames(xts_Rhur) &lt;- c(\"Bachum\", \"Oeventrop\", \"Villigst\")\nxts_Rhur &lt;- xts_Rhur[seq(as_date(\"1991-01-01\"), as_date(\"2020-12-31\"), \"days\"), ]\n\n# Deal with negative\ndf_Ruhr &lt;- coredata(xts_Rhur)\ndf_Ruhr[df_Ruhr &lt; 0] &lt;- NA\n\n# Summary in month\nxts_Ruhr_Clean &lt;- xts(df_Ruhr, index(xts_Rhur))\ndf_Ruhr_Month &lt;- apply.monthly(xts_Ruhr_Clean, mean)\n\nIn this article, we will leverage the power of the ggplot2 library to create plots and visualizations. To achieve this, the first step is to reformat the dataframe to a structure suitable for plotting.\n\ngdf_Ruhr &lt;- reshape2::melt(data.frame(date=index(df_Ruhr_Month), df_Ruhr_Month), \"date\")\n\n\n\n2 Timeserise line\nThe time series lines will provide us with discharge from 1991-01-01 to 2020-12-31 of the three gauges.\n\ngeom_line()\n\n\ngg_TS_Ruhr &lt;- ggplot(gdf_Ruhr) +\n  geom_line(aes(date, value, color = variable)) +\n  labs(x = \"Date\", y = \"Discharge [m^3/s]\", color = \"Gauge\")\n\nggplotly(gg_TS_Ruhr)\n\n\n\n\n\n\n\n3 Frequency Plots/Histogram\nHistograms and frequency plots are graphical representations of data distribution.\nHistograms display the counts (or frequency) with bars; frequency plots display the counts (or frequency) with lines.\nThe frequency plot represents the relative density of the data points by the relative height of the bars, while in a histogram, the area within the bar represents the relative density of the data points.\n\ngeom_histogram()\n\n\ngg_Hist_Ruhr &lt;- ggplot(gdf_Ruhr) +\n  geom_histogram(aes(value, group = variable, fill = variable, color = variable), position = \"dodge\", alpha = .5) +\n  labs(y = \"Count\", x = \"Discharge [m^3/s]\", color = \"Gauge\", fill = \"Gauge\")\n\nggplotly(gg_Hist_Ruhr)\n\n\n\n\n\n\ngeom_freqpoly()\n\n\ngg_Freq_Ruhr &lt;- ggplot(gdf_Ruhr) +\n  geom_freqpoly(aes(value, y = after_stat(count / sum(count)), group = variable, fill = variable, color = variable)) +\n  labs(y = \"Frequency\", x = \"Discharge [m^3/s]\", color = \"Gauge\")\n\nggplotly(gg_Freq_Ruhr)\n\n\n\n\n\n\n\n4 Box and Whisker Plot\nA Box and Whisker Plot, also known as a box plot, is a graphical representation of the distribution of a dataset. It provides a concise summary of the dataset’s key statistical measures and helps you visualize the spread and skewness of the data (Machiwal and Jha 2012). Here’s how a typical box and whisker plot is structured:\n\nBox: The box in the middle of the plot represents the interquartile range (IQR), which contains the middle 50% of the data. The bottom edge of the box represents the 25th percentile (Q1), and the top edge represents the 75th percentile (Q3).\nWhiskers: The whiskers extend from the box and represent the range of the data, excluding outliers. They typically extend to a certain multiple of the IQR beyond the quartiles. Outliers beyond the whiskers are often plotted as individual points.\nMedian (line inside the box): A horizontal line inside the box represents the median (Q2), which is the middle value of the dataset when it’s sorted.\n\n\n\n\nFigure from Internet\n\n\n\ngg_Box_Ruhr &lt;- ggplot(gdf_Ruhr) +\n  geom_boxplot(aes(variable, value, fill = variable, color = variable), alpha = .5) +\n  labs(x = \"Gauge\", y = \"Discharge [m^3/s]\", color = \"Gauge\") +\n  theme(legend.position = \"none\")\n\nggplotly(gg_Box_Ruhr)\n\n\n\n\n\n\n\n5 Quantile Plot\nA ‘quantile plot’ can be used to evaluate the quantile information such as the median, quartiles, and interquartile range of the data points (Machiwal and Jha 2012).\n\ngeom_qq()\n\n\ngg_QQ_Ruhr &lt;- ggplot(gdf_Ruhr, aes(sample = value, color = variable)) +\n  geom_qq(alpha = .5, distribution = stats::qunif) +\n  geom_qq_line(distribution = stats::qunif) +\n  labs(x = \"Fraction\", y = \"Discharge [m^3/s]\", color = \"Gauge\")\n\nggplotly(gg_QQ_Ruhr)\n\n\n\n\n\n\n\n\n\n\nReferences\n\nMachiwal, Deepesh, and Madan Kumar Jha. 2012. Hydrologic Time Series Analysis: Theory and Practice. Neu Dehli: Captial Publishing Company."
  },
  {
    "objectID": "dataprocess/timeserises_process.html",
    "href": "dataprocess/timeserises_process.html",
    "title": "Basic Processing",
    "section": "",
    "text": "In this article, we will cover fundamental techniques for manipulating and analyzing time series data. This includes tasks such as creating time series, summarizing data based on time indices, identifying trends, and more.\n\n1 Library\nTime series data structures are not standard in R, but the xts package is commonly used to work with time indices. However, it’s important to note that for processes that don’t rely on specific time indexing, the original data structure is sufficient. Time series structures are particularly useful when you need to perform time-based operations and analysis.\n\nlibrary(xts)\nlibrary(tidyverse)\n\n\n\n2 Example Files\nThe example files provided consist of three discharge time series for the Ruhr River in the Rhein basin, Germany. These data sets are sourced from open data available at ELWAS-WEB NRW. You can also access it directly from the internet via Github.\n\nfn_Bachum &lt;- \"https://raw.githubusercontent.com/HydroSimul/Web/main/data_share/Bachum_2763190000100.csv\"\nfn_Oeventrop &lt;- \"https://raw.githubusercontent.com/HydroSimul/Web/main/data_share/Oeventrop_2761759000100.csv\"\nfn_Villigst &lt;- \"https://raw.githubusercontent.com/HydroSimul/Web/main/data_share/Villigst_2765590000100.csv\"\n\n\n\n3 Create data\nBefore creating a time series structure, the data should be loaded into R. Time series in R can typically (only) support two-dimensional data structures, such as matrices and data frames.\nIf the date-time information is not correctly recognized during reading or if there is no time data present, you need to make sure that you have a valid time index.\nThere are two primary ways to create a time series in R:\n\nxts(): With this method, you explicitly specify the time index and create a time series object. This is useful when you have a matrix with an external time index.\nas.xts(): This method is more straightforward and is suitable when you have a data frame with a date column. The function will automatically recognize the date column and create a time series.\n\n\n# Read a CSV file as data.frame\ndf_Bachum &lt;- read_csv2(fn_Bachum, skip = 10, col_names = FALSE)\ndf_Villigst &lt;- read_csv2(fn_Villigst, skip = 10, col_names = FALSE)\n\n# Convert Date column to a Date type\ndf_Bachum$X1 &lt;- as_date(df_Bachum$X1, format = \"%d.%m.%Y\")\ndf_Villigst$X1 &lt;- as_date(df_Villigst$X1, format = \"%d.%m.%Y\")\n\n# Create an xts object\nxts_Bachum &lt;- xts(df_Bachum$X2, order.by = df_Bachum$X1)\nxts_Villigst &lt;- as.xts(df_Villigst)\n\n\n\n4 Merging Several Time Series\nIn R, the time index is consistent and follows a standardized format. This consistency in time indexing makes it easy to combine multiple time series into a single dataset based on their time index.\n\nmerge()\n\n\nxts_Rhur &lt;- merge(xts_Bachum, xts_Villigst)\nnames(xts_Rhur) &lt;- c(\"Bachum\", \"Villigst\")\n\nIt’s worth noting that when working with time series data in R, the length of the time series doesn’t necessarily have to be the same for all time series. This flexibility allows you to work with data that may have missing or varying data points over time, which is common in many real-world scenarios.\n\nlength(xts_Bachum)\n\n[1] 12053\n\nlength(xts_Villigst)\n\n[1] 11499\n\n\n\n\n5 Subsetting (Index with time)\nYou can work with time series data in R using both integer indexing, and time-based indexing using time intervals.\n\n# Create a time sequence\nts_Inteval &lt;- seq(as_date(\"1996-01-01\"), as_date(\"1996-12-31\"), \"days\")\n\n# Subset\nxts_Inteval &lt;- xts_Rhur[ts_Inteval, ]\nhead(xts_Inteval, 10)\n\n           Bachum Villigst\n1996-01-01 13.459    11.03\n1996-01-02 12.331    10.03\n1996-01-03 11.112     9.12\n1996-01-04 11.272     8.11\n1996-01-05 11.412     8.71\n1996-01-06 11.526     8.29\n1996-01-07 12.589     9.45\n1996-01-08 12.508    10.09\n1996-01-09 12.336     9.42\n1996-01-10 12.510     8.47\n\n\n\n\n6 Rolling Windows\nMoving averages are a valuable tool for smoothing time series data and uncovering underlying trends or patterns. With rolling windows, you can calculate not only the mean value but also other statistics like the median and sum. To expand the range of functions available, you can utilize the rollapply(). This enables you to apply a wide variety of functions to your time series data within specified rolling windows.\n\nrollmean()\nrollmedian()\nrollsum()\nrollmax()\n\n\nxts_RollMean &lt;- rollmean(xts_Inteval, 7)\nhead(xts_RollMean, 10)\n\n             Bachum Villigst\n1996-01-04 11.95729 9.248571\n1996-01-05 11.82143 9.114286\n1996-01-06 11.82214 9.027143\n1996-01-07 12.02186 8.934286\n1996-01-08 12.23314 9.242857\n1996-01-09 12.37214 9.238571\n1996-01-10 12.61357 9.541429\n1996-01-11 12.62643 9.535714\n1996-01-12 12.56257 9.357143\n1996-01-13 12.50186 9.242857\n\n\n\n\n7 Summary in Calendar Period\nDealing with irregularly spaced time series data can be challenging. One fundamental operation in time series analysis is applying a function by calendar period. This process helps in summarizing and analyzing time series data more effectively, even when the data points are irregularly spaced in time.\n\napply.daily()\napply.weekly()\napply.monthly()\napply.quarterly()\napply.yearly()\n\n\nxts_Month &lt;- apply.monthly(xts_Inteval, mean)\nxts_Month\n\n              Bachum  Villigst\n1996-01-31 12.478387  9.348065\n1996-02-29 15.794241 17.403448\n1996-03-31 14.244613 13.252903\n1996-04-30 10.217533  7.310667\n1996-05-31  9.331129  7.094839\n1996-06-30 10.589067  6.700667\n1996-07-31 11.607968  8.248710\n1996-08-31 12.897806  9.410968\n1996-09-30 14.516733 12.750000\n1996-10-31 18.214161 17.702903\n1996-11-30 30.673967 35.472667\n1996-12-31 35.720290 39.940645"
  },
  {
    "objectID": "dataset/hydro.html",
    "href": "dataset/hydro.html",
    "title": "Hydro",
    "section": "",
    "text": "About this site test1\n\n1 + 1\n\n[1] 2\n\n\n\n\nAbout this site test2\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "dataset/hydro.html#h2",
    "href": "dataset/hydro.html#h2",
    "title": "Hydro",
    "section": "",
    "text": "About this site test2\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "dataset/meteo.html",
    "href": "dataset/meteo.html",
    "title": "Meteological",
    "section": "",
    "text": "About this site test1\n\n1 + 1\n\n[1] 2\n\n\n\n\nAbout this site test2\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "dataset/meteo.html#h2",
    "href": "dataset/meteo.html#h2",
    "title": "Meteological",
    "section": "",
    "text": "About this site test2\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "modelling/basic_process.html",
    "href": "modelling/basic_process.html",
    "title": "Concept of Modelling",
    "section": "",
    "text": "Within the process of hydrological modeling, there are fundamental concepts that play a crucial role. In this article, we will elucidate these concepts through illustrative figures, enhancing your comprehension. These concepts can be categorized into two main parts: the Data part, which encompasses aspects related to input data, parameters, and parameter ranges, and the Process part, which outlines the workflow of the entire modeling process. By exploring these concepts and their visual representations, you’ll gain a deeper understanding of hydrological modeling."
  },
  {
    "objectID": "modelling/basic_process.html#forcing-data-boundary-condition-data",
    "href": "modelling/basic_process.html#forcing-data-boundary-condition-data",
    "title": "Concept of Modelling",
    "section": "1.1 Forcing Data / Boundary Condition Data",
    "text": "1.1 Forcing Data / Boundary Condition Data\nFor hydrological modeling, you need data that describes the boundary conditions or forcing factors affecting the model. This includes information on precipitation, temperature, humidity, and other meteorological variables."
  },
  {
    "objectID": "modelling/basic_process.html#initial-condition-data-warm-up-time",
    "href": "modelling/basic_process.html#initial-condition-data-warm-up-time",
    "title": "Concept of Modelling",
    "section": "1.2 Initial Condition Data & Warm-up Time",
    "text": "1.2 Initial Condition Data & Warm-up Time\nInitial conditions represent the state of the watershed or catchment at the beginning of the simulation. Warm-up time refers to the period during which the model is run to reach a stable state before starting the actual simulation."
  },
  {
    "objectID": "modelling/basic_process.html#parameter",
    "href": "modelling/basic_process.html#parameter",
    "title": "Concept of Modelling",
    "section": "1.3 Parameter",
    "text": "1.3 Parameter\nParameters are essential components of hydrological models. There are three main model categories:\n\nPhysical-based models (white box): These models simulate processes with detailed data and deterministic formulas, ideally requiring no parameters.\nConceptual models (gray box): These models, which are the most commonly used, rely on parameters that represent physical characteristics of the watershed. Calibration is typically needed.\nEmpirical and data-driven models (black box): These models rely heavily on observed data and may not have explicit physical representations.\n\n\n1.3.1 Initial Parameters\nInitial parameters represent the initial values of model parameters at the beginning of the simulation.\n\n\n1.3.2 Parameter Range\nParameter ranges define the possible values that model parameters can take within certain bounds.\n\n\n1.3.3 Calibrated Parameters\nCalibrated parameters are fine-tuned to optimize model performance and make model predictions more accurate.\n\n\n1.3.4 Validated Parameters\nValidated parameters are parameters that have been verified through comparison with observed data to ensure that the model accurately represents the real-world system.\n\n\n1.3.5 Parameter Mapping with Groups\nParameters can be organized into groups based on their characteristics, simplifying the modeling process."
  },
  {
    "objectID": "modelling/basic_process.html#model-running",
    "href": "modelling/basic_process.html#model-running",
    "title": "Concept of Modelling",
    "section": "2.1 Model Running",
    "text": "2.1 Model Running\nThe core step in hydrological modeling involves running the model using the provided forcing data, initial conditions, and parameter values. This phase represents the simulation of the hydrological processes within the watershed or catchment."
  },
  {
    "objectID": "modelling/basic_process.html#evaluation",
    "href": "modelling/basic_process.html#evaluation",
    "title": "Concept of Modelling",
    "section": "2.2 Evaluation",
    "text": "2.2 Evaluation\nAfter the model run, an evaluation process is conducted to assess the performance of the model. This involves comparing the model’s simulated output to observed data or reference values. Various performance metrics and statistical measures are used to determine how well the model simulates real-world conditions."
  },
  {
    "objectID": "modelling/basic_process.html#calibration",
    "href": "modelling/basic_process.html#calibration",
    "title": "Concept of Modelling",
    "section": "2.3 Calibration",
    "text": "2.3 Calibration\nCalibration is a critical step in hydrological modeling. It involves adjusting the model’s parameters to improve its accuracy and alignment with observed data. Optimization techniques are often used to find parameter values that minimize the difference between model output and observed data."
  },
  {
    "objectID": "modelling/basic_process.html#validation",
    "href": "modelling/basic_process.html#validation",
    "title": "Concept of Modelling",
    "section": "2.4 Validation",
    "text": "2.4 Validation\nOnce the model has been calibrated, it is essential to validate its performance. Validation involves testing the calibrated model against independent datasets or data from a different time period. This step ensures that the model’s performance is not solely tailored to the calibration data but remains reliable for a broader range of conditions."
  },
  {
    "objectID": "modelling/test_shiny.html",
    "href": "modelling/test_shiny.html",
    "title": "Old Faithful",
    "section": "",
    "text": "Number of bins:"
  },
  {
    "objectID": "modelling/model_linearReservoir.html",
    "href": "modelling/model_linearReservoir.html",
    "title": "Linear Reservoir",
    "section": "",
    "text": "Linear Reservoir is a method and just assuming that the watershed behaves like a linear reservoir, where the outflow is proportional to the water storage within the reservoir.\n\\[\nQ_{out} = \\frac{1}{K}S(t)\n\\tag{1}\\]\nIn addition to their relationship with output and storage, linear reservoir models also adhere to the continuity equation, often referred to as the water balance equation.\n\\[\n\\frac{\\mathrm{d}S(t)}{\\mathrm{d}t} = Q_{in} - Q_{out}\n\\tag{2}\\]\nBy combining both equations, we obtain a differential equation (DGL).\n\\[\nQ_{in} = Q_{out} + K\\frac{\\mathrm{d}Q_{out}(t)}{\\mathrm{d}t}\n\\tag{3}\\]\n\\[\nQ_{out}(t)=\\int_{\\tau=t0}^{t}Q_{in}(\\tau)\\frac{1}{K}e^{-\\frac{t-\\tau}{K}}\\mathrm{d}\\tau + Q_{out}(t_0)\\frac{1}{K}e^{-\\frac{t-t0}{K}}\n\\tag{4}\\]\nWhere:\n\n\\(Q_{in}\\) is the inflow of the reservoir\n\\(Q_{out}\\) is the outflow of the reservoir\n\\(S\\) is the storage of the reservoir\n\\(K\\) is the parameter that defines the relationship between \\(Q_{out}\\) and \\(S\\)"
  },
  {
    "objectID": "modelling/model_linearReservoir.html#analytical-solution",
    "href": "modelling/model_linearReservoir.html#analytical-solution",
    "title": "Linear Reservoir",
    "section": "2.1 Analytical Solution",
    "text": "2.1 Analytical Solution\nThe final form of the equation under the simplifying hypothesis of linear input looks like this:\n\\[\nQ_{out}(t_1) = Q_{out}(t_0) + (Q_{in}(t_1) - Q_{out}(t_0))\\cdot (1-e^{-\\frac{1}{K}}) + (Q_{in}(t_1) - Q_{in}(t_0))\\cdot [1-K(1-e^{-\\frac{1}{K}})]\n\\]\n\nlinear_reservoir_Ana &lt;- function(Q_In, Q_Out0 = 0, param_K = 1) {\n  n_Step &lt;- length(Q_In)\n  Q_Out &lt;- c(Q_Out0, rep(0, n_Step - 1))\n  \n  for (i in 2:n_Step) {\n    Q_Out[i] &lt;- Q_Out[i-1] + (Q_In[i] - Q_Out[i-1]) * (1 - exp(-1 / param_K)) + (Q_In[i] - Q_In[i-1]) * (1 - param_K * (1 - exp(-1 / param_K)))\n  }\n  \n  Q_Out\n  \n}"
  },
  {
    "objectID": "modelling/model_linearReservoir.html#numerical-solution",
    "href": "modelling/model_linearReservoir.html#numerical-solution",
    "title": "Linear Reservoir",
    "section": "2.2 Numerical Solution",
    "text": "2.2 Numerical Solution\nWhen we simplify the difficult continuous form into a discrete form using \\(\\Delta S / \\Delta t\\) to replace \\(\\mathrm{d}S/\\mathrm{d}t\\), we can obtain the numerical (discrete) format:\n\\[\nQ_{out}(t_1) = Q_{out}(t_0) + (Q_{in}(t_0) - Q_{out}(t_0)) \\frac{1}{K + 0.5} + (Q_{in}(t_1) - Q_{in}(t_0)) \\frac{0.5}{K + 0.5}\n\\]\n\nlinear_reservoir_Num &lt;- function(Q_In, Q_Out0 = 0, param_K = 1) {\n  n_Step &lt;- length(Q_In)\n  Q_Out &lt;- c(Q_Out0, rep(0, n_Step - 1))\n  \n  for (i in 2:n_Step) {\n    Q_Out[i] &lt;- Q_Out[i-1] + (Q_In[i-1] - Q_Out[i-1]) / (param_K + 0.5) + (Q_In[i] - Q_In[i-1]) * .5 / (param_K + 0.5)\n  }\n  \n  Q_Out\n  \n}"
  },
  {
    "objectID": "modelling/model_linearReservoir.html#compare-the-results-of-both-functions",
    "href": "modelling/model_linearReservoir.html#compare-the-results-of-both-functions",
    "title": "Linear Reservoir",
    "section": "2.3 Compare the results of both Functions",
    "text": "2.3 Compare the results of both Functions\n\nlibrary(tidyverse)\ntheme_set(theme_bw())\nlibrary(plotly)\nload(\"../data_share/color.Rdata\")\n\n\n\nCode\nnum_TestIn &lt;- c(rep(100, 100), 0:100, rep(0,99))\nnum_Out_Ana &lt;- linear_reservoir_Ana(num_TestIn, param_K = 60)\nnum_Out_Num &lt;- linear_reservoir_Num(num_TestIn, param_K = 60)\n\ngg_Test &lt;- ggplot() +\n  geom_line(aes(1:300, num_TestIn, color = \"Input\")) +\n  geom_line(aes(1:300, num_Out_Ana, color = \"Output\\n(Analytical)\")) +\n  geom_line(aes(1:300, num_Out_Num, color = \"Output\\n(Numerical)\")) +\n  scale_color_manual(values = c(\"cyan\", \"red\", \"orange\"))+\n  labs(x = \"Time [T]\", y = \"Water Flow [V/T]\", color = \"\") \n\nggplotly(gg_Test)\n\n\n\n\n\n\nIn the test forcing data, there are constant, linear, and null input scenarios. In all three situations, the analytical and numerical solutions yield almost the same results. Therefore, we can use either of them for subsequent analysis."
  },
  {
    "objectID": "modelling/model_linearReservoir.html#boundaray-condition-forcing",
    "href": "modelling/model_linearReservoir.html#boundaray-condition-forcing",
    "title": "Linear Reservoir",
    "section": "3.1 Boundaray Condition Forcing",
    "text": "3.1 Boundaray Condition Forcing\nFor the single linear reservoir, the boundary condition is the time series of the inflow \\(Q_{in}(t)\\) (Q_In).\nIn the one-variable experiment of the boundary condition, we will consider five boundary conditions, including three constants at 10, 50, and 100 [V/L], as well as an increasing (0 to 100 [V/L]) and decreasing (100 to 0 [V/L]) series.\nThe\n\n\nCode\nnum_BC10 &lt;- rep(c(10,0), each = 100)\nnum_BC50 &lt;- rep(c(50,0), each = 100)\nnum_BC100 &lt;- rep(c(100,0), each = 100)\nnum_BCin &lt;- c(0:100, rep(0,99))\nnum_BCde &lt;- c(100:0, rep(0,99))\n\nlst_BC_in &lt;- list(num_BC10, num_BC50, num_BC100, num_BCin, num_BCde)\ndf_BC_in &lt;- bind_cols(lst_BC_in) |&gt; as.data.frame()\nnames(df_BC_in) &lt;- c(\"BC10\", \"BC50\", \"BC100\", \"BCin\", \"BCde\")\ngdf_BC_in &lt;- reshape2::melt(df_BC_in)\ngdf_BC_in$time &lt;- 1:200\ngdf_BC_in$facet &lt;- \"Q_In\"\n\nlst_BC_out &lt;- map(lst_BC_in, linear_reservoir_Num, param_K = 60)\n\ndf_BC_out &lt;- bind_cols(lst_BC_out) |&gt; as.data.frame()\nnames(df_BC_out) &lt;- c(\"BC10\", \"BC50\", \"BC100\", \"BCin\", \"BCde\")\ngdf_BC_out &lt;- reshape2::melt(df_BC_out)\ngdf_BC_out$time &lt;- 1:200\ngdf_BC_out$facet &lt;- \"Q_Out\"\ngdf_BC &lt;- rbind(gdf_BC_in, gdf_BC_out)\ngg_BC &lt;- ggplot(gdf_BC) +\n  geom_line(aes(time, value, group = variable, color = variable)) +\n  scale_color_manual(values = color_TUD_diskrete)+\n  facet_grid(cols = vars(facet))+\n  scale_alpha_manual(values = c(.6,1)) +\n    labs(x = \"Time [T]\", y = \"Water Flow [V/T]\", color = \"Vari (BC):\") \nggplotly(gg_BC)\n\n\n\n\n\nFigure 1: The facet labeled Q_In displays five input time series (\\(Q_{in}\\)) scenarios. In scenarios BC10, BC50, and BC100, the input remains constant for the initial 100 timesteps. BCin and BCde represent scenarios where the input increases and decreases, respectively, during the first 100 timesteps. Notably, BC50, BCin, and BCde scenarios all have the same total volume of 5000 [V]. The facet labeled Q_Out presents the corresponding simulated results, showcasing the model’s responses to different boundary conditions (\\(Q_{in}\\))."
  },
  {
    "objectID": "modelling/model_linearReservoir.html#innitial-condition-forcing",
    "href": "modelling/model_linearReservoir.html#innitial-condition-forcing",
    "title": "Linear Reservoir",
    "section": "3.2 Innitial Condition Forcing",
    "text": "3.2 Innitial Condition Forcing\nNormally, the initial condition represents the initial state of state variables, such as the water content of the soil or the storage of the reservoir. However, in the case of a single linear reservoir, the storage of the reservoir is simplified as the variable \\(Q_{out}\\) (Q_Out). For this one-variable experiment, \\(Q_{out}\\) will vary from 10 to 90 [V/L].\n\n\nCode\nlst_IC_in &lt;- as.list(seq(10, 90, 10))\nlst_IC_out &lt;- map(lst_IC_in, linear_reservoir_Ana, Q_In = num_BC100, param_K = 60)\n\ndf_IC_out &lt;- bind_cols(lst_IC_out) |&gt; as.data.frame()\ngdf_IC_out &lt;- reshape2::melt(df_IC_out)\ngdf_IC_out$time &lt;- 1:200\ngdf_IC_out$variable &lt;- rep(seq(10, 90, 10), each = 200)\ngg_BC &lt;- ggplot(gdf_IC_out) +\n  geom_line(aes(time, value, group = variable, color = variable)) +\n  scale_color_gradientn(colours = color_DRESDEN)+\n    labs(x = \"Time [T]\", y = \"Water Flow [V/T]\", color = \"Vari\\n(IC):\") \nggplotly(gg_BC)\n\n\n\n\n\nFigure 2: The different initial condition \\(Q_{out}(t_0)\\) values result in distinct outflow time series. The line colors correspond to the values of the initial condition."
  },
  {
    "objectID": "modelling/model_linearReservoir.html#parameter",
    "href": "modelling/model_linearReservoir.html#parameter",
    "title": "Linear Reservoir",
    "section": "3.3 Parameter",
    "text": "3.3 Parameter\nIn the case of the single linear reservoir, there is only one parameter, denoted as \\(K\\) (param_K). The parameter \\(K\\) can vary widely due to differences in the scale of the simulation domain. It has physical units of time, which can be specified in units such as seconds, hours, or days depending on the scale of the hydrological model.\n\n\nCode\nlst_Param_in &lt;- as.list(seq(10, 90, 10))\nlst_Param_out &lt;- map(lst_Param_in, linear_reservoir_Ana, Q_In = num_BC100, Q_Out0 = 0)\n\ndf_Param_out &lt;- bind_cols(lst_Param_out) |&gt; as.data.frame()\ngdf_Param_out &lt;- reshape2::melt(df_Param_out)\ngdf_Param_out$time &lt;- 1:200\ngdf_Param_out$variable &lt;- rep(seq(10, 90, 10), each = 200)\ngg_BC &lt;- ggplot(gdf_Param_out) +\n  geom_line(aes(time, value, group = variable, color = variable)) +\n  scale_color_gradientn(colours = color_DRESDEN)+\n  labs(x = \"Time [T]\", y = \"Water Flow [V/T]\", color = \"Vari\\n(Param):\") \nggplotly(gg_BC)\n\n\n\n\n\nFigure 3: The different parameter \\(K\\) values result in distinct outflow time series. The line colors correspond to the values of the parameter."
  },
  {
    "objectID": "modelling/basic_concept.html",
    "href": "modelling/basic_concept.html",
    "title": "Concept of Modelling",
    "section": "",
    "text": "Within the process of hydrological modeling, there are fundamental concepts that play a crucial role. In this article, we will elucidate these concepts through illustrative figures, enhancing your comprehension. These concepts can be categorized into two main parts: the Data part, which encompasses aspects related to input data, parameters, and parameter ranges, and the Process part, which outlines the workflow of the entire modeling process. By exploring these concepts and their visual representations, you’ll gain a deeper understanding of hydrological modeling."
  },
  {
    "objectID": "modelling/basic_concept.html#forcing-data-boundary-condition-data",
    "href": "modelling/basic_concept.html#forcing-data-boundary-condition-data",
    "title": "Concept of Modelling",
    "section": "1.1 Forcing Data / Boundary Condition Data",
    "text": "1.1 Forcing Data / Boundary Condition Data\nFor hydrological modeling, you need data that describes the boundary conditions or forcing factors affecting the model. This includes information on precipitation, temperature, humidity, and other meteorological variables."
  },
  {
    "objectID": "modelling/basic_concept.html#initial-condition-data-warm-up-time",
    "href": "modelling/basic_concept.html#initial-condition-data-warm-up-time",
    "title": "Concept of Modelling",
    "section": "1.2 Initial Condition Data & Warm-up Time",
    "text": "1.2 Initial Condition Data & Warm-up Time\nInitial conditions represent the state of the watershed or catchment at the beginning of the simulation.\nHowever, in many cases, the exact initial conditions are unknown or difficult to measure accurately. To address this uncertainty, hydrological models incorporate a warm-up period. This warm-up period refers to the initial phase of model simulation where the model runs to establish a stable or equilibrium state before commencing the actual simulation.\nIt’s crucial to ensure that the warm-up period is of sufficient duration to reach a stable state. Typically, this period should span at least two complete cycles of the dominant hydrological processes within the watershed. This requirement ensures that the model has the opportunity to capture the full range of variability associated with these processes.\nThe periodicity of hydrological processes is a crucial consideration when implementing a warm-up period. By assuming that the initial state during warm-up has only a minimal influence, we rely on the repeated cycles of hydrological events. During the second cycle, the system tends to reach its maximum or minimum state, which is indicative of equilibrium. Subsequently, the simulation becomes stable, and this equilibrium state serves as the initial condition for the formal simulation."
  },
  {
    "objectID": "modelling/basic_concept.html#parameter",
    "href": "modelling/basic_concept.html#parameter",
    "title": "Concept of Modelling",
    "section": "1.3 Parameter",
    "text": "1.3 Parameter\nParameters are essential components of hydrological models for several reasons. Firstly, hydrological models cannot simulate the entire complexity of the real world, so parameters are used to represent various physical characteristics and processes. Secondly, real-world measurements are often limited in scope and may not capture all relevant data across the entire watershed. Parameters help bridge these gaps by allowing models to make predictions based on available information.\nThere are three main categories of hydrological models based on their use of parameters:\n\nPhysical-based models (white box): These models aim to simulate hydrological processes with a high level of detail. They rely on deterministic formulas and aim to represent physical processes as accurately as possible. Ideally, physical-based models do not require the use of parameters, as all processes are simulated with detailed data and deterministic equations. However, in practice, some level of parameterization may still be necessary, especially for processes that are not well understood or difficult to represent mathematically.\nConceptual models (gray box): Conceptual models are the most commonly used hydrological models. They strike a balance between complexity and simplicity. These models use parameters to represent various physical characteristics of the watershed, such as soil properties, land use, and drainage patterns. Calibration, a process of adjusting parameter values to match observed data, is typically needed to make the model’s predictions consistent with real-world conditions.\nEmpirical and data-driven models (black box): Empirical models rely heavily on observed data and may not explicitly represent physical processes. Instead, they use statistical relationships to make predictions. These models often require fewer parameters than physical-based or conceptual models but may still involve parameter estimation based on data.\n\n\n1.3.1 Initial Parameters\nInitial parameters are the starting values for model parameters that are often suggested based on previous research or prior knowledge. These initial parameter values are used to test the fundamental functionality of the model and its applicability to the study area. While these initial parameters may provide a reasonable starting point, they may or may not be a good fit for the specific study area and objectives.\n\n\n1.3.2 Parameter Range\nParameter ranges define the range of allowable values that model parameters can assume within specified bounds. While a broad parameter range can provide a greater opportunity to find optimal parameter values, it also expands the search space, making it more challenging to identify the best-fitting parameters.\nParameter ranges can be categorized into two main types:\n\nPhysical Range: This refers to the range of parameter values that are physically meaningful and are constrained by the fundamental characteristics of the system being modeled. For example, hydraulic conductivity in groundwater models cannot be negative, so it has a physical lower bound of zero. Physical range limits ensure that parameter values are consistent with the underlying physical processes.\nRegional Range: In addition to the physical limits, parameters may have regional variations based on the specific characteristics of the study area. These regional variations account for local geological, climatic, or land-use differences that influence parameter values. Regional ranges help to capture the heterogeneity within a larger study domain and allow for parameterization that reflects local conditions.\n\nBalancing the scope of parameter ranges is essential in hydrological modeling. While broader ranges offer flexibility, they also increase the complexity of parameter estimation. The challenge lies in finding a balance that allows for the exploration of diverse parameter values while ensuring that the model remains physically meaningful and regionally applicable.\n\n\n1.3.3 Calibrated Parameters\nCalibrated parameters are the parameter set that has been adjusted and fine-tuned during the calibration process. These parameters represent the current best-fit values for the hydrological model in a specific study area. They are chosen to optimize the model’s performance and ensure that it provides accurate predictions and simulations based on observed data.\n\n\n1.3.4 Validated Parameters\nValidated parameters are parameters that have been **verified* through comparison with observed data to ensure that the model accurately represents the real-world system.\n\n\n1.3.5 Parameter Mapping with Groups\nParameters in hydrological modeling vary based on the physical characteristics of the system being studied. However, in reality, different regions within a study area often exhibit distinct physical characteristics. Consequently, when performing simulations, it becomes necessary to calibrate parameters for each Hydrological Response Unit (HRU), which represents a homogeneous area within the study domain.\nCalibrating parameters for every HRU can be a formidable task, as it would involve a substantial number of individual calibrations. To address this challenge, researchers often employ a strategy of grouping HRUs with similar characteristics. By doing so, they can map parameters to these groups, effectively reducing the parameter calibration space.\nOne of the most commonly used grouping criteria includes categorizing HRUs based on factors such as land use, soil type, soil class, or climate zone. These characteristics often play a significant role in shaping the hydrological behavior of a region."
  },
  {
    "objectID": "modelling/basic_concept.html#model-running",
    "href": "modelling/basic_concept.html#model-running",
    "title": "Concept of Modelling",
    "section": "2.1 Model Running",
    "text": "2.1 Model Running\nThe core step in hydrological modeling involves running the model, which treats the model as a function that calculates a process with the input data to produce output data.\n\nThis process utilizes the provided boundary conditions, initial conditions, and parameter values to simulate the hydrological processes within the watershed or catchment. Boundary conditions and initial conditions are often collectively referred to as forcing data or input data."
  },
  {
    "objectID": "modelling/basic_concept.html#evaluation",
    "href": "modelling/basic_concept.html#evaluation",
    "title": "Concept of Modelling",
    "section": "2.2 Evaluation",
    "text": "2.2 Evaluation\nAfter the model run, an evaluation process is conducted to assess the performance of the model. This involves comparing the model’s simulated output to observed data or reference values. Various performance metrics and statistical measures are used to determine how well the model simulates real-world conditions."
  },
  {
    "objectID": "modelling/basic_concept.html#calibration",
    "href": "modelling/basic_concept.html#calibration",
    "title": "Concept of Modelling",
    "section": "2.4 Calibration",
    "text": "2.4 Calibration\nCalibration is a critical step in hydrological modeling. It involves adjusting the model’s parameters to improve its accuracy and alignment with observed data. Optimization techniques are often used to find parameter values that minimize the difference between model output and observed data."
  },
  {
    "objectID": "modelling/basic_concept.html#validation",
    "href": "modelling/basic_concept.html#validation",
    "title": "Concept of Modelling",
    "section": "2.5 Validation",
    "text": "2.5 Validation\nOnce the model has been calibrated, it is essential to validate its performance. Validation involves testing the calibrated model against independent datasets or data from a different time period. This step ensures that the model’s performance is not solely tailored to the calibration data but remains reliable for a broader range of conditions."
  },
  {
    "objectID": "modelling/basic_concept.html#boundary-condition-forcing-data",
    "href": "modelling/basic_concept.html#boundary-condition-forcing-data",
    "title": "Concept of Modelling",
    "section": "1.1 Boundary Condition (Forcing Data)",
    "text": "1.1 Boundary Condition (Forcing Data)\nFor hydrological modeling, you need data that describes the boundary conditions or forcing factors affecting the model. They define how water and other related variables enter or exit the modeled domain. This includes information on precipitation, temperature, humidity, and other meteorological variables."
  },
  {
    "objectID": "modelling/basic_concept.html#uncertainty-and-sensitivity",
    "href": "modelling/basic_concept.html#uncertainty-and-sensitivity",
    "title": "Concept of Modelling",
    "section": "2.3 Uncertainty and sensitivity",
    "text": "2.3 Uncertainty and sensitivity\nUncertainty and sensitivity analysis are crucial components of hydrological modeling, helping us understand the reliability of model predictions and the influence of different input parameters. Uncertainty analysis assesses the overall uncertainty in model outputs, considering various sources of uncertainty, while sensitivity analysis identifies which input parameters have the most significant impact on model results. Together, they provide valuable insights into the robustness and reliability of hydrological models."
  },
  {
    "objectID": "modelling/model_caliLinearReser.html",
    "href": "modelling/model_caliLinearReser.html",
    "title": "Calibration Prozess",
    "section": "",
    "text": "In this article, we will learn how to manage the entire hydrological modeling process (mehr Concept Details in Concept of Modelling) with the minimal model Linear-Reservoir model, including automatic calibration."
  },
  {
    "objectID": "modelling/model_caliLinearReser.html#load-forcing-data-and-obesevations",
    "href": "modelling/model_caliLinearReser.html#load-forcing-data-and-obesevations",
    "title": "Calibration Prozess",
    "section": "4.1 load Forcing Data and Obesevations",
    "text": "4.1 load Forcing Data and Obesevations\n\ndf_Labor &lt;- read_delim(\"L:\\\\Aktuelle Vorlesungsunterlagen\\\\MSc Wasserhaushaltsmodellierung\\\\Einzellinearspeicher Teil 1\\\\tbl_LaborMess_LinearReservior.txt\", delim = \"\\t\")\n\nnames(df_Labor) &lt;- c(\"t\", \"QZ\", \"QA\")\n\n\n\n1.1 -&gt; Q_In = df_Labor$QZ\n1.2 -&gt; Q_Out0 = 0\n1.3 -&gt; param_K = 50\nf1 -&gt; model_linearReservoir()\n2 -&gt; df_Labor$QZ\nf2 -&gt; NSE(), KGE()"
  },
  {
    "objectID": "modelling/model_caliLinearReser.html#run-the-model-with-forcing-data",
    "href": "modelling/model_caliLinearReser.html#run-the-model-with-forcing-data",
    "title": "Calibration Prozess",
    "section": "4.3 Run the model with Forcing Data",
    "text": "4.3 Run the model with Forcing Data\n\nnum_Q_Sim &lt;- model_linearReservoir(df_Labor$QZ, 0, param_K =  50)\n\nggLabor &lt;- ggplot(df_Labor) +\n  geom_line(aes(t, QZ), color = \"cyan\") +\n  geom_line(aes(t, num_Q_Sim), color = \"tomato\") +\n  geom_line(aes(t, QA), color = \"blue\") +\n  labs(x = \"Time [s]\", y = \"In-/Outflow [L/s]\")\nggplotly(ggLabor)"
  },
  {
    "objectID": "modelling/model_caliLinearReser.html#evaluete-with-nse-and-kge",
    "href": "modelling/model_caliLinearReser.html#evaluete-with-nse-and-kge",
    "title": "Calibration Prozess",
    "section": "4.4 Evaluete with NSE and KGE",
    "text": "4.4 Evaluete with NSE and KGE\n\nNSE(num_Q_Sim, df_Labor$QA)\n\n[1] 0.8141169\n\nKGE(num_Q_Sim, df_Labor$QA)\n\n[1] 0.8912488"
  },
  {
    "objectID": "modelling/model_caliLinearReser.html#create-the-fit-function",
    "href": "modelling/model_caliLinearReser.html#create-the-fit-function",
    "title": "Calibration Prozess",
    "section": "5.1 Create the Fit Function",
    "text": "5.1 Create the Fit Function\nBefore proceeding with automatic calibration, an important step is to create a function that the calibration algorithm will use. This function should take the parameter to be calibrated as an input. Thus, we need to modify our model and evaluation function into a “Fit Function.”\n\neva_fit &lt;- function(model_Param,\n                     model_Input,\n                     Q_Observ,\n                     fct_gof = NSE) {\n  Q_Simu &lt;- model_linearReservoir(model_Input, param_K = model_Param)\n  \n  - fct_gof(Q_Simu, Q_Observ)\n  \n}\n\neva_fit(60, df_Labor$QZ, df_Labor$QA)\n\n[1] -0.8752109\n\n\nThere is another critical point to consider when creating the Fit Function. Calibration algorithms need to know which criteria is better. Most calibration algorithms compare the current criteria value with the previous one (or several previous ones) and consider the minimum (or maximum) criteria value as the best. However, in the case of NSE and KGE, a better simulation results in a higher value. To handle this, we should set these criteria as negative values. By doing so, calibration algorithms like cali_UVS() can work effectively with them."
  },
  {
    "objectID": "modelling/model_caliLinearReser.html#calibrating",
    "href": "modelling/model_caliLinearReser.html#calibrating",
    "title": "Calibration Prozess",
    "section": "5.2 Calibrating",
    "text": "5.2 Calibrating\nWith the fit function in place, we can choose a calibration algorithm to optimize our model parameter (in this case, parameter \\(K\\)).\n\nlst_Cali &lt;- cali_UVS(eva_fit, x_Min = 40, x_Max = 90, model_Input = df_Labor$QZ, Q_Observ = df_Labor$QA, fct_gof = KGE)\n\n\n  |                                                                            \n  |                                                                      |   0%"
  },
  {
    "objectID": "modelling/model_caliLinearReser.html#load-labor-data",
    "href": "modelling/model_caliLinearReser.html#load-labor-data",
    "title": "Calibration Prozess",
    "section": "4.1 load Labor Data",
    "text": "4.1 load Labor Data\n\n# Load Labor Data\ndf_Labor &lt;- read_delim(\"L:\\\\Aktuelle Vorlesungsunterlagen\\\\MSc Wasserhaushaltsmodellierung\\\\Einzellinearspeicher Teil 1\\\\tbl_LaborMess_LinearReservior.txt\", delim = \"\\t\")\n\n# Rename the data, in order to more flexible Manipulation\nnames(df_Labor) &lt;- c(\"t\", \"QZ\", \"QA\")\n\n# The first 10 Line\nhead(df_Labor)\n\n# A tibble: 6 × 3\n      t    QZ    QA\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0     0    30\n2     1     0    30\n3     2     0    29\n4     3     0    29\n5     4     0    29\n6     5     0    29"
  },
  {
    "objectID": "modelling/model_caliLinearReser.html#concept-to-simulation",
    "href": "modelling/model_caliLinearReser.html#concept-to-simulation",
    "title": "Calibration Prozess",
    "section": "4.2 Concept to Simulation",
    "text": "4.2 Concept to Simulation\n\n\n1.1 -&gt; Q_In = df_Labor$QZ\n1.2 -&gt; Q_Out0 = 0\n1.3 -&gt; param_K = 50\nf1 -&gt; model_linearReservoir()\n2 -&gt; df_Labor$QZ\nf2 -&gt; NSE(), KGE()"
  },
  {
    "objectID": "modelling/model_caliLinearReser.html#load-experimental-data",
    "href": "modelling/model_caliLinearReser.html#load-experimental-data",
    "title": "Calibration Prozess",
    "section": "4.1 Load Experimental Data",
    "text": "4.1 Load Experimental Data\nIn this phase, we will utilize experimental data from a labor experiment. This dataset involves the physical simulation of a linear reservoir and provides the measured inflow and outflow data in liters per second (L/s).\n\n# Load Labor Data\ndf_Labor &lt;- read_delim(\"L:\\\\Aktuelle Vorlesungsunterlagen\\\\MSc Wasserhaushaltsmodellierung\\\\Einzellinearspeicher Teil 1\\\\tbl_LaborMess_LinearReservior.txt\", delim = \"\\t\")\n\n# Rename the data, in order to more flexible Manipulation\nnames(df_Labor) &lt;- c(\"t\", \"QZ\", \"QA\")\n\n# The first 10 Line\nhead(df_Labor)\n\n# A tibble: 6 × 3\n      t    QZ    QA\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0     0    30\n2     1     0    30\n3     2     0    29\n4     3     0    29\n5     4     0    29\n6     5     0    29"
  },
  {
    "objectID": "modelling/model_caliLinearReser.html#mapping-from-concept-to-proceeding",
    "href": "modelling/model_caliLinearReser.html#mapping-from-concept-to-proceeding",
    "title": "Calibration Prozess",
    "section": "4.2 Mapping from Concept to Proceeding",
    "text": "4.2 Mapping from Concept to Proceeding\nTo provide clarity, let’s map the conceptual understanding to the simulation before proceeding:\n\n\n1.1 -&gt; Q_In = df_Labor$QZ\n1.2 -&gt; Q_Out0 = 0\n1.3 -&gt; param_K = 50\nf1 -&gt; model_linearReservoir()\n2 -&gt; df_Labor$QZ\nf2 -&gt; NSE(), KGE()"
  },
  {
    "objectID": "modelling/model_caliLinearReser.html#running-the-model-with-forcing-data",
    "href": "modelling/model_caliLinearReser.html#running-the-model-with-forcing-data",
    "title": "Calibration Prozess",
    "section": "4.3 Running the Model with Forcing Data",
    "text": "4.3 Running the Model with Forcing Data\nAs a preliminary test, we can suggest certain parameter values, such as \\(K\\) at 90 and 60. After the simulation, we will store the results in variables num_Q_Sim and num_Q_Sim2 for further analysis.\n\n# run the model\n\nnum_Q_Sim &lt;- model_linearReservoir(df_Labor$QZ, 0, param_K =  90)\n\nnum_Q_Sim2 &lt;- model_linearReservoir(df_Labor$QZ, 0, param_K =  60)"
  },
  {
    "objectID": "modelling/model_caliLinearReser.html#visual-evaluation",
    "href": "modelling/model_caliLinearReser.html#visual-evaluation",
    "title": "Calibration Prozess",
    "section": "4.4 Visual Evaluation",
    "text": "4.4 Visual Evaluation\nBefore employing quantitative criteria, it’s beneficial to visually evaluate the simulation results using time series plots, which provide an initial sense of the model’s performance.\n\n# Visual Evaluation\nggLabor &lt;- ggplot(df_Labor) +\n  geom_line(aes(t, QZ, color = \"Inflow\")) +\n  geom_line(aes(t, num_Q_Sim, color = \"Simul1\")) +\n  geom_line(aes(t, num_Q_Sim2, color = \"Simul2\")) +\n  geom_line(aes(t, QA, color = \"Observ\")) +\n  scale_color_manual(values = c(Inflow = \"cyan\", Simul1 = \"red\", Simul2 = \"orange\", Observ = \"blue\"))+\n  labs(x = \"Time [s]\", y = \"In-/Outflow [L/s]\", color = \"Flow\")\nggplotly(ggLabor)"
  },
  {
    "objectID": "modelling/model_caliLinearReser.html#quantitative-evaluation",
    "href": "modelling/model_caliLinearReser.html#quantitative-evaluation",
    "title": "Calibration Prozess",
    "section": "4.5 Quantitative Evaluation",
    "text": "4.5 Quantitative Evaluation\nFor short or simplified time series, visual evaluation may suffice. However, when dealing with long-term data, we require standardized criteria to objectively assess the model’s performance.\nNSE and KGE are the most commonly used criteria in hydrological research. However, there are also additional criteria available in the hydroGOF package (use ?hydroGOF):\nQuantitative statistics included are: Mean Error (me), Mean Absolute Error (mae), Root Mean Square Error (rms), Normalized Root Mean Square Error (nrms), Pearson product-moment correlation coefficient (r), Spearman Correlation coefficient (r.Spearman), Coefficient of Determination (R2), Ratio of Standard Deviations (rSD), Nash-Sutcliffe efficiency (NSE), Modified Nash-Sutcliffe efficiency (mNSE), Relative Nash-Sutcliffe efficiency (rNSE), Index of Agreement (d), Modified Index of Agreement (md), Relative Index of Agreement (rd), Coefficient of Persistence (cp), Percent Bias (pbias), Kling-Gupta efficiency (KGE), the coef. of determination multiplied by the slope of the linear regression between ‘sim’ and ‘obs’ (bR2), and volumetric efficiency (VE).\n\nNSE(num_Q_Sim, df_Labor$QA)\n\n[1] 0.9234607\n\nNSE(num_Q_Sim2, df_Labor$QA)\n\n[1] 0.8752109\n\nKGE(num_Q_Sim, df_Labor$QA)\n\n[1] 0.8425622\n\nKGE(num_Q_Sim2, df_Labor$QA)\n\n[1] 0.9273187"
  },
  {
    "objectID": "dataprocess/NetCDF.html#r-7",
    "href": "dataprocess/NetCDF.html#r-7",
    "title": "NetCDF",
    "section": "4 R",
    "text": "4 R\n\nncvar_def()\n\nname\nunits\ndim\n\n\nYou also have the option to create a dimension with no data values, effectively making it a null dimension. However, you can still set attributes for this dimension to store non-array information.\nAfter defining all the variables, it’s necessary to gather them into a list.\n\n# Define a variable named \"T0\" with the units \"cel\" and dimensions dim_lat, dim_lon, and dim_time.\n# The missing value for this variable is set to -9999.\nvar_T0 &lt;- ncvar_def(\"T0\", \"cel\", list(dim_lat, dim_lon, dim_time), -9999)\n\n# Define a variable named \"crs\" with no units and no dimensions (empty list).\n# This variable is defined as NULL initially.\nvar_crs &lt;- ncvar_def(\"crs\", \"\", list(), NULL)\n\n# Combine variables into a list\nvars &lt;- list(var_T0, var_crs)"
  },
  {
    "objectID": "dataprocess/NetCDF.html#python-7",
    "href": "dataprocess/NetCDF.html#python-7",
    "title": "NetCDF",
    "section": "5 Python",
    "text": "5 Python\n\n# Define variables\nvar_T0 = nc_Create.createVariable(\"T0\", \"f4\", (\"latitude\", \"longitude\", \"time\"))\nvar_T0.units = \"cel\"\nvar_T0.missing_value = -9999\n\nvar_crs = nc_Create.createVariable(\"crs\", \"S1\")  # Create an empty variable\n\n:::\n\n5.1 Create new empty NetCDF file (R)\n\nR\n\n\n\nnc_create(filename, vars)\n\n\nnc_Create &lt;- nc_create(\"C:\\\\Lei\\\\HS_Web\\\\data_share\\\\minibeispiel_NetCDF.nc\", vars)\n\n\n\n\nYou can already create a NetCDF file with the (list of) variables you have:\n\n\n5.2 Put the Data\nAfter creating the NetCDF file, it will be an empty file in your local folder. The next step is to populate the file with data for each of the variables. This involves specifying the values for each variable and writing them to the file. ::: {.panel-tabset}"
  },
  {
    "objectID": "dataprocess/NetCDF.html#r-9",
    "href": "dataprocess/NetCDF.html#r-9",
    "title": "NetCDF",
    "section": "6 R",
    "text": "6 R\n\nncvar_put()\n\n\nncvar_put(nc_Create, var_T0, runif(length(num_Dim_Lat) * length(num_Dim_Lon) * length(num_Dim_Time)))"
  },
  {
    "objectID": "dataprocess/NetCDF.html#python-8",
    "href": "dataprocess/NetCDF.html#python-8",
    "title": "NetCDF",
    "section": "7 Python",
    "text": "7 Python\n\n# Add data to the \"T0\" variable (random data)\nvar_T0[:] = np.random.rand(len(num_Dim_Lat), len(num_Dim_Lon), len(num_Dim_Time))\n\n:::\n\n7.1 Put Attributes\nWhen populating a NetCDF file, it’s essential to not only specify the variable data values but also the attributes associated with those variables. Attributes provide crucial metadata that describes the data, such as units, long names, and other relevant information.\n\nRPython\n\n\n\nncatt_put()\n\nAbsolutely, you can set attributes not only for individual variables.\n\n# Add the \"long_name\" and \"EPSG\" attributes to the variable \"var_crs\"\nncatt_put(nc_Create, var_crs, \"long_name\", \"coordinate reference system\")\nncatt_put(nc_Create, var_crs, \"EPSG\", \"EPSG:4236\")\n\n\n\n\nvar_crs.long_name = \"coordinate reference system\"\nvar_crs.EPSG = \"EPSG:4236\"\n\n\n\n\nBut also for the entire NetCDF file as global attributes. Global attributes provide overarching information about the dataset, such as its title, source, creation date, and any other relevant details.\n\nRPython\n\n\n\n# Add the \"title\" and \"author\" global attributes to the NetCDF file\nncatt_put(nc_Create, 0, \"title\", \"Multidimensional data example\")\nncatt_put(nc_Create, 0, \"author\", \"Kan, Lei, kan.lei@ruhr-uni-bochum.de\")\n\n\n\n\n# Add global attributes\nnc_Create.title = \"Multidimensional data example\"\nnc_Create.author = \"Kan, Lei, kan.lei@ruhr-uni-bochum.de\"\n\n\n\n\n\n\n7.2 Close\nAt the end, make sure to close the connections to your NetCDF files.\n\nRPython\n\n\n\nnc_close(nc_Create)\n\n\n\n\n# Close the NetCDF file\nnc_Create.close()\n\n\n\n\nOnce you’ve gone through these steps, you’ll have a well-maintained NetCDF file that can be easily used for any further processing, transformations, or visualization."
  }
]